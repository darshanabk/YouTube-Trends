{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"def Source_File_Extraction(repo_url, kaggle_repo_url, source_path):\n    \"\"\"\n    This function checks if a specified Git repository already exists in the local system.\n    If the repository exists, it pulls the latest changes from the remote repository.\n    If the repository doesn't exist, it clones the repository from the provided URL.\n    \n    After ensuring the repository is up-to-date, it searches for a JSON file that starts with \"S_\" \n    and ends with \"records.json\" in the specified source directory, loads the file using pandas, \n    and returns the data as a DataFrame.\n\n    Args:\n    - repo_url (str): The URL of the Git repository to clone if not already present.\n    - kaggle_repo_url (str): The local path where the repository is stored or will be cloned to.\n    - source_path (str): The directory where the JSON file is stored.\n\n    Returns:\n    - pd.DataFrame: The data from the JSON file as a pandas DataFrame.\n    \"\"\"\n    if os.path.exists(kaggle_repo_url):\n        print(\"Already cloned and the repo file exists\")\n        repo = git.Repo(kaggle_repo_url)  # Access the existing repository\n        origin = repo.remote(name='origin')  # Get the remote repository\n        origin.pull()  # Pull the latest changes from the repository\n        print(\"Successfully pulled the git repo before push\")\n    else:\n        # Clone the repository if it doesn't exist\n        repo = git.Repo.clone_from(repo_url, kaggle_repo_url)\n        print(\"Successfully cloned the git repo\")\n    \n    # List all files in the source path and find the relevant JSON file\n    output_files = os.listdir(source_path)\n    Source_File = max([i for i in output_files if i.startswith(\"S_\") and i.endswith('records.json')])\n    \n    # Read the found JSON file into a pandas DataFrame\n    Source_File = pd.read_json(f'{source_path}/{Source_File}')\n    \n    return Source_File","metadata":{"_uuid":"3d8fc9ab-7ea9-4676-9a5a-3cfe6d40cac1","_cell_guid":"80166acd-edad-4811-a869-d38a91d87cf6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-05T12:44:12.557694Z","iopub.execute_input":"2025-03-05T12:44:12.558040Z","iopub.status.idle":"2025-03-05T12:44:12.564932Z","shell.execute_reply.started":"2025-03-05T12:44:12.558012Z","shell.execute_reply":"2025-03-05T12:44:12.563563Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n**Observation:**  \n\n1. Null values are present in the following columns:\n   - **`videoDefaultLanguage`**  (will be dropped after data cleaning)\n   - **`videoDefaultAudioLanguage`** \n   - **`channelCountry`**\n\n---\n\n2. The following columns will be dropped as part of data cleaning:\n   - **`videoDescription`**: Reserved for analysis in future NLP project with a broader dataset.  \n   - **`videoLiveBroadcastContent`**: All values are `'none'`, providing no variability or insights. \n   - **`videoFavoriteCount`**: All values are `0`, making it redundant.  \n   - **`videoTags`**: Reserved for analysis in future NLP project with a broader dataset.  \n   - **`videoUniqueId`**: Identified as a duplicate column.  \n   - **`channelIdUnique`**: Identified as a duplicate column.  \n   - **`channelTitleCheck`**: Identified as a duplicate column.  \n   - **`channelDescription`**: Reserved for analysis in future NLP project with a broader dataset.\n---\n\n3. The columns **`channelName`** and **`videoTitle`** require further processing due to the presence of:\n    - Multilingual text.  \n    - Emojis and special characters.  \n\n---","metadata":{"_uuid":"aa9d486b-bd79-49c0-bbb7-5127bceda966","_cell_guid":"f1875798-c4a9-452a-8611-86e2859e2bb1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def DataCleaning(Target_File):\n    \"\"\"\n    Cleans the input DataFrame by performing the following operations:\n    1. Drops irrelevant columns.\n    2. Removes duplicate rows based on videoId, videoTitle, and channelId.\n    3. Filters videos based on language (only those with 'videoDefaultAudioLanguage' starting with 'en').\n    4. Translates non-ASCII characters, removes emojis, and decodes HTML entities from 'channelName' and 'videoTitle'.\n    5. Fills missing values in 'channelCountry' with 'Unknown'.\n    6. Keeps the latest or highest view count record when duplicates are found.\n\n    Args:\n    - Target_File (pd.DataFrame): The DataFrame to clean.\n\n    Returns:\n    - pd.DataFrame: The cleaned DataFrame.\n    \"\"\"\n\n    # Drop irrelevant columns\n    Target_File = Target_File.drop(['videoDescription', 'videoLiveBroadcastContent', 'videoFavoriteCount',\n                                    'videoTags', 'videoUniqueId', 'channelIdUnique', 'channelTitleCheck', 'channelDescription'], axis=1, errors='ignore')\n\n    # Filter only English audio language\n    Target_File_EN = Target_File[Target_File['videoDefaultAudioLanguage'].str.startswith(\"en\", na=False)].reset_index(drop=True)\n\n    # Translate, remove emojis, and clean non-ASCII characters\n    for col in ['channelName', 'videoTitle']:\n        Target_File_EN[col] = Target_File_EN[col].apply(lambda x: GoogleTranslator(source='auto', target='en').translate(x) if not x.isascii() else x)\n        Target_File_EN[col] = Target_File_EN[col].apply(lambda x: emoji.replace_emoji(x, replace=''))\n        Target_File_EN[col] = Target_File_EN[col].apply(lambda x: html.unescape(x))\n        Target_File_EN[col] = Target_File_EN[col].apply(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', x))\n\n    # Fill missing channelCountry values with 'Unknown'\n    Target_File_EN['channelCountry'] = Target_File_EN['channelCountry'].fillna('Unknown')\n\n    # Remove duplicates based on 'videoId' and keep the latest or highest view count record\n    Target_File_EN = Target_File_EN.sort_values(by=['videoViewCount', 'videoPublishedOn'], ascending=[False, False])\n    Target_File_EN = Target_File_EN.drop_duplicates(subset=['videoId'], keep='first').reset_index(drop=True)\n\n    # Drop unnecessary column\n    if 'videoDefaultLanguage' in Target_File_EN.columns:\n        Target_File_EN = Target_File_EN.drop(['videoDefaultLanguage'], axis=1)\n\n    return Target_File_EN\n","metadata":{"_uuid":"b1738051-e9d8-44fc-9f50-01088e4005ff","_cell_guid":"58c40ae4-a980-4861-bb62-e7d40a10e3ed","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-05T12:44:12.566771Z","iopub.execute_input":"2025-03-05T12:44:12.567209Z","iopub.status.idle":"2025-03-05T12:44:12.601742Z","shell.execute_reply.started":"2025-03-05T12:44:12.567164Z","shell.execute_reply":"2025-03-05T12:44:12.600590Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def GitHubPush(Target_File_EN):\n    \"\"\"\n    This function handles the process of saving a cleaned and processed DataFrame as a JSON file, \n    pushing it to a GitHub repository. It ensures that the file is properly named with a timestamp \n    and number of records, creates necessary directories, and commits the changes to the repository.\n    \n    Args:\n    - Target_File_EN (pd.DataFrame): The DataFrame that contains the processed data to be saved and pushed.\n    \n    Returns:\n    - None: This function performs file handling and Git operations but does not return anything.\n    \"\"\"\n\n    # Count the number of records in the DataFrame\n    record_count = len(Target_File_EN)\n    \n    # Generate a timestamp for the file name using the current time in IST (Indian Standard Time).\n    timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H_%M_%S\")\n    \n    # Create a filename using the generated timestamp and number of records to ensure uniqueness.\n    filename = f\"DC_{timestamp}_{record_count}_records.json\"\n    \n    # Save the DataFrame to a JSON file in a readable format (with indentation)\n    Target_File_EN.to_json(filename, orient=\"records\", indent=4)\n    print(f\"DataFrame saved as {filename}\")\n    \n    # Check if the destination directory exists\n    if not os.path.exists(destination_path):\n        # If the directory does not exist, create it\n        os.makedirs(destination_path)\n        print('Created the destination directory, DataCleaning/Daily')\n        # Copy the saved file into the newly created directory\n        shutil.copyfile(f'/kaggle/working/{filename}', f'{destination_path}/{filename}')\n    else:\n        print('Destination directory already exists')\n        # Copy the file to the existing directory\n        shutil.copyfile(f'/kaggle/working/{filename}', f'{destination_path}/{filename}')\n    \n    # Initialize the repository for git operations using the local GitHub repository URL\n    repo = Repo(kaggle_repo_url)\n    \n    # Add the copied file to the staging area for git commit\n    repo.index.add([f\"{destination_path}/{filename}\"])\n    \n    # Create a timestamp for the commit message\n    timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H:%M:%S\")\n    # Commit the changes with a message that includes the timestamp and the filename\n    repo.index.commit(f\"{timestamp} Added files from Kaggle notebook, {filename}\")\n    \n    # Push the changes to the remote repository\n    origin = repo.remote(name=\"origin\")\n    # push_result = origin.push()\n    push_result = origin.push(refspec=f\"HEAD:refs/heads/data-cleaning\")\n    \n    # Check if the push was successful and print the result\n    if push_result:\n        print(\"Push successful.\")\n    else:\n        print(\"Push failed.\")","metadata":{"_uuid":"3dd98ff5-e097-412c-9e49-ae8827a5d714","_cell_guid":"67ef6e82-4e7b-47c7-81db-c0ca812dc060","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-05T12:44:12.603072Z","iopub.execute_input":"2025-03-05T12:44:12.603479Z","iopub.status.idle":"2025-03-05T12:44:12.627746Z","shell.execute_reply.started":"2025-03-05T12:44:12.603442Z","shell.execute_reply":"2025-03-05T12:44:12.626594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    \"\"\"\n    The main function orchestrates the entire data pipeline by:\n    1. Extracting the source data from the given repository URL.\n    2. Cleaning the extracted data using the DataCleaning function.\n    3. Pushing the final cleaned file to a GitHub repository.\n    \n    This function executes the steps in sequence to process and upload data.\n    \n    Args:\n    - None: This function does not accept any arguments. It uses predefined repository URLs and paths.\n    \n    Returns:\n    - None: This function does not return anything but performs data processing and Git operations.\n    \"\"\"\n    \n    # Step 1: Extract the source file from the repository based on the provided URL and path.\n    Source_File = Source_File_Extraction(repo_url, kaggle_repo_url, source_path)\n    \n    # Step 2: Clean the extracted data using the DataCleaning function.\n    Cleaned_File = DataCleaning(Source_File)\n    \n    # Optional: Uncomment to display the cleaned file sorted by video duration.\n    # display(Cleaned_File.sort_values(by='videoDurationInSeconds', ascending=True))\n    \n    # Step 3: Push the processed and feature-engineered data to GitHub using GitHubPush function.\n    GitHubPush(Cleaned_File)","metadata":{"_uuid":"34697395-1f1c-4045-af03-e014da7e1db1","_cell_guid":"7d97eb4c-4052-439b-a03a-d97007941f5f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-05T12:44:12.628792Z","iopub.execute_input":"2025-03-05T12:44:12.629093Z","iopub.status.idle":"2025-03-05T12:44:12.652751Z","shell.execute_reply.started":"2025-03-05T12:44:12.629065Z","shell.execute_reply":"2025-03-05T12:44:12.651620Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    \"\"\"\n    This script is the entry point for the data cleaning pipeline.\n    It performs the following tasks:\n    1. Imports necessary libraries for data processing, file handling, and Git operations.\n    2. Retrieves user secrets for repository URL.\n    3. Sets up paths for different directories (source, destination, etc.).\n    4. Configures pandas to display all columns and rows without truncation.\n    5. Calls the main function to execute the pipeline.\n\n    The script is designed to be executed as the main module in a Python environment.\n    It ensures that all necessary operations are performed, including fetching source data, \n    cleaning, and pushing the final data to a GitHub repository.\n    \"\"\"\n\n    # Import necessary libraries\n    import os  \n    import git  # Git library for interacting with repositories\n    from git import Repo  # GitHub repository interaction\n    import time  # For time-related operations\n    import datetime  # For working with date and time\n    from pytz import timezone  # For timezone management\n    import pytz  # Timezone handling\n    import pandas as pd  # For data manipulation and analysis\n    import deep_translator  # For translation services\n    from deep_translator import GoogleTranslator  # Google Translate API integration\n    import shutil  # For file operations like copying or removing\n    import emoji  # For handling emojis in the data\n    import re  # For regular expression operations\n    import html  # For HTML parsing and escaping\n    from kaggle_secrets import UserSecretsClient  # For accessing Kaggle's secret management system\n    \n    # Retrieve secret value for repository URL from Kaggle secrets storage\n    user_secrets = UserSecretsClient()\n    secret_value_0 = user_secrets.get_secret(\"dataCleanRepoUrl\")\n    repo_url = secret_value_0  # URL for the GitHub repository used in this pipeline\n    \n    # Set timezone to Indian Standard Time (IST)\n    ist = timezone('Asia/Kolkata')\n    \n    # Define paths for different directories\n    kaggle_repo_url = '/kaggle/working/DevOps-YouTube-Trends'  # Path to the working repository on Kaggle\n    destination_path = '/kaggle/working/DevOps-YouTube-Trends/DataCleaning/Daily'  # Path to store cleaned data\n    source_path = '/kaggle/working/DevOps-YouTube-Trends/Source/Daily'  # Path to source raw data\n    \n    # Configure pandas to display all columns and rows without truncation for easier debugging\n    pd.set_option(\"display.max_columns\", None)  # Prevent truncating columns\n    pd.set_option(\"display.max_rows\", None)  # Prevent truncating rows\n    \n    # Call the main function to execute the data pipeline\n    main()","metadata":{"_uuid":"d1e05076-eab2-4606-8973-3e9d364364ce","_cell_guid":"34ba1552-9550-46ad-bf07-46dcd41bc8dd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-05T12:44:12.653879Z","iopub.execute_input":"2025-03-05T12:44:12.654200Z","iopub.status.idle":"2025-03-05T12:44:20.356425Z","shell.execute_reply.started":"2025-03-05T12:44:12.654172Z","shell.execute_reply":"2025-03-05T12:44:20.354805Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}
