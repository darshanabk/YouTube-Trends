{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde443da",
   "metadata": {
    "_cell_guid": "896255f9-fd11-4a04-b36a-3c9427433a4b",
    "_uuid": "232f159d-1364-498f-864a-1ef104b1ab54",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-10T14:28:36.061107Z",
     "iopub.status.busy": "2025-09-10T14:28:36.060742Z",
     "iopub.status.idle": "2025-09-10T14:28:36.069026Z",
     "shell.execute_reply": "2025-09-10T14:28:36.068053Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.017025,
     "end_time": "2025-09-10T14:28:36.070658",
     "exception": false,
     "start_time": "2025-09-10T14:28:36.053633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def DataCleaning_File_Extraction(repo_url, kaggle_repo_url, DataCleaning_path):\n",
    "    \"\"\"\n",
    "    Extracts and processes a data cleaning file from a specified GitHub repository.\n",
    "\n",
    "    This function checks if the repository is already cloned locally. If found, it pulls the latest changes;\n",
    "    otherwise, it clones the repository. It then searches for a JSON file in the specified directory that starts\n",
    "    with 'DC_' and ends with 'records.json', reads it into a pandas DataFrame, and returns the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    repo_url (str): The URL of the GitHub repository to clone or update.\n",
    "    kaggle_repo_url (str): The local path where the repository is cloned.\n",
    "    DataCleaning_path (str): The directory path where the data cleaning files are stored.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A pandas DataFrame containing the extracted data from the JSON file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the repository already exists locally\n",
    "    if os.path.exists(kaggle_repo_url):\n",
    "        print(\"Repository already exists locally.\")\n",
    "        repo = git.Repo(kaggle_repo_url)  # Access the existing repository\n",
    "        origin = repo.remote(name='origin')  # Get the remote repository reference\n",
    "        origin.pull()  # Pull the latest updates from the remote repository\n",
    "        print(\"Successfully pulled the latest changes.\")\n",
    "    else:\n",
    "        # Clone the repository if it does not exist\n",
    "        repo = git.Repo.clone_from(repo_url, kaggle_repo_url)\n",
    "        print(\"Successfully cloned the repository.\")\n",
    "\n",
    "    # List all files in the specified directory and filter for the relevant JSON file\n",
    "    output_files = os.listdir(DataCleaning_path)\n",
    "    DataCleaning_File = max(\n",
    "        [file for file in output_files if file.startswith(\"DC_\") and file.endswith('records.json')]\n",
    "    )\n",
    "\n",
    "    # Read the identified JSON file into a pandas DataFrame\n",
    "    DataCleaning_File = pd.read_json(os.path.join(DataCleaning_path, DataCleaning_File))\n",
    "\n",
    "    return DataCleaning_File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af4103e",
   "metadata": {
    "_cell_guid": "2a1b6fe7-329d-43bd-b6dd-210d60c277ce",
    "_uuid": "077cb9ac-6ccb-45dc-be77-f9a007f149fc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-10T14:28:36.082223Z",
     "iopub.status.busy": "2025-09-10T14:28:36.081859Z",
     "iopub.status.idle": "2025-09-10T14:28:36.088390Z",
     "shell.execute_reply": "2025-09-10T14:28:36.087455Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.013956,
     "end_time": "2025-09-10T14:28:36.089923",
     "exception": false,
     "start_time": "2025-09-10T14:28:36.075967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Requirement_File_Extraction(repo_url, kaggle_repo_url, requirement_path):\n",
    "    \"\"\"\n",
    "    Ensures the repository is up-to-date by either pulling the latest changes or cloning it.\n",
    "    Then, extracts and returns the most recent JSON file starting with \"RE_\" and ending with \n",
    "    \"country_details.json\" from the specified requirement directory as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - repo_url (str): Git repository URL to clone if not present.\n",
    "    - kaggle_repo_url (str): Local directory path for the repository.\n",
    "    - requirement_path (str): Directory containing the JSON files.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Data from the most recent JSON file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the repository already exists locally\n",
    "    if os.path.exists(kaggle_repo_url):\n",
    "        print(\"Already cloned and the repo file exists\")\n",
    "        \n",
    "        # Access the existing repository and pull the latest changes\n",
    "        repo = git.Repo(kaggle_repo_url)\n",
    "        origin = repo.remote(name='origin')\n",
    "        origin.pull()  # Pull the latest changes\n",
    "        print(\"Successfully pulled the git repo before push\")\n",
    "    else:\n",
    "        # Clone the repository if it doesn't exist locally\n",
    "        repo = git.Repo.clone_from(repo_url, kaggle_repo_url)\n",
    "        print(\"Successfully cloned the git repo\")\n",
    "    \n",
    "    # List all files in the requirement directory\n",
    "    output_files = os.listdir(requirement_path)\n",
    "    \n",
    "    # Find the most recent JSON file that starts with \"RE_\" and ends with \"country_details.json\"\n",
    "    Requirement_File = max([i for i in output_files if i.startswith(\"RE_\") and i.endswith('country_details.json')])\n",
    "    \n",
    "    # Read the found JSON file into a pandas DataFrame\n",
    "    Requirement_File = pd.read_json(f'{requirement_path}/{Requirement_File}')\n",
    "    \n",
    "    return Requirement_File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89fe3f74",
   "metadata": {
    "_cell_guid": "f416fbba-8bd7-4ee4-88b0-335e8603adc8",
    "_uuid": "66d9607a-3b6f-433e-87b0-144e18846690",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-10T14:28:36.101568Z",
     "iopub.status.busy": "2025-09-10T14:28:36.101223Z",
     "iopub.status.idle": "2025-09-10T14:28:36.106859Z",
     "shell.execute_reply": "2025-09-10T14:28:36.105927Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.0132,
     "end_time": "2025-09-10T14:28:36.108435",
     "exception": false,
     "start_time": "2025-09-10T14:28:36.095235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def videoDurationClassification(videoDurationInSeconds):\n",
    "    \"\"\"\n",
    "    Classifies the video duration into categories based on its length in seconds.\n",
    "\n",
    "    Args:\n",
    "    - videoDurationInSeconds (int): Duration of the video in seconds.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string indicating the classification of the video duration.\n",
    "    \"\"\"\n",
    "    \n",
    "     # Classifying the video duration into new categories\n",
    "    if 0 <= videoDurationInSeconds < 120:\n",
    "        return \"Shorts (<2 min)\"  # Less than 2 minutes\n",
    "    elif 120 <= videoDurationInSeconds < 600:\n",
    "        return \"Standard (2–10 min)\"  # Between 2 and 10 minutes\n",
    "    elif 600 <= videoDurationInSeconds < 1800:\n",
    "        return \"Intermediate (10–30 min)\"  # Between 10 and 30 minutes\n",
    "    elif 1800 <= videoDurationInSeconds < 3600:\n",
    "        return \"Workshop (30 min–1 hr)\"  # Between 30 minutes and 1 hour\n",
    "    elif 3600 <= videoDurationInSeconds < 7200:\n",
    "        return \"Deep Dive (1–2 hr)\"  # Between 1 and 2 hours\n",
    "    elif 7200 <= videoDurationInSeconds < 18000:\n",
    "        return \"End-to-End (2–5 hr)\"  # Between 2 and 5 hours\n",
    "    elif videoDurationInSeconds >= 18000:\n",
    "        return \"Marathon (5 hr+)\"  # More than 5 hours\n",
    "    else:\n",
    "        return \"Invalid video duration\"  # Invalid value for video duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74495823",
   "metadata": {
    "_cell_guid": "b43e05a9-be8d-4dde-9076-674c12cd5fb4",
    "_uuid": "021c44d4-aa9b-4ec1-a34d-0ea849830603",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.005216,
     "end_time": "2025-09-10T14:28:36.118775",
     "exception": false,
     "start_time": "2025-09-10T14:28:36.113559",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Channel Growth Calculation\n",
    "\n",
    "## **Overview**\n",
    "The channel growth metric is designed to assess the growth of a YouTube channel based on key engagement indicators: \n",
    "- **Channel View Count**\n",
    "- **Channel Subscriber Count**\n",
    "- **Channel Video Count**\n",
    "- **Channel Age (in years)**\n",
    "\n",
    "## **Formula**\n",
    "The channel growth score is computed as:\n",
    "\n",
    "```python\n",
    "channel_growth = ((normalized_views * weight_views) + \n",
    "                  (normalized_subscribers * weight_subscribers) + \n",
    "                  (normalized_videos * weight_videos)) / channel_age_in_years\n",
    "```\n",
    "\n",
    "where:\n",
    "- **Min-Max Normalization** is applied to views, subscribers, and video count:\n",
    "  ```python\n",
    "  normalized_value = (value - min_value) / (max_value - min_value)\n",
    "  ```\n",
    "- **Weighting factors** determine the relative importance of each metric:\n",
    "  - `weight_views = 0.5`\n",
    "  - `weight_subscribers = 0.3`\n",
    "  - `weight_videos = 0.2`\n",
    "- **Channel Age (years)** is computed from:\n",
    "  ```python\n",
    "  channel_age_in_years = (current_timestamp - channelPublishedOnInSeconds) / (365 * 24 * 60 * 60)\n",
    "  ```\n",
    "\n",
    "## **Concepts Used**\n",
    "1. **Min-Max Normalization**: Scales values between 0 and 1.\n",
    "   - [Feature Scaling (Wikipedia)](https://en.wikipedia.org/wiki/Feature_scaling)\n",
    "2. **Weighted Scoring**: Prioritizes key metrics based on impact.\n",
    "   - [Weighted Scoring Model](https://theproductmanager.com/topics/weighted-scoring-model/)\n",
    "3. **YouTube Analytics Metrics**: Defines the importance of views, subscribers, and videos.\n",
    "   - [YouTube Analytics Help](https://support.google.com/youtube/answer/9002587?hl=en)\n",
    "4. **Channel Age Calculation**: Determines the time span since the channel was created.\n",
    "   - [Unix Time Conversion](https://www.unixtimestamp.com/)\n",
    "\n",
    "## **Implementation Example**\n",
    "```python\n",
    "import time\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "def calculate_channel_growth(view_count, subscriber_count, video_count, channel_published_on):\n",
    "    utc_timestamp = int(time.time())\n",
    "    zone = pytz.timezone('Asia/Kolkata')\n",
    "    current_ist_time = datetime.datetime.fromtimestamp(utc_timestamp, zone)\n",
    "    channel_age_in_years = (current_ist_time - channel_published_on) / (365 * 24 * 60 * 60)\n",
    "    \n",
    "    # Min-Max normalization (Assume predefined min/max values from dataset)\n",
    "    min_views, max_views = 1000, 10000000\n",
    "    min_subscribers, max_subscribers = 10, 1000000\n",
    "    min_videos, max_videos = 1, 10000\n",
    "    \n",
    "    normalized_views = (view_count - min_views) / (max_views - min_views)\n",
    "    normalized_subscribers = (subscriber_count - min_subscribers) / (max_subscribers - min_subscribers)\n",
    "    normalized_videos = (video_count - min_videos) / (max_videos - min_videos)\n",
    "    \n",
    "    # Weighted sum\n",
    "    growth_score = ((normalized_views * 0.5) + (normalized_subscribers * 0.3) + (normalized_videos * 0.2)) / channel_age_in_years\n",
    "    \n",
    "    return growth_score\n",
    "```\n",
    "\n",
    "## **Conclusion**\n",
    "This approach helps in analyzing a YouTube channel's growth potential by factoring in **engagement, longevity, and content volume**. It provides a scalable and adaptable framework for evaluating growth trends over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "347ba2c3",
   "metadata": {
    "_cell_guid": "a707e77e-0892-46c5-9bd7-497214f1975e",
    "_uuid": "c312c98a-e18c-4939-829b-a7fb6ade5d8c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-10T14:28:36.130465Z",
     "iopub.status.busy": "2025-09-10T14:28:36.130122Z",
     "iopub.status.idle": "2025-09-10T14:28:36.134126Z",
     "shell.execute_reply": "2025-09-10T14:28:36.133181Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.011382,
     "end_time": "2025-09-10T14:28:36.135661",
     "exception": false,
     "start_time": "2025-09-10T14:28:36.124279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def parse_datetime(value):\n",
    "#     \"\"\"\n",
    "#     Parses a datetime string into a Pandas datetime object based on specific formats.\n",
    "    \n",
    "#     Handles two datetime formats:\n",
    "#     1. \"%Y-%m-%dT%H:%M:%SZ\" for ISO 8601 format without fractional seconds.\n",
    "#     2. \"%Y-%m-%dT%H:%M:%S.%fZ\" for ISO 8601 format with fractional seconds.\n",
    "\n",
    "#     Args:\n",
    "#     - value (str): The input datetime string to be parsed.\n",
    "\n",
    "#     Returns:\n",
    "#     - pd.Timestamp or pd.NaT: A Pandas Timestamp object if the format matches, otherwise pd.NaT.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Check for the presence of \"Z\" and determine the format based on whether the string contains a decimal point\n",
    "#     if \"Z\" in value and \".\" not in value:\n",
    "#         return pd.to_datetime(value, format=\"%Y-%m-%dT%H:%M:%SZ\")  # Format without fractional seconds\n",
    "#     elif \"Z\" in value and \".\" in value:\n",
    "#         return pd.to_datetime(value, format=\"%Y-%m-%dT%H:%M:%S.%fZ\")  # Format with fractional seconds\n",
    "#     else:\n",
    "#         return pd.NaT  # Return Not a Time (NaT) if the format doesn't match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a124470b",
   "metadata": {
    "_cell_guid": "5df42003-f4f2-42d2-946a-94f5b3bfa596",
    "_uuid": "b0c1f55b-9271-492d-8fe0-21b2791aefe2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-10T14:28:36.146789Z",
     "iopub.status.busy": "2025-09-10T14:28:36.146400Z",
     "iopub.status.idle": "2025-09-10T14:28:36.150921Z",
     "shell.execute_reply": "2025-09-10T14:28:36.149957Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.011866,
     "end_time": "2025-09-10T14:28:36.152517",
     "exception": false,
     "start_time": "2025-09-10T14:28:36.140651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_datetime(value):\n",
    "    \"\"\"\n",
    "    Parses a datetime string into a Pandas datetime object.\n",
    "    Handles ISO 8601 formats with and without fractional seconds.\n",
    "    Returns NaT if parsing fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.to_datetime(value, errors=\"coerce\")  # Automatically detects format\n",
    "    except Exception:\n",
    "        return pd.NaT  # Return NaT if parsing fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "859c5511",
   "metadata": {
    "_cell_guid": "d74dfd98-1142-4873-a66c-7b7c9408b506",
    "_uuid": "7ab4d689-c71e-4d1f-85ba-2994e708fa74",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-10T14:28:36.163857Z",
     "iopub.status.busy": "2025-09-10T14:28:36.163465Z",
     "iopub.status.idle": "2025-09-10T14:28:36.167272Z",
     "shell.execute_reply": "2025-09-10T14:28:36.166375Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.011076,
     "end_time": "2025-09-10T14:28:36.168822",
     "exception": false,
     "start_time": "2025-09-10T14:28:36.157746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def normalize(series):\n",
    "#     \"\"\"\n",
    "#     Normalizes a Pandas Series using Min-Max Scaling.\n",
    "\n",
    "#     Parameters:\n",
    "#     series (pd.Series): The input series to be normalized.\n",
    "\n",
    "#     Returns:\n",
    "#     np.ndarray: A NumPy array containing normalized values between 0 and 1.\n",
    "#     \"\"\"\n",
    "#     scaler = MinMaxScaler()  # Initialize the MinMaxScaler\n",
    "#     return scaler.fit_transform(series.values.reshape(-1, 1)).flatten()  # Normalize and return a 1D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25029bba",
   "metadata": {
    "_cell_guid": "ab836dbe-e6c3-43a5-919d-8fc8214c50a0",
    "_uuid": "599ab2bc-39ad-4a83-91da-a21c9292c8d3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-10T14:28:36.180123Z",
     "iopub.status.busy": "2025-09-10T14:28:36.179738Z",
     "iopub.status.idle": "2025-09-10T14:28:36.184337Z",
     "shell.execute_reply": "2025-09-10T14:28:36.183370Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012185,
     "end_time": "2025-09-10T14:28:36.186058",
     "exception": false,
     "start_time": "2025-09-10T14:28:36.173873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize(series):\n",
    "    \"\"\"\n",
    "    Normalizes a Pandas Series using Min-Max Scaling after handling NaN values.\n",
    "    \"\"\"\n",
    "    series = series.fillna(1e-6)  # Replace NaNs with a small value\n",
    "    scaler = MinMaxScaler()\n",
    "    return scaler.fit_transform(series.values.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6c35bc5",
   "metadata": {
    "_cell_guid": "bafa6ba7-8554-4329-9e25-a51c713a5568",
    "_uuid": "a12bec73-f434-497a-881e-4a528247919a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-10T14:28:36.197640Z",
     "iopub.status.busy": "2025-09-10T14:28:36.197236Z",
     "iopub.status.idle": "2025-09-10T14:28:36.201491Z",
     "shell.execute_reply": "2025-09-10T14:28:36.200539Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012152,
     "end_time": "2025-09-10T14:28:36.203531",
     "exception": false,
     "start_time": "2025-09-10T14:28:36.191379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_log_transform(series):\n",
    "    \"\"\"\n",
    "    Applies log transformation while handling zeros and negatives.\n",
    "    \"\"\"\n",
    "    return np.log1p(np.maximum(series, 0))  # Ensures no negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7faedf1b",
   "metadata": {
    "_cell_guid": "e2b9d506-5c92-43b5-acd0-3b5c17144efe",
    "_uuid": "915702de-77e0-41d4-ac4b-42a9be88b022",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-10T14:28:36.214534Z",
     "iopub.status.busy": "2025-09-10T14:28:36.214202Z",
     "iopub.status.idle": "2025-09-10T14:28:36.218589Z",
     "shell.execute_reply": "2025-09-10T14:28:36.217683Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.011768,
     "end_time": "2025-09-10T14:28:36.220302",
     "exception": false,
     "start_time": "2025-09-10T14:28:36.208534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def calculate_channel_growth(Cleaned_File):\n",
    "#     \"\"\"\n",
    "#     Calculates the growth score for channels and engagement score for videos based on various metrics.\n",
    "    \n",
    "#     The growth score for each channel is calculated using the normalized values of:\n",
    "#     - View count\n",
    "#     - Subscriber count\n",
    "#     - Video count\n",
    "#     - Channel age (in years)\n",
    "    \n",
    "#     The engagement score for each video is calculated using:\n",
    "#     - Views per day\n",
    "#     - Like-to-view ratio\n",
    "#     - Comment-to-view ratio\n",
    "\n",
    "#     Args:\n",
    "#     - Cleaned_File (pd.DataFrame): The dataframe containing channel and video data to calculate growth and engagement scores.\n",
    "\n",
    "#     Returns:\n",
    "#     - pd.DataFrame: The cleaned dataframe with the added columns for growth and engagement scores.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Get the current IST time (Indian Standard Time) to calculate age-based metrics\n",
    "#     utc_timestamp = int(time.time())\n",
    "#     zone = pytz.timezone('Asia/Kolkata')\n",
    "#     current_ist_time = datetime.datetime.fromtimestamp(utc_timestamp, zone).replace(tzinfo=None)\n",
    "#     # Channel Age Calculation (in years)\n",
    "#     channelPublishedOn = Cleaned_File[\"channelPublishedOn\"].apply(parse_datetime)\n",
    "#     Cleaned_File['channelAgeInYears'] = (current_ist_time - channelPublishedOn).dt.total_seconds() / (365 * 24 * 60 * 60)\n",
    "    \n",
    "#     # Min-Max normalization for channel metrics\n",
    "#     Cleaned_File['channelNormalizedViewCount'] = normalize(Cleaned_File['channelViewCount'])\n",
    "#     Cleaned_File['channelNormalizedSubscriberCount'] = normalize(Cleaned_File['channelSubscriberCount'])\n",
    "#     Cleaned_File['channelNormalizedVideoCount'] = normalize(Cleaned_File['channelVideoCount'])\n",
    "#     Cleaned_File['channelNormalizedChannelAge'] = normalize(Cleaned_File['channelAgeInYears'])\n",
    "    \n",
    "#     # Define weights for each metric in the growth score calculation\n",
    "#     weight_views = 50\n",
    "#     weight_subscribers = 30\n",
    "#     weight_videos = 20\n",
    "    \n",
    "#     # Growth Score Calculation for the channel\n",
    "#     Cleaned_File['channelGrowthScore'] = (\n",
    "#         (Cleaned_File['channelNormalizedViewCount'] * weight_views) +\n",
    "#         (Cleaned_File['channelNormalizedSubscriberCount'] * weight_subscribers) +\n",
    "#         (Cleaned_File['channelNormalizedVideoCount'] * weight_videos)\n",
    "#     ) / (Cleaned_File['channelNormalizedChannelAge'] + 1e-6)  # Avoid division by zero\n",
    "    \n",
    "#     # Video Age Calculation (in days)\n",
    "#     videoPublishedOn = Cleaned_File[\"videoPublishedOn\"].apply(parse_datetime)\n",
    "#     Cleaned_File[\"videoAgeInDays\"] = (current_ist_time - videoPublishedOn).dt.total_seconds() / (24 * 60 * 60)\n",
    "    \n",
    "#     # Engagement Metrics for videos\n",
    "#     Cleaned_File[\"videoViewsPerDay\"] = Cleaned_File[\"videoViewCount\"] / (Cleaned_File[\"videoAgeInDays\"] + 1e-6)  # Avoid division by zero\n",
    "#     Cleaned_File[\"videoLikeToViewRatio\"] = Cleaned_File[\"videoLikeCount\"] / (Cleaned_File[\"videoViewCount\"] + 1e-6)\n",
    "#     Cleaned_File[\"videoCommentToViewRatio\"] = Cleaned_File[\"videoCommentCount\"] / (Cleaned_File[\"videoViewCount\"] + 1e-6)\n",
    "    \n",
    "#     # Engagement Score Calculation for the video\n",
    "#     Cleaned_File[\"videoEngagementScore\"] = (\n",
    "#         (Cleaned_File[\"videoViewsPerDay\"] * 50) +\n",
    "#         (Cleaned_File[\"videoLikeToViewRatio\"] * 100 * 30) +\n",
    "#         (Cleaned_File[\"videoCommentToViewRatio\"] * 100 * 20)\n",
    "#     )\n",
    "    \n",
    "#     # Return the dataframe with added growth and engagement scores\n",
    "#     return Cleaned_File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd40457f",
   "metadata": {
    "_cell_guid": "75e083cb-9bb2-4516-a996-62845763af34",
    "_uuid": "4fb259e3-b869-4973-a412-084570a472e8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-10T14:28:36.231792Z",
     "iopub.status.busy": "2025-09-10T14:28:36.231384Z",
     "iopub.status.idle": "2025-09-10T14:28:36.242785Z",
     "shell.execute_reply": "2025-09-10T14:28:36.241693Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.019053,
     "end_time": "2025-09-10T14:28:36.244548",
     "exception": false,
     "start_time": "2025-09-10T14:28:36.225495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_channel_growth(df):\n",
    "    \"\"\"\n",
    "    Calculates growth and engagement scores for YouTube channels and videos.\n",
    "    Fixes log transformation, MinMax Scaling, and division issues.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure datetime columns are properly parsed\n",
    "    df[\"channelPublishedOn\"] = pd.to_datetime(df[\"channelPublishedOn\"], errors=\"coerce\")\n",
    "    df[\"videoPublishedOn\"] = pd.to_datetime(df[\"videoPublishedOn\"], errors=\"coerce\")\n",
    "\n",
    "    # Convert timestamps to timezone-naive\n",
    "    df[\"channelPublishedOn\"] = df[\"channelPublishedOn\"].dt.tz_localize(None)\n",
    "    df[\"videoPublishedOn\"] = df[\"videoPublishedOn\"].dt.tz_localize(None)\n",
    "\n",
    "    # Get current IST time and make it timezone-naive\n",
    "    current_ist_time = datetime.datetime.now(pytz.timezone(\"Asia/Kolkata\")).astimezone(pytz.utc).replace(tzinfo=None)\n",
    "\n",
    "    # Channel Age Calculation (in years)\n",
    "    df[\"channelAgeInYears\"] = (current_ist_time - df[\"channelPublishedOn\"]).dt.total_seconds() / (365 * 24 * 60 * 60)\n",
    "    df[\"channelAgeInYears\"] = df[\"channelAgeInYears\"].fillna(1).clip(lower=1)  # Ensure minimum age is 1\n",
    "\n",
    "    # Apply log transformation before Min-Max Scaling\n",
    "    df[\"logChannelViewCount\"] = safe_log_transform(df[\"channelViewCount\"])\n",
    "    df[\"logChannelSubscriberCount\"] = safe_log_transform(df[\"channelSubscriberCount\"])\n",
    "    df[\"logChannelVideoCount\"] = safe_log_transform(df[\"channelVideoCount\"])\n",
    "    df[\"logChannelAgeInYears\"] = safe_log_transform(df[\"channelAgeInYears\"])\n",
    "\n",
    "    # Normalize using Min-Max Scaling\n",
    "    df[\"channelNormalizedViewCount\"] = normalize(df[\"logChannelViewCount\"])\n",
    "    df[\"channelNormalizedSubscriberCount\"] = normalize(df[\"logChannelSubscriberCount\"])\n",
    "    df[\"channelNormalizedVideoCount\"] = normalize(df[\"logChannelVideoCount\"])\n",
    "    df[\"channelNormalizedChannelAge\"] = normalize(df[\"logChannelAgeInYears\"])\n",
    "\n",
    "    # Ensure minimum value for channel age normalization to avoid division by near-zero values\n",
    "    df[\"channelNormalizedChannelAge\"] = df[\"channelNormalizedChannelAge\"].clip(lower=0.1)\n",
    "\n",
    "    # Growth Score Calculation\n",
    "    weight_views, weight_subscribers, weight_videos = 50, 30, 20\n",
    "    df[\"channelGrowthScore\"] = (\n",
    "        (df[\"channelNormalizedViewCount\"] * weight_views) +\n",
    "        (df[\"channelNormalizedSubscriberCount\"] * weight_subscribers) +\n",
    "        (df[\"channelNormalizedVideoCount\"] * weight_videos)\n",
    "    ) / df[\"channelNormalizedChannelAge\"]  # Avoids division errors\n",
    "\n",
    "    # Video Age Calculation (in days)\n",
    "    df[\"videoAgeInDays\"] = (current_ist_time - df[\"videoPublishedOn\"]).dt.total_seconds() / (24 * 60 * 60)\n",
    "    df[\"videoAgeInDays\"] = df[\"videoAgeInDays\"].fillna(1).clip(lower=1)  # Ensure minimum age is 1\n",
    "\n",
    "    # Log Transformation for Video Engagement Metrics\n",
    "    df[\"logVideoViewCount\"] = safe_log_transform(df[\"videoViewCount\"])\n",
    "    df[\"logVideoLikeCount\"] = safe_log_transform(df[\"videoLikeCount\"])\n",
    "    df[\"logVideoCommentCount\"] = safe_log_transform(df[\"videoCommentCount\"])\n",
    "    df[\"logVideoAgeInDays\"] = safe_log_transform(df[\"videoAgeInDays\"])\n",
    "\n",
    "    # Engagement Metrics\n",
    "    df[\"videoViewsPerDay\"] = df[\"logVideoViewCount\"] / df[\"logVideoAgeInDays\"]\n",
    "    df[\"videoLikeToViewRatio\"] = df[\"logVideoLikeCount\"] / (df[\"logVideoViewCount\"] + 1e-6)\n",
    "    df[\"videoCommentToViewRatio\"] = df[\"logVideoCommentCount\"] / (df[\"logVideoViewCount\"] + 1e-6)\n",
    "\n",
    "    # Normalize Engagement Metrics\n",
    "    df[\"normalizedVideoViewsPerDay\"] = normalize(df[\"videoViewsPerDay\"])\n",
    "    df[\"normalizedVideoLikeToViewRatio\"] = normalize(df[\"videoLikeToViewRatio\"])\n",
    "    df[\"normalizedVideoCommentToViewRatio\"] = normalize(df[\"videoCommentToViewRatio\"])\n",
    "\n",
    "    # Engagement Score Calculation\n",
    "    df[\"videoEngagementScore\"] = (\n",
    "        (df[\"normalizedVideoViewsPerDay\"] * 50) +\n",
    "        (df[\"normalizedVideoLikeToViewRatio\"] * 100 * 30) +\n",
    "        (df[\"normalizedVideoCommentToViewRatio\"] * 100 * 20)\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ba84015",
   "metadata": {
    "_cell_guid": "d101f38f-584a-40b6-8533-6ea1ed98e4dc",
    "_uuid": "38f48351-b9b4-4143-8e8a-41befb2868c6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-10T14:28:36.255809Z",
     "iopub.status.busy": "2025-09-10T14:28:36.255417Z",
     "iopub.status.idle": "2025-09-10T14:28:36.262243Z",
     "shell.execute_reply": "2025-09-10T14:28:36.261056Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.01427,
     "end_time": "2025-09-10T14:28:36.263936",
     "exception": false,
     "start_time": "2025-09-10T14:28:36.249666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def HierarchicalWeightRanking(Cleaned_File):\n",
    "    \"\"\"\n",
    "    Assigns rankings to channels and videos based on their growth and engagement scores.\n",
    "    \n",
    "    - Channels are ranked only once to prevent duplication.\n",
    "    - Videos are ranked individually.\n",
    "    \n",
    "    Parameters:\n",
    "    Cleaned_File (pd.DataFrame): The input DataFrame containing channel and video data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with added ranking columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Rank channels uniquely (consider only one entry per channel)\n",
    "    channel_ranking_df = Cleaned_File.drop_duplicates(subset=['channelId']).copy()\n",
    "\n",
    "    # Sort by channel growth-related features\n",
    "    sort_orderby_columns = ['channelGrowthScore', 'channelNormalizedViewCount', 'channelViewCount',\n",
    "                            'channelNormalizedSubscriberCount', 'channelSubscriberCount', \n",
    "                            'channelNormalizedVideoCount', 'channelVideoCount', \n",
    "                            'channelNormalizedChannelAge', 'channelAgeInYears']\n",
    "    ascending_bool = [False] * len(sort_orderby_columns)\n",
    "\n",
    "    channel_ranking_df = channel_ranking_df.sort_values(by=sort_orderby_columns, ascending=ascending_bool)\n",
    "    channel_ranking_df[\"channelGrowthScoreRank\"] = range(1, len(channel_ranking_df) + 1)\n",
    "\n",
    "    # Merge the unique channel ranks back to the original DataFrame\n",
    "    Cleaned_File = Cleaned_File.merge(channel_ranking_df[['channelId', 'channelGrowthScoreRank']], on='channelId', how='left')\n",
    "\n",
    "    # Rank videos normally (since they are unique)\n",
    "    sort_orderby_columns = ['videoEngagementScore', 'videoViewsPerDay', 'videoViewCount', \n",
    "                            'videoLikeToViewRatio', 'videoLikeCount', 'videoCommentToViewRatio', \n",
    "                            'videoCommentCount', 'videoAgeInDays']\n",
    "    ascending_bool = [False, False, False, False, False, False, False, True]\n",
    "\n",
    "    Cleaned_File = Cleaned_File.sort_values(by=sort_orderby_columns, ascending=ascending_bool)\n",
    "    Cleaned_File[\"videoEngagementScoreRank\"] = range(1, len(Cleaned_File) + 1)\n",
    "\n",
    "    return Cleaned_File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "678e7a6f",
   "metadata": {
    "_cell_guid": "2e2fab3f-76dc-4a3a-b573-de6876adf578",
    "_uuid": "00bf365e-98e3-491c-a887-a51c73430f78",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-10T14:28:36.275564Z",
     "iopub.status.busy": "2025-09-10T14:28:36.275187Z",
     "iopub.status.idle": "2025-09-10T14:28:36.282331Z",
     "shell.execute_reply": "2025-09-10T14:28:36.281223Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.015048,
     "end_time": "2025-09-10T14:28:36.284108",
     "exception": false,
     "start_time": "2025-09-10T14:28:36.269060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def FeatureEngineering(Cleaned_File):\n",
    "    \"\"\"\n",
    "    This function performs feature engineering to enhance the dataset for analysis by creating new features \n",
    "    and transforming existing ones, such as categorizing video duration, calculating channel growth and \n",
    "    video engagement scores, and enriching geographic details.\n",
    "\n",
    "    The key steps include:\n",
    "    - Extracting the day of the week from the video publish timestamp.\n",
    "    - Classifying video durations into predefined categories.\n",
    "    - Calculating channel growth and video engagement scores.\n",
    "    - Ranking channels and videos based on their growth and engagement scores.\n",
    "    - Merging geographic details like country, continent, and IT hub information with the dataset.\n",
    "    \n",
    "    Args:\n",
    "    - Cleaned_File (pd.DataFrame): The input dataframe containing video and channel data for feature engineering.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The transformed dataframe with newly engineered features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Feature: videoPublishedWeekDay - Derive the day of the week from the videoPublishedOn timestamp.\n",
    "    Cleaned_File['videoPublishedWeekDay'] = pd.to_datetime(Cleaned_File[\"videoPublishedOn\"]).dt.day_name()\n",
    "    \n",
    "    # Feature: videoDurationClassification - Categorize videos based on their duration in seconds into predefined segments.\n",
    "    # Categories:\n",
    "    #     Very Short (0 - 60 sec), Short (61 sec - 2 min), Medium (2 min 1 sec - 5 min),\n",
    "    #     Long (5 min 1 sec - 10 min), Very Long (10 min 1 sec - 1 hour),\n",
    "    #     Extended (1 hour 1 sec - 3 hours), Ultra Long (3 hours 1 sec and above)\n",
    "    Cleaned_File['videoDurationClassification'] = Cleaned_File['videoDurationInSeconds'].apply(videoDurationClassification)\n",
    "    \n",
    "    # Feature: channelGrowth metric - Calculate channel growth using factors such as views, subscribers, video count, and age.\n",
    "    # Normalization of key columns: channelPublishedOn, channelViewCount, channelSubscriberCount, and channelVideoCount\n",
    "    Cleaned_File = calculate_channel_growth(Cleaned_File)\n",
    "    \n",
    "    # Feature: videoEngagementScore - Calculate the video engagement score using video views, likes, and comments.\n",
    "    # Normalization of key columns: videoPublishedOn, videoViewCount, videoLikeCount, and videoCommentCount\n",
    "    Cleaned_File = calculate_channel_growth(Cleaned_File)  # This also handles the video engagement scores\n",
    "    \n",
    "    # Feature: channelGrowthScoreRank - Rank channels based on their growth score.\n",
    "    # Feature: videoEngagementScoreRank - Rank videos based on their engagement score.\n",
    "    Cleaned_File = HierarchicalWeightRanking(Cleaned_File)\n",
    "    \n",
    "    # Feature: Geographic Classification - Enrich dataset with geographic details (country, continent, IT hub classification).\n",
    "    # This merges additional country and continent details from an external source based on the channel's country.\n",
    "    \n",
    "    # Fetch geographic details (ISO codes, country names, continent, etc.) from an external file\n",
    "    Country_Details_ISO = Requirement_File_Extraction(repo_url, kaggle_repo_url, requirement_path).transpose()\n",
    "    Country_Details_ISO = Country_Details_ISO.reset_index()\n",
    "    Country_Details_ISO.rename(columns={'index': 'country_code'}, inplace=True)\n",
    "    \n",
    "    # Merge geographic details (from Country_Details_ISO) with the cleaned file\n",
    "    resultDataFrame = pd.merge(Cleaned_File, Country_Details_ISO, left_on='channelCountry', right_on='country_code', how='left')\n",
    "    \n",
    "    # Fill missing geographic data with 'Unknown' (in case a country code doesn't match)\n",
    "    cols_to_fill = ['country_code', 'country_name', 'continent', 'continent_code', 'it_hub_country']\n",
    "    resultDataFrame[cols_to_fill] = resultDataFrame[cols_to_fill].fillna('Unknown')\n",
    "    \n",
    "    # Return the enriched dataframe with new features\n",
    "    return resultDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d94cd2c7",
   "metadata": {
    "_cell_guid": "e84b5023-10d8-48d7-8c77-df70bbcd46cb",
    "_uuid": "05a6e9b4-a56c-46a7-b275-cde2c7f8bc9e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-10T14:28:36.295697Z",
     "iopub.status.busy": "2025-09-10T14:28:36.295275Z",
     "iopub.status.idle": "2025-09-10T14:28:36.302429Z",
     "shell.execute_reply": "2025-09-10T14:28:36.301316Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.015013,
     "end_time": "2025-09-10T14:28:36.304215",
     "exception": false,
     "start_time": "2025-09-10T14:28:36.289202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GitHubPush(Feature_File):\n",
    "    \"\"\"\n",
    "    Saves a DataFrame as a JSON file and pushes it to a GitHub repository.\n",
    "\n",
    "    This function:\n",
    "    - Counts the number of records in the DataFrame.\n",
    "    - Generates a unique filename using a timestamp in IST (Indian Standard Time) and the record count.\n",
    "    - Saves the DataFrame as a JSON file in a readable format.\n",
    "    - Checks if the destination directory exists; if not, creates it.\n",
    "    - Copies the saved file to the destination directory.\n",
    "    - Commits and pushes the file to a GitHub repository.\n",
    "\n",
    "    Parameters:\n",
    "    Feature_File (pd.DataFrame): The DataFrame to be saved and pushed.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Count the number of records in the DataFrame\n",
    "    record_count = len(Feature_File)\n",
    "    \n",
    "    # Generate a timestamp for the filename using the current time in IST (Indian Standard Time)\n",
    "    timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "    \n",
    "    # Create a unique filename using the timestamp and number of records\n",
    "    filename = f\"FE_{timestamp}_{record_count}_records.json\"\n",
    "    \n",
    "    # Save the DataFrame as a JSON file with indentation for readability\n",
    "    Feature_File.to_json(filename, orient=\"records\", indent=4)\n",
    "    print(f\"DataFrame saved as {filename}\")\n",
    "    \n",
    "    # Check if the destination directory exists\n",
    "    if not os.path.exists(destination_path):\n",
    "        # Create the directory if it does not exist\n",
    "        os.makedirs(destination_path)\n",
    "        print(\"Created the destination directory: FeatureEngineering/Daily\")\n",
    "    \n",
    "    # Copy the saved file to the destination directory\n",
    "    shutil.copyfile(f'/kaggle/working/{filename}', f'{destination_path}/{filename}')\n",
    "    \n",
    "    # Initialize the local Git repository\n",
    "    repo = Repo(kaggle_repo_url)\n",
    "    \n",
    "    # Add the copied file to the Git staging area\n",
    "    repo.index.add([f\"{destination_path}/{filename}\"])\n",
    "    \n",
    "    # Create a commit message including the timestamp and filename\n",
    "    timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    repo.index.commit(f\"{timestamp} Added files from Kaggle notebook, {filename}\")\n",
    "    \n",
    "    # Push the committed changes to the remote GitHub repository\n",
    "    origin = repo.remote(name=\"origin\")\n",
    "    # push_result = origin.push()\n",
    "    push_result = origin.push(refspec=f\"HEAD:refs/heads/feature-engineering\")\n",
    "    \n",
    "    # Check if the push was successful and print the result\n",
    "    if push_result:\n",
    "        print(\"Push successful.\")\n",
    "    else:\n",
    "        print(\"Push failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9abe6abd",
   "metadata": {
    "_cell_guid": "eb5b0ecf-4f23-4c97-b5d4-ee11630dfbdb",
    "_uuid": "954dd087-fe13-47da-8673-59ef73dbb427",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-10T14:28:36.315658Z",
     "iopub.status.busy": "2025-09-10T14:28:36.315255Z",
     "iopub.status.idle": "2025-09-10T14:28:36.320234Z",
     "shell.execute_reply": "2025-09-10T14:28:36.319185Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012695,
     "end_time": "2025-09-10T14:28:36.322011",
     "exception": false,
     "start_time": "2025-09-10T14:28:36.309316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the data extraction, feature engineering, and GitHub push process.\n",
    "\n",
    "    Steps:\n",
    "    1. Extracts the cleaned data file from the repository using the provided URL and path.\n",
    "    2. Applies feature engineering to enhance the cleaned data.\n",
    "    3. Pushes the processed and feature-engineered data to GitHub.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Extract the cleaned data file from the repository using the provided URL and path.\n",
    "    DataCleaning_File = DataCleaning_File_Extraction(repo_url, kaggle_repo_url, DataCleaning_path)\n",
    "    \n",
    "    # Optional: Uncomment the following line to display the cleaned file sorted by video duration.\n",
    "    # display(DataCleaning_File.sort_values(by='videoDurationInSeconds', ascending=True))\n",
    "    \n",
    "    # Step 2: Apply feature engineering transformations to the cleaned data.\n",
    "    Feature_File = FeatureEngineering(DataCleaning_File)\n",
    "    \n",
    "    # Optional: Uncomment the following line to display the feature-engineered file.\n",
    "    # display(Feature_File)\n",
    "    \n",
    "    # Step 3: Push the processed and feature-engineered data to GitHub.\n",
    "    GitHubPush(Feature_File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3847fcf2",
   "metadata": {
    "_cell_guid": "41ae9cd4-01dc-489e-8b0e-dbd999fa0c17",
    "_uuid": "a8001338-eeff-4563-b706-b7f30632e1ae",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-10T14:28:36.333540Z",
     "iopub.status.busy": "2025-09-10T14:28:36.333165Z",
     "iopub.status.idle": "2025-09-10T14:28:48.554273Z",
     "shell.execute_reply": "2025-09-10T14:28:48.552691Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 12.228983,
     "end_time": "2025-09-10T14:28:48.556154",
     "exception": false,
     "start_time": "2025-09-10T14:28:36.327171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully cloned the repository.\n",
      "Already cloned and the repo file exists\n",
      "Successfully pulled the git repo before push\n",
      "DataFrame saved as FE_2025-09-10_19_58_48_428_records.json\n",
      "Push successful.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Entry point for the data pipeline execution. \n",
    "\n",
    "    This script:\n",
    "    - Imports necessary libraries for file handling, Git operations, time management, data manipulation, and Kaggle secret access.\n",
    "    - Retrieves the GitHub repository URL from Kaggle secrets.\n",
    "    - Sets up the Indian Standard Time (IST) timezone for consistent timestamping.\n",
    "    - Defines paths for various directories used in the pipeline, including repositories, data cleaning, and feature engineering storage.\n",
    "    - Configures pandas to display all columns and rows for better debugging.\n",
    "    - Calls the `main()` function to execute the full data pipeline, including data extraction, feature engineering, and pushing data to GitHub.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Import necessary libraries\n",
    "    import os  # For file and directory operations\n",
    "    import git  # For interacting with Git repositories\n",
    "    from git import Repo  # For handling GitHub repository interactions\n",
    "    import time  # For time-related operations\n",
    "    import datetime  # For date and time manipulations\n",
    "    import pytz  # For timezone handling\n",
    "    from pytz import timezone  # To manage different timezones\n",
    "    import pandas as pd  # For data manipulation and analysis\n",
    "    import numpy as np\n",
    "    import shutil  # For file operations like copying and removing files\n",
    "    from kaggle_secrets import UserSecretsClient  # For securely accessing secrets in Kaggle\n",
    "    from sklearn.preprocessing import MinMaxScaler # Using MinMaxScaler for efficient and consistent normalization\n",
    "\n",
    "    # Retrieve the GitHub repository URL stored in Kaggle's secret management system\n",
    "    user_secrets = UserSecretsClient()\n",
    "    secret_value_0 = user_secrets.get_secret(\"featureEngineeringRepoUrl\")\n",
    "    repo_url = secret_value_0  # URL for the GitHub repository used in this pipeline\n",
    "\n",
    "    # Set the timezone to Indian Standard Time (IST) for consistent timestamping\n",
    "    ist = timezone('Asia/Kolkata')\n",
    "\n",
    "    # Define paths for various directories used in the data pipeline\n",
    "    kaggle_repo_url = '/kaggle/working/DevOps-YouTube-Trends'  # Local path to the cloned GitHub repository\n",
    "    destination_path = '/kaggle/working/DevOps-YouTube-Trends/FeatureEngineering/Daily'  # Directory for storing feature-engineered data\n",
    "    DataCleaning_path = '/kaggle/working/DevOps-YouTube-Trends/DataCleaning/Daily'  # Directory for cleaned data files\n",
    "    requirement_path = '/kaggle/working/DevOps-YouTube-Trends/Requirement/Daily'  # Directory for requirement-related files\n",
    "\n",
    "    # Configure pandas display settings to show all columns and rows for better visibility during debugging\n",
    "    pd.set_option(\"display.max_columns\", None)  # Display all columns without truncation\n",
    "    pd.set_option(\"display.max_rows\", None)  # Display all rows without truncation\n",
    "\n",
    "    # Execute the main function to run the data pipeline\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16.613998,
   "end_time": "2025-09-10T14:28:49.283467",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-10T14:28:32.669469",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
