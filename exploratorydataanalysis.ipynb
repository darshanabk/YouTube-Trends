{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06dc92d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T13:37:55.112039Z",
     "iopub.status.busy": "2025-03-05T13:37:55.111690Z",
     "iopub.status.idle": "2025-03-05T13:37:55.118514Z",
     "shell.execute_reply": "2025-03-05T13:37:55.117733Z"
    },
    "papermill": {
     "duration": 0.01315,
     "end_time": "2025-03-05T13:37:55.120303",
     "exception": false,
     "start_time": "2025-03-05T13:37:55.107153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# import logging\n",
    "# from git import Repo\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from datetime import datetime\n",
    "# from pytz import timezone\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# import psutil\n",
    "# print(f\"RAM Usage: {psutil.virtual_memory().percent}%\")\n",
    "\n",
    "\n",
    "# # Configure Logging\n",
    "# logging.basicConfig(\n",
    "#     filename=\"pipeline.log\",\n",
    "#     level=logging.INFO,\n",
    "#     format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "# )\n",
    "\n",
    "# def load_data(file_path):\n",
    "#     \"\"\"Load dataset from JSON file.\"\"\"\n",
    "#     try:\n",
    "#         df = pd.read_json(file_path)\n",
    "#         logging.info(f\"Dataset loaded successfully from {file_path}\")\n",
    "#         return df\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Failed to load dataset: {e}\")\n",
    "#         raise\n",
    "\n",
    "\n",
    "# def pre_eda_validation(df, batch_size=1000):\n",
    "#     \"\"\"Perform pre-EDA validation in batches.\"\"\"\n",
    "#     try:\n",
    "#         total_batches = (len(df) // batch_size) + 1\n",
    "#         report_list = []\n",
    "\n",
    "#         logging.info(f\"Processing DataFrame in {total_batches} batches...\")\n",
    "\n",
    "#         for i in range(total_batches):\n",
    "#             batch = df.iloc[i * batch_size:(i + 1) * batch_size]\n",
    "#             batch_report = {\n",
    "#                 'missing_values': batch.isnull().sum(),\n",
    "#                 'duplicates': batch.duplicated().sum(),\n",
    "#                 'data_types': batch.dtypes.astype(str),\n",
    "#                 'cardinality': batch.nunique()\n",
    "#             }\n",
    "#             report_list.append(pd.DataFrame(batch_report))\n",
    "\n",
    "#         final_report = pd.concat(report_list).reset_index(drop=True)\n",
    "\n",
    "#         if not final_report.empty:\n",
    "#             timestamp = datetime.now(ist).strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "#             filename = f\"PEDA_{timestamp}_{len(final_report)}_records.json\"\n",
    "#             final_report.to_json(filename, orient=\"index\", indent=4)\n",
    "#             logging.info(f\"DataFrame saved: {filename}\")\n",
    "#             return filename\n",
    "#         else:\n",
    "#             logging.info(\"No data to save since empty DataFrame returned.\")\n",
    "#             return None\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Pre-EDA validation failed: {e}\")\n",
    "#         raise\n",
    "\n",
    "\n",
    "\n",
    "# def detect_outliers_iqr(df, column):\n",
    "#     \"\"\"Detect and handle outliers using IQR method.\"\"\"\n",
    "#     try:\n",
    "#         Q1 = df[column].quantile(0.25)\n",
    "#         Q3 = df[column].quantile(0.75)\n",
    "#         IQR = Q3 - Q1\n",
    "#         lower_bound = max(Q1 - 1.5 * IQR, df[column].min())\n",
    "#         upper_bound = min(Q3 + 1.5 * IQR, df[column].max())\n",
    "#         df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
    "#         df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
    "#         logging.info(f\"Outliers handled in column: {column}\")\n",
    "#         return df\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Failed to detect outliers in {column}: {e}\")\n",
    "#         raise\n",
    "\n",
    "# # def GitHubPush(file, filename, kaggle_repo_url, destination_path):\n",
    "# #     try:\n",
    "# #         if not os.path.exists(destination_path):\n",
    "# #             os.makedirs(destination_path)\n",
    "# #             logging.info(f\"Created directory: {destination_path}\")\n",
    "\n",
    "# #         record_count = len(file)\n",
    "# #         timestamp = datetime.now(ist).strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "# #         filename = f\"{filename}_{timestamp}_{record_count}_records.json\"\n",
    "# #         file.to_json(filename, orient=\"records\", indent=4)\n",
    "# #         shutil.copyfile(filename, f\"{destination_path}/{filename}\")\n",
    "\n",
    "# #         repo = Repo(kaggle_repo_url)\n",
    "# #         repo.index.add([f\"{destination_path}/{filename}\"])\n",
    "# #         repo.index.commit(f\"{timestamp} - Added {filename}\")\n",
    "# #         repo.remote(name=\"origin\").push()\n",
    "# #         logging.info(f\"Pushed file: {filename} to GitHub\")\n",
    "\n",
    "# #     except Exception as e:\n",
    "# #         logging.error(f\"GitHub push failed: {e}\")\n",
    "# #         raise\n",
    "\n",
    "# def main(df, destination_path, kaggle_repo_url):\n",
    "#     try:\n",
    "#         report = pre_eda_validation(df)\n",
    "#         # GitHubPush(report, \"PEDA\", kaggle_repo_url, destination_path)\n",
    "\n",
    "#         numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "#         for col in numeric_cols:\n",
    "#             df = detect_outliers_iqr(df, col)\n",
    "\n",
    "#         duplicates = df[df.duplicated('videoTitle', keep=False)][['videoTitle', 'channelName']]\n",
    "#         logging.info(f\"Duplicate Titles Found: {len(duplicates)}\")\n",
    "#         print(duplicates)\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Pipeline execution failed: {e}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         user_secrets = UserSecretsClient()\n",
    "#         name = user_secrets.get_secret(\"EDARepoOwner\")\n",
    "#         email = user_secrets.get_secret(\"EDARepoOwnerMail\")\n",
    "#         repo_url = user_secrets.get_secret(\"EDARepoURL\")\n",
    "        \n",
    "#         kaggle_repo_url = '/kaggle/working/DevOps-YouTube-Trends'\n",
    "#         dataset_path = '/kaggle/working/DevOps-YouTube-Trends/FeatureEngineering/Daily'\n",
    "#         destination_path = '/kaggle/working/DevOps-YouTube-Trends/ExploratoryDataAnalysis/Daily'\n",
    "\n",
    "#         ist = timezone('Asia/Kolkata')\n",
    "#         year = datetime.now(ist).strftime(\"%Y\")\n",
    "#         today = datetime.now(ist).strftime(\"%Y-%m-%d\")\n",
    "#         destination_path = f\"{destination_path}/{year}/{today}\"\n",
    "\n",
    "#         if not os.path.exists(kaggle_repo_url):\n",
    "#             Repo.clone_from(repo_url, kaggle_repo_url)\n",
    "#             logging.info(\"Repository cloned successfully.\")\n",
    "#         else:\n",
    "#             repo = Repo(kaggle_repo_url)\n",
    "#             repo.remote(name=\"origin\").pull()\n",
    "#             logging.info(\"Repository updated successfully.\")\n",
    "\n",
    "#         datasets = os.listdir(dataset_path)\n",
    "#         Target_Dataset = max([d for d in datasets if d.startswith(\"FE_\") and d.endswith('records.json')])\n",
    "#         file_path = os.path.join(dataset_path, Target_Dataset)\n",
    "#         df = load_data(file_path)\n",
    "#         main(df, destination_path, kaggle_repo_url)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.critical(f\"Main pipeline execution failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a8268b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T13:37:55.127422Z",
     "iopub.status.busy": "2025-03-05T13:37:55.127112Z",
     "iopub.status.idle": "2025-03-05T13:37:56.233090Z",
     "shell.execute_reply": "2025-03-05T13:37:56.231991Z"
    },
    "papermill": {
     "duration": 1.111413,
     "end_time": "2025-03-05T13:37:56.235021",
     "exception": false,
     "start_time": "2025-03-05T13:37:55.123608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from git import Repo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "import json\n",
    "from kaggle_secrets import UserSecretsClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eff1ce8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T13:37:56.242252Z",
     "iopub.status.busy": "2025-03-05T13:37:56.241768Z",
     "iopub.status.idle": "2025-03-05T13:37:56.249984Z",
     "shell.execute_reply": "2025-03-05T13:37:56.249062Z"
    },
    "papermill": {
     "duration": 0.013377,
     "end_time": "2025-03-05T13:37:56.251432",
     "exception": false,
     "start_time": "2025-03-05T13:37:56.238055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def FeatureEngineering_File_Extraction(repo_url, kaggle_repo_url, FeatureEngineering_path):\n",
    "    # Check if the repository already exists locally\n",
    "    if os.path.exists(kaggle_repo_url):\n",
    "        print(\"Repository already exists locally.\")\n",
    "        repo = Repo(kaggle_repo_url)  # Access the existing repository\n",
    "        repo.config_writer().set_value(\"user\", \"name\", name).release()\n",
    "        repo.config_writer().set_value(\"user\", \"email\", email).release()\n",
    "        origin = repo.remote(name='origin')  # Get the remote repository reference\n",
    "        origin.pull()  # Pull the latest updates from the remote repository\n",
    "        print(\"Successfully pulled the latest changes.\")\n",
    "    else:\n",
    "        # Clone the repository if it does not exist\n",
    "        repo = Repo.clone_from(repo_url, kaggle_repo_url)\n",
    "        repo.config_writer().set_value(\"user\", \"name\", name).release()\n",
    "        repo.config_writer().set_value(\"user\", \"email\", email).release()\n",
    "        print(\"Successfully cloned the repository.\")\n",
    "\n",
    "    # List all files in the specified directory and filter for the relevant JSON file\n",
    "    output_files = os.listdir(FeatureEngineering_path)\n",
    "    FeatureEngineering_File = max(\n",
    "        [file for file in output_files if file.startswith(\"FE_\") and file.endswith('records.json')]\n",
    "    )\n",
    "\n",
    "    # Read the identified JSON file into a pandas DataFrame\n",
    "    FeatureEngineering_File = pd.read_json(os.path.join(FeatureEngineering_path, FeatureEngineering_File))\n",
    "\n",
    "    return FeatureEngineering_File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "958d0876",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T13:37:56.257996Z",
     "iopub.status.busy": "2025-03-05T13:37:56.257704Z",
     "iopub.status.idle": "2025-03-05T13:37:56.265053Z",
     "shell.execute_reply": "2025-03-05T13:37:56.264216Z"
    },
    "papermill": {
     "duration": 0.012115,
     "end_time": "2025-03-05T13:37:56.266417",
     "exception": false,
     "start_time": "2025-03-05T13:37:56.254302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def PushToGithub(filename,destination_path):\n",
    "    try:\n",
    "        if os.path.exists(kaggle_repo_url):\n",
    "            print(\"Already cloned and the repo file exists\")\n",
    "            repo = Repo(kaggle_repo_url)\n",
    "            repo.config_writer().set_value(\"user\", \"name\", name).release()\n",
    "            repo.config_writer().set_value(\"user\", \"email\", email).release()\n",
    "            origin = repo.remote(name='origin')\n",
    "            origin.pull()\n",
    "            print(\"Successfully pulled the git repo before push\")\n",
    "        else:\n",
    "            repo = Repo.clone_from(repo_url, kaggle_repo_url)\n",
    "            repo.config_writer().set_value(\"user\", \"name\", name).release()\n",
    "            repo.config_writer().set_value(\"user\", \"email\", email).release()\n",
    "            print(\"Successfully cloned the git repo\")\n",
    "        \n",
    "        if os.path.exists(destination_path):\n",
    "            shutil.copyfile(f'/kaggle/working/{filename}', f'{destination_path}/{filename}')\n",
    "        else:\n",
    "            os.makedirs(destination_path)\n",
    "            shutil.copyfile(f'/kaggle/working/{filename}', f'{destination_path}/{filename}')\n",
    "        \n",
    "        repo = Repo(kaggle_repo_url)\n",
    "        repo.index.add([f\"{destination_path}/{filename}\"])\n",
    "        timestamp = datetime.now(ist).strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "        repo.index.commit(f\"{timestamp} Added files from Kaggle notebook, {filename}\")\n",
    "        origin = repo.remote(name=\"origin\")\n",
    "        push_result = origin.push()\n",
    "        \n",
    "        if push_result:\n",
    "            print(\"Output files successfully pushed to GitHub!\")\n",
    "        else:\n",
    "            print(\"Output files pushed to GitHub failed:(\")\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred at git automation code: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "835744e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T13:37:56.272929Z",
     "iopub.status.busy": "2025-03-05T13:37:56.272642Z",
     "iopub.status.idle": "2025-03-05T13:37:56.276287Z",
     "shell.execute_reply": "2025-03-05T13:37:56.275434Z"
    },
    "papermill": {
     "duration": 0.00866,
     "end_time": "2025-03-05T13:37:56.277880",
     "exception": false,
     "start_time": "2025-03-05T13:37:56.269220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def pre_eda_validation(dataFrame):\n",
    "#     timestamp = datetime.now(ist).strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "#     record_count = len(dataFrame)\n",
    "\n",
    "#     report_df = pd.DataFrame({\n",
    "#         \"missing_values\": dataFrame.isnull().sum(),\n",
    "#         \"duplicates\": [dataFrame.duplicated().sum()] * len(dataFrame.columns),\n",
    "#         \"data_types\": dataFrame.dtypes.astype(str),\n",
    "#         \"cardinality\": dataFrame.nunique()\n",
    "#     }).reset_index().rename(columns={\"index\": \"columns\"})\n",
    "\n",
    "#     filename = f\"PEDA_{timestamp}_{record_count}_records.json\"\n",
    "\n",
    "#     if not report_df.empty:\n",
    "#         report_df.to_json(filename, orient=\"records\", indent=4)\n",
    "#         print(f\"DataFrame validation report saved as {filename}\")\n",
    "#     else:\n",
    "#         print(\"No data to save since empty DataFrame returned.\")\n",
    "\n",
    "#     destination_path = '/kaggle/working/DevOps-YouTube-Trends/ExploratoryDataAnalysis/PEDA/Daily'  \n",
    "#     PushToGithub(filename,destination_path)\n",
    "        \n",
    "#     return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66aa5c0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T13:37:56.284255Z",
     "iopub.status.busy": "2025-03-05T13:37:56.283928Z",
     "iopub.status.idle": "2025-03-05T13:37:56.290977Z",
     "shell.execute_reply": "2025-03-05T13:37:56.290041Z"
    },
    "papermill": {
     "duration": 0.011798,
     "end_time": "2025-03-05T13:37:56.292456",
     "exception": false,
     "start_time": "2025-03-05T13:37:56.280658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pre_eda_validation(dataFrame):\n",
    "    timestamp = datetime.now(ist).strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "    record_count = len(dataFrame)\n",
    "\n",
    "    report_df = pd.DataFrame({\n",
    "        \"missing_values\": dataFrame.isnull().sum(),\n",
    "        \"duplicates\": [dataFrame.duplicated().sum()] * len(dataFrame.columns),\n",
    "        \"data_types\": dataFrame.dtypes.astype(str),\n",
    "        \"cardinality\": dataFrame.nunique()\n",
    "    }).reset_index().rename(columns={\"index\": \"columns\"})\n",
    "\n",
    "    # Extract inconsistent records for group1\n",
    "    inconsistent_group1 = dataFrame[dataFrame.duplicated(subset=[\"channelId\"], keep=False)][\n",
    "        [\"channelId\", \"channelName\", \"channelCustomUrl\", \"channelGrowthScoreRank\"]]\n",
    "\n",
    "    # Extract inconsistent records for group2\n",
    "    inconsistent_group2 = dataFrame[dataFrame.duplicated(subset=[\"videoId\"], keep=False)][\n",
    "        [\"videoId\", \"videoTitle\", \"videoEngagementScoreRank\"]]\n",
    "\n",
    "    report = {\n",
    "        \"Pre_EDA\": report_df.to_dict(orient=\"records\"),\n",
    "        \"inconsistent_records_group1\": inconsistent_group1.to_dict(orient=\"records\"),\n",
    "        \"inconsistent_records_group2\": inconsistent_group2.to_dict(orient=\"records\")\n",
    "    }\n",
    "\n",
    "    filename = f\"PEDA_{timestamp}_{record_count}_records.json\"\n",
    "\n",
    "    if report[\"Pre_EDA\"]:\n",
    "        with open(filename, \"w\") as json_file:\n",
    "            json.dump(report, json_file, indent=4)\n",
    "        print(f\"DataFrame validation report saved as {filename}\")\n",
    "    else:\n",
    "        print(\"No data to save since empty DataFrame returned.\")\n",
    "\n",
    "    destination_path = '/kaggle/working/DevOps-YouTube-Trends/ExploratoryDataAnalysis/PEDA/Daily'\n",
    "    PushToGithub(filename, destination_path)\n",
    "        \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a828e746",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T13:37:56.298756Z",
     "iopub.status.busy": "2025-03-05T13:37:56.298350Z",
     "iopub.status.idle": "2025-03-05T13:37:56.302583Z",
     "shell.execute_reply": "2025-03-05T13:37:56.301589Z"
    },
    "papermill": {
     "duration": 0.009084,
     "end_time": "2025-03-05T13:37:56.304219",
     "exception": false,
     "start_time": "2025-03-05T13:37:56.295135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(repo_url, kaggle_repo_url, FeatureEngineering_path, ExploratoryDataAnalysis_path):\n",
    "    FeatureEngineering_File = FeatureEngineering_File_Extraction(repo_url, kaggle_repo_url, FeatureEngineering_path)\n",
    "    pre_eda_validation(FeatureEngineering_File)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5cbe8e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T13:37:56.310691Z",
     "iopub.status.busy": "2025-03-05T13:37:56.310350Z",
     "iopub.status.idle": "2025-03-05T13:38:01.725569Z",
     "shell.execute_reply": "2025-03-05T13:38:01.724446Z"
    },
    "papermill": {
     "duration": 5.420327,
     "end_time": "2025-03-05T13:38:01.727402",
     "exception": false,
     "start_time": "2025-03-05T13:37:56.307075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully cloned the repository.\n",
      "DataFrame validation report saved as PEDA_2025-03-05_19_08_00_392_records.json\n",
      "Already cloned and the repo file exists\n",
      "Successfully pulled the git repo before push\n",
      "Output files successfully pushed to GitHub!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":    \n",
    "    user_secrets = UserSecretsClient()\n",
    "    secret_value_0 = user_secrets.get_secret(\"EDARepoOwner\")\n",
    "    secret_value_1 = user_secrets.get_secret(\"EDARepoOwnerMail\")\n",
    "    secret_value_2 = user_secrets.get_secret(\"EDARepoURL\")\n",
    "    \n",
    "    name = secret_value_0\n",
    "    email = secret_value_1\n",
    "    repo_url = secret_value_2\n",
    "    \n",
    "    kaggle_repo_url = '/kaggle/working/DevOps-YouTube-Trends'\n",
    "    FeatureEngineering_path = '/kaggle/working/DevOps-YouTube-Trends/FeatureEngineering/Daily'\n",
    "    ExploratoryDataAnalysis_path = '/kaggle/working/DevOps-YouTube-Trends/ExploratoryDataAnalysis'\n",
    "\n",
    "    ist = timezone(\"Asia/Kolkata\")\n",
    "    \n",
    "    main(repo_url, kaggle_repo_url, FeatureEngineering_path, ExploratoryDataAnalysis_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.944589,
   "end_time": "2025-03-05T13:38:02.351319",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-05T13:37:52.406730",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
