{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c24ee70a",
   "metadata": {
    "_cell_guid": "80166acd-edad-4811-a869-d38a91d87cf6",
    "_uuid": "3d8fc9ab-7ea9-4676-9a5a-3cfe6d40cac1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-05T12:56:46.623429Z",
     "iopub.status.busy": "2025-03-05T12:56:46.623078Z",
     "iopub.status.idle": "2025-03-05T12:56:46.630907Z",
     "shell.execute_reply": "2025-03-05T12:56:46.629922Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.014331,
     "end_time": "2025-03-05T12:56:46.632875",
     "exception": false,
     "start_time": "2025-03-05T12:56:46.618544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Source_File_Extraction(repo_url, kaggle_repo_url, source_path):\n",
    "    \"\"\"\n",
    "    This function checks if a specified Git repository already exists in the local system.\n",
    "    If the repository exists, it pulls the latest changes from the remote repository.\n",
    "    If the repository doesn't exist, it clones the repository from the provided URL.\n",
    "    \n",
    "    After ensuring the repository is up-to-date, it searches for a JSON file that starts with \"S_\" \n",
    "    and ends with \"records.json\" in the specified source directory, loads the file using pandas, \n",
    "    and returns the data as a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - repo_url (str): The URL of the Git repository to clone if not already present.\n",
    "    - kaggle_repo_url (str): The local path where the repository is stored or will be cloned to.\n",
    "    - source_path (str): The directory where the JSON file is stored.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The data from the JSON file as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    if os.path.exists(kaggle_repo_url):\n",
    "        print(\"Already cloned and the repo file exists\")\n",
    "        repo = git.Repo(kaggle_repo_url)  # Access the existing repository\n",
    "        origin = repo.remote(name='origin')  # Get the remote repository\n",
    "        origin.pull()  # Pull the latest changes from the repository\n",
    "        print(\"Successfully pulled the git repo before push\")\n",
    "    else:\n",
    "        # Clone the repository if it doesn't exist\n",
    "        repo = git.Repo.clone_from(repo_url, kaggle_repo_url)\n",
    "        print(\"Successfully cloned the git repo\")\n",
    "    \n",
    "    # List all files in the source path and find the relevant JSON file\n",
    "    output_files = os.listdir(source_path)\n",
    "    Source_File = max([i for i in output_files if i.startswith(\"S_\") and i.endswith('records.json')])\n",
    "    \n",
    "    # Read the found JSON file into a pandas DataFrame\n",
    "    Source_File = pd.read_json(f'{source_path}/{Source_File}')\n",
    "    \n",
    "    return Source_File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e0aabd",
   "metadata": {
    "_cell_guid": "f1875798-c4a9-452a-8611-86e2859e2bb1",
    "_uuid": "aa9d486b-bd79-49c0-bbb7-5127bceda966",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.002282,
     "end_time": "2025-03-05T12:56:46.638321",
     "exception": false,
     "start_time": "2025-03-05T12:56:46.636039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "**Observation:**  \n",
    "\n",
    "1. Null values are present in the following columns:\n",
    "   - **`videoDefaultLanguage`**  (will be dropped after data cleaning)\n",
    "   - **`videoDefaultAudioLanguage`** \n",
    "   - **`channelCountry`**\n",
    "\n",
    "---\n",
    "\n",
    "2. The following columns will be dropped as part of data cleaning:\n",
    "   - **`videoDescription`**: Reserved for analysis in future NLP project with a broader dataset.  \n",
    "   - **`videoLiveBroadcastContent`**: All values are `'none'`, providing no variability or insights. \n",
    "   - **`videoFavoriteCount`**: All values are `0`, making it redundant.  \n",
    "   - **`videoTags`**: Reserved for analysis in future NLP project with a broader dataset.  \n",
    "   - **`videoUniqueId`**: Identified as a duplicate column.  \n",
    "   - **`channelIdUnique`**: Identified as a duplicate column.  \n",
    "   - **`channelTitleCheck`**: Identified as a duplicate column.  \n",
    "   - **`channelDescription`**: Reserved for analysis in future NLP project with a broader dataset.\n",
    "---\n",
    "\n",
    "3. The columns **`channelName`** and **`videoTitle`** require further processing due to the presence of:\n",
    "    - Multilingual text.  \n",
    "    - Emojis and special characters.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bfe056a",
   "metadata": {
    "_cell_guid": "58c40ae4-a980-4861-bb62-e7d40a10e3ed",
    "_uuid": "b1738051-e9d8-44fc-9f50-01088e4005ff",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-05T12:56:46.644649Z",
     "iopub.status.busy": "2025-03-05T12:56:46.644315Z",
     "iopub.status.idle": "2025-03-05T12:56:46.654206Z",
     "shell.execute_reply": "2025-03-05T12:56:46.653122Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.014962,
     "end_time": "2025-03-05T12:56:46.655802",
     "exception": false,
     "start_time": "2025-03-05T12:56:46.640840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def DataCleaning(Target_File):\n",
    "    \"\"\"\n",
    "    Cleans the input DataFrame by performing the following operations:\n",
    "    1. Drops irrelevant columns.\n",
    "    2. Removes duplicate rows.\n",
    "    3. Filters videos based on language (only those with 'videoDefaultAudioLanguage' starting with 'en').\n",
    "    4. Translates non-ASCII characters in 'channelName' and 'videoTitle' to English.\n",
    "    5. Removes emojis and decodes HTML entities from 'channelName' and 'videoTitle'.\n",
    "    6. Removes non-ASCII characters from 'channelName' and 'videoTitle'.\n",
    "    7. Fills missing values in 'channelCountry' with 'Unknown'.\n",
    "    8. Returns the cleaned DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - Target_File (pd.DataFrame): The DataFrame to clean.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Drop irrelevant columns\n",
    "    Target_File = Target_File.drop(['videoDescription', 'videoLiveBroadcastContent', 'videoFavoriteCount',\n",
    "                                    'videoTags', 'videoUniqueId', 'channelIdUnique', 'channelTitleCheck', 'channelDescription'], axis=1)\n",
    "    \n",
    "    # # Identify and keep all duplicates\n",
    "    # duplicates = Target_File[Target_File.duplicated(keep=False)]  # Selects all duplicates, including the first occurrence\n",
    "    \n",
    "    # # Remove duplicates\n",
    "    # Target_File = Target_File.drop_duplicates(ignore_index=True)\n",
    "\n",
    "    Target_File_EN = Target_File.sort_values(\"videoPublishedOn\", ascending=False).drop_duplicates(subset=[\"videoId\", \"videoTitle\", \"channelId\"], keep='first').reset_index(drop=True)\n",
    "    \n",
    "    # Filter for videos with 'videoDefaultAudioLanguage' starting with 'en'\n",
    "    Target_File_EN = Target_File[Target_File['videoDefaultAudioLanguage'].str.startswith(\"en\", na=False)].reset_index(drop=True)\n",
    "\n",
    "    # Iterate through each row in 'Target_File_EN' to clean 'channelName' and 'videoTitle'\n",
    "    for i in range(len(Target_File_EN['channelName'])):\n",
    "        try:\n",
    "            # Translate non-ASCII characters in 'channelName' and 'videoTitle' to English\n",
    "            if not Target_File_EN['channelName'][i].isascii():\n",
    "                Target_File_EN.loc[i, 'channelName'] = GoogleTranslator(source='auto', target='en').translate(Target_File_EN['channelName'][i])\n",
    "            if not Target_File_EN['videoTitle'][i].isascii():\n",
    "                Target_File_EN.loc[i, 'videoTitle'] = GoogleTranslator(source='auto', target='en').translate(Target_File_EN['videoTitle'][i])\n",
    "\n",
    "            # Remove emojis\n",
    "            Target_File_EN.loc[i, 'channelName'] = emoji.replace_emoji(Target_File_EN['channelName'][i], replace='')\n",
    "            Target_File_EN.loc[i, 'videoTitle'] = emoji.replace_emoji(Target_File_EN['videoTitle'][i], replace='')\n",
    "\n",
    "            # Decode HTML entities like &amp; and &#39;\n",
    "            Target_File_EN.loc[i, 'channelName'] = html.unescape(Target_File_EN['channelName'][i])\n",
    "            Target_File_EN.loc[i, 'videoTitle'] = html.unescape(Target_File_EN['videoTitle'][i])\n",
    "\n",
    "            # Remove non-ASCII characters from 'channelName' and 'videoTitle'\n",
    "            Target_File_EN.loc[i, 'channelName'] = re.sub(r'[^\\x00-\\x7F]+', '', Target_File_EN['channelName'][i])\n",
    "            Target_File_EN.loc[i, 'videoTitle'] = re.sub(r'[^\\x00-\\x7F]+', '', Target_File_EN['videoTitle'][i])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Remove duplicates after the transformations\n",
    "    Target_File_EN = Target_File_EN.drop_duplicates(ignore_index=True)\n",
    "    \n",
    "    # Drop 'videoDefaultLanguage' column as it is no longer needed\n",
    "    Target_File_EN = Target_File_EN.drop(['videoDefaultLanguage'], axis=1)\n",
    "    \n",
    "    # Fill missing values in 'channelCountry' with 'Unknown'\n",
    "    Target_File_EN['channelCountry'] = Target_File_EN['channelCountry'].fillna('Unknown')\n",
    "\n",
    "    # Identify and keep all duplicates\n",
    "    duplicates = Target_File_EN[Target_File_EN['videoId'].duplicated(keep=False)]  # Selects all duplicates, including the first occurrence\n",
    "    \n",
    "    # Remove duplicates\n",
    "    Target_File_EN = Target_File_EN.drop_duplicates(ignore_index=True)\n",
    "    \n",
    "    return Target_File_EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "100b8d95",
   "metadata": {
    "_cell_guid": "67ef6e82-4e7b-47c7-81db-c0ca812dc060",
    "_uuid": "3dd98ff5-e097-412c-9e49-ae8827a5d714",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-05T12:56:46.662846Z",
     "iopub.status.busy": "2025-03-05T12:56:46.662513Z",
     "iopub.status.idle": "2025-03-05T12:56:46.669465Z",
     "shell.execute_reply": "2025-03-05T12:56:46.668455Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012308,
     "end_time": "2025-03-05T12:56:46.671106",
     "exception": false,
     "start_time": "2025-03-05T12:56:46.658798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GitHubPush(Target_File_EN):\n",
    "    \"\"\"\n",
    "    This function handles the process of saving a cleaned and processed DataFrame as a JSON file, \n",
    "    pushing it to a GitHub repository. It ensures that the file is properly named with a timestamp \n",
    "    and number of records, creates necessary directories, and commits the changes to the repository.\n",
    "    \n",
    "    Args:\n",
    "    - Target_File_EN (pd.DataFrame): The DataFrame that contains the processed data to be saved and pushed.\n",
    "    \n",
    "    Returns:\n",
    "    - None: This function performs file handling and Git operations but does not return anything.\n",
    "    \"\"\"\n",
    "\n",
    "    # Count the number of records in the DataFrame\n",
    "    record_count = len(Target_File_EN)\n",
    "    \n",
    "    # Generate a timestamp for the file name using the current time in IST (Indian Standard Time).\n",
    "    timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "    \n",
    "    # Create a filename using the generated timestamp and number of records to ensure uniqueness.\n",
    "    filename = f\"DC_{timestamp}_{record_count}_records.json\"\n",
    "    \n",
    "    # Save the DataFrame to a JSON file in a readable format (with indentation)\n",
    "    Target_File_EN.to_json(filename, orient=\"records\", indent=4)\n",
    "    print(f\"DataFrame saved as {filename}\")\n",
    "    \n",
    "    # Check if the destination directory exists\n",
    "    if not os.path.exists(destination_path):\n",
    "        # If the directory does not exist, create it\n",
    "        os.makedirs(destination_path)\n",
    "        print('Created the destination directory, DataCleaning/Daily')\n",
    "        # Copy the saved file into the newly created directory\n",
    "        shutil.copyfile(f'/kaggle/working/{filename}', f'{destination_path}/{filename}')\n",
    "    else:\n",
    "        print('Destination directory already exists')\n",
    "        # Copy the file to the existing directory\n",
    "        shutil.copyfile(f'/kaggle/working/{filename}', f'{destination_path}/{filename}')\n",
    "    \n",
    "    # Initialize the repository for git operations using the local GitHub repository URL\n",
    "    repo = Repo(kaggle_repo_url)\n",
    "    \n",
    "    # Add the copied file to the staging area for git commit\n",
    "    repo.index.add([f\"{destination_path}/{filename}\"])\n",
    "    \n",
    "    # Create a timestamp for the commit message\n",
    "    timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    # Commit the changes with a message that includes the timestamp and the filename\n",
    "    repo.index.commit(f\"{timestamp} Added files from Kaggle notebook, {filename}\")\n",
    "    \n",
    "    # Push the changes to the remote repository\n",
    "    origin = repo.remote(name=\"origin\")\n",
    "    push_result = origin.push()\n",
    "    \n",
    "    # Check if the push was successful and print the result\n",
    "    if push_result:\n",
    "        print(\"Push successful.\")\n",
    "    else:\n",
    "        print(\"Push failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f2596d2",
   "metadata": {
    "_cell_guid": "7d97eb4c-4052-439b-a03a-d97007941f5f",
    "_uuid": "34697395-1f1c-4045-af03-e014da7e1db1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-05T12:56:46.677570Z",
     "iopub.status.busy": "2025-03-05T12:56:46.677268Z",
     "iopub.status.idle": "2025-03-05T12:56:46.681604Z",
     "shell.execute_reply": "2025-03-05T12:56:46.680659Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.009541,
     "end_time": "2025-03-05T12:56:46.683387",
     "exception": false,
     "start_time": "2025-03-05T12:56:46.673846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    The main function orchestrates the entire data pipeline by:\n",
    "    1. Extracting the source data from the given repository URL.\n",
    "    2. Cleaning the extracted data using the DataCleaning function.\n",
    "    3. Pushing the final cleaned file to a GitHub repository.\n",
    "    \n",
    "    This function executes the steps in sequence to process and upload data.\n",
    "    \n",
    "    Args:\n",
    "    - None: This function does not accept any arguments. It uses predefined repository URLs and paths.\n",
    "    \n",
    "    Returns:\n",
    "    - None: This function does not return anything but performs data processing and Git operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Extract the source file from the repository based on the provided URL and path.\n",
    "    Source_File = Source_File_Extraction(repo_url, kaggle_repo_url, source_path)\n",
    "    \n",
    "    # Step 2: Clean the extracted data using the DataCleaning function.\n",
    "    Cleaned_File = DataCleaning(Source_File)\n",
    "    \n",
    "    # Optional: Uncomment to display the cleaned file sorted by video duration.\n",
    "    # display(Cleaned_File.sort_values(by='videoDurationInSeconds', ascending=True))\n",
    "    \n",
    "    # Step 3: Push the processed and feature-engineered data to GitHub using GitHubPush function.\n",
    "    GitHubPush(Cleaned_File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ada5640",
   "metadata": {
    "_cell_guid": "34ba1552-9550-46ad-bf07-46dcd41bc8dd",
    "_uuid": "d1e05076-eab2-4606-8973-3e9d364364ce",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-05T12:56:46.689837Z",
     "iopub.status.busy": "2025-03-05T12:56:46.689508Z",
     "iopub.status.idle": "2025-03-05T12:56:57.114213Z",
     "shell.execute_reply": "2025-03-05T12:56:57.112817Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 10.430113,
     "end_time": "2025-03-05T12:56:57.116209",
     "exception": false,
     "start_time": "2025-03-05T12:56:46.686096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully cloned the git repo\n",
      "DataFrame saved as DC_2025-03-05_18_26_56_392_records.json\n",
      "Destination directory already exists\n",
      "Push successful.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    This script is the entry point for the data cleaning pipeline.\n",
    "    It performs the following tasks:\n",
    "    1. Imports necessary libraries for data processing, file handling, and Git operations.\n",
    "    2. Retrieves user secrets for repository URL.\n",
    "    3. Sets up paths for different directories (source, destination, etc.).\n",
    "    4. Configures pandas to display all columns and rows without truncation.\n",
    "    5. Calls the main function to execute the pipeline.\n",
    "\n",
    "    The script is designed to be executed as the main module in a Python environment.\n",
    "    It ensures that all necessary operations are performed, including fetching source data, \n",
    "    cleaning, and pushing the final data to a GitHub repository.\n",
    "    \"\"\"\n",
    "\n",
    "    # Import necessary libraries\n",
    "    import os  \n",
    "    import git  # Git library for interacting with repositories\n",
    "    from git import Repo  # GitHub repository interaction\n",
    "    import time  # For time-related operations\n",
    "    import datetime  # For working with date and time\n",
    "    from pytz import timezone  # For timezone management\n",
    "    import pytz  # Timezone handling\n",
    "    import pandas as pd  # For data manipulation and analysis\n",
    "    import deep_translator  # For translation services\n",
    "    from deep_translator import GoogleTranslator  # Google Translate API integration\n",
    "    import shutil  # For file operations like copying or removing\n",
    "    import emoji  # For handling emojis in the data\n",
    "    import re  # For regular expression operations\n",
    "    import html  # For HTML parsing and escaping\n",
    "    from kaggle_secrets import UserSecretsClient  # For accessing Kaggle's secret management system\n",
    "    \n",
    "    # Retrieve secret value for repository URL from Kaggle secrets storage\n",
    "    user_secrets = UserSecretsClient()\n",
    "    secret_value_0 = user_secrets.get_secret(\"dataCleanRepoUrl\")\n",
    "    repo_url = secret_value_0  # URL for the GitHub repository used in this pipeline\n",
    "    \n",
    "    # Set timezone to Indian Standard Time (IST)\n",
    "    ist = timezone('Asia/Kolkata')\n",
    "    \n",
    "    # Define paths for different directories\n",
    "    kaggle_repo_url = '/kaggle/working/DevOps-YouTube-Trends'  # Path to the working repository on Kaggle\n",
    "    destination_path = '/kaggle/working/DevOps-YouTube-Trends/DataCleaning/Daily'  # Path to store cleaned data\n",
    "    source_path = '/kaggle/working/DevOps-YouTube-Trends/Source/Daily'  # Path to source raw data\n",
    "    \n",
    "    # Configure pandas to display all columns and rows without truncation for easier debugging\n",
    "    pd.set_option(\"display.max_columns\", None)  # Prevent truncating columns\n",
    "    pd.set_option(\"display.max_rows\", None)  # Prevent truncating rows\n",
    "    \n",
    "    # Call the main function to execute the data pipeline\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.152381,
   "end_time": "2025-03-05T12:56:57.840930",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-05T12:56:43.688549",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
