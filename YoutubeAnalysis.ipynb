{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b1635d",
   "metadata": {
    "_cell_guid": "2b4b8e6e-dc9e-4738-8f18-4638d01ef606",
    "_uuid": "5453e780-c25d-4964-ae3e-dd79f1a1a307",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-19T18:13:34.130689Z",
     "iopub.status.busy": "2025-03-19T18:13:34.130163Z",
     "iopub.status.idle": "2025-03-19T18:13:35.520773Z",
     "shell.execute_reply": "2025-03-19T18:13:35.519593Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.401722,
     "end_time": "2025-03-19T18:13:35.522843",
     "exception": false,
     "start_time": "2025-03-19T18:13:34.121121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pytz import timezone\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "# try:\n",
    "#     secret_value = user_secrets.get_secret(\"api_key_youtube_analysis\")\n",
    "#     print(\"Secret found:\", secret_value[:5] + \"***\")  # Just showing part of the secret\n",
    "# except Exception as e:\n",
    "#     print(\"Secret not found:\", e)\n",
    "secret_value_0 = user_secrets.get_secret(\"api_key_youtube_analysis\")\n",
    "secret_value_1 = user_secrets.get_secret(\"repo_url_youtube_analysis\")\n",
    "\n",
    "# Assigning secrets to variables\n",
    "api_key = secret_value_0\n",
    "repo_url = secret_value_1\n",
    "\n",
    "# Initialize an empty DataFrame with required columns\n",
    "FileExecution = pd.DataFrame(columns=['ScriptFile', 'StartTime', 'EndTime', 'TimeTaken', 'Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa012e0",
   "metadata": {
    "_cell_guid": "5c19a2fb-e8d8-49f7-b91c-9e2979e41e8d",
    "_uuid": "ddc19c6b-f1ed-44bb-9163-c97e1da2db53",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.005678,
     "end_time": "2025-03-19T18:13:35.534956",
     "exception": false,
     "start_time": "2025-03-19T18:13:35.529278",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SourceDaily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "609ee662",
   "metadata": {
    "_cell_guid": "f13e9a25-c025-4b69-8dbd-8fbca3e5e182",
    "_uuid": "44d5aa6c-0aca-4108-b965-3da3c91781d1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-19T18:13:35.548339Z",
     "iopub.status.busy": "2025-03-19T18:13:35.547852Z",
     "iopub.status.idle": "2025-03-19T18:13:35.553517Z",
     "shell.execute_reply": "2025-03-19T18:13:35.552304Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.014343,
     "end_time": "2025-03-19T18:13:35.555350",
     "exception": false,
     "start_time": "2025-03-19T18:13:35.541007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution started...\n"
     ]
    }
   ],
   "source": [
    "# Recording the start time of execution\n",
    "start_time = datetime.datetime.now()\n",
    "# Code block for which execution time need to measure\n",
    "print(\"Execution started...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d489748e",
   "metadata": {
    "_cell_guid": "05a9ec6f-e59b-4c99-9105-55c74901b307",
    "_uuid": "abddf0e3-05e8-4375-a60e-52688408ebaf",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-19T18:13:35.571285Z",
     "iopub.status.busy": "2025-03-19T18:13:35.570882Z",
     "iopub.status.idle": "2025-03-19T18:13:52.477478Z",
     "shell.execute_reply": "2025-03-19T18:13:52.475521Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 16.91764,
     "end_time": "2025-03-19T18:13:52.479618",
     "exception": false,
     "start_time": "2025-03-19T18:13:35.561978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as S_2025-03-19_23_43_46_517_records.json\n",
      "S_2025-03-19_23_43_46_517_records.json\n",
      "Successfully cloned the git repo\n",
      "Output files successfully pushed to GitHub!\n"
     ]
    }
   ],
   "source": [
    "def VideoDetailExtraction(kw_list, maxResults=50):\n",
    "    \"\"\"\n",
    "    Fetches a list of video details from YouTube based on the given keyword(s) for the initial batch.\n",
    "\n",
    "    Args:\n",
    "        kw_list (str): The keyword(s) to search for.\n",
    "        maxResults (int, optional): The maximum number of results to fetch in this request (default is 50).\n",
    "\n",
    "    Returns:\n",
    "        dict: The API response containing video details. Returns None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare the API request to fetch video details\n",
    "        request = youtube.search().list(\n",
    "            part='snippet',            # Fetch metadata such as title, description, and thumbnails\n",
    "            order='viewCount',         # Order results by view count\n",
    "            q=kw_list,                 # Search query\n",
    "            relevanceLanguage='en',    # Limit results to English-relevant videos\n",
    "            type='video',              # Restrict results to videos only\n",
    "            # videoCategoryId=26,      # Optional: Filter by specific category (currently commented)\n",
    "            # regionCode='IN',         # Optional: Restrict to a specific region (currently commented)\n",
    "            maxResults=maxResults,     # Number of results to fetch (up to 50 per API limits)\n",
    "            videoCaption='closedCaption'  # Restrict results to videos with closed captions\n",
    "        )\n",
    "\n",
    "        # Execute the API request\n",
    "        response = request.execute()\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log any errors encountered during the API call\n",
    "        print(f\"Error during VideoDetailExtraction(): {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def VideoDetailExtractionNextPageToken(kw_list, nextPageToken, maxResults=50):\n",
    "    \"\"\"\n",
    "    Fetches the next page of video details from YouTube using a continuation token.\n",
    "\n",
    "    Args:\n",
    "        kw_list (str): The keyword(s) to search for.\n",
    "        nextPageToken (str): The token for fetching the next page of results.\n",
    "        maxResults (int, optional): The maximum number of results to fetch in this request (default is 50).\n",
    "\n",
    "    Returns:\n",
    "        dict: The API response containing video details for the next page. Returns None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare the API request to fetch the next page of video details\n",
    "        request = youtube.search().list(\n",
    "            part='snippet',            # Fetch metadata such as title, description, and thumbnails\n",
    "            order='viewCount',         # Order results by view count\n",
    "            q=kw_list,                 # Search query\n",
    "            relevanceLanguage='en',    # Limit results to English-relevant videos\n",
    "            type='video',              # Restrict results to videos only\n",
    "            # videoCategoryId=26,      # Optional: Filter by specific category (currently commented)\n",
    "            # regionCode='IN',         # Optional: Restrict to a specific region (currently commented)\n",
    "            maxResults=maxResults,     # Number of results to fetch (up to 50 per API limits)\n",
    "            pageToken=nextPageToken,   # Token for fetching the next page\n",
    "            videoCaption='closedCaption'  # Restrict results to videos with closed captions\n",
    "        )\n",
    "\n",
    "        # Execute the API request\n",
    "        response = request.execute()\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log any errors encountered during the API call\n",
    "        print(f\"Error during VideoDetailExtractionNextPageToken(): {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def VideoDataFrame(response):\n",
    "    \"\"\"\n",
    "    Processes video and channel details from the YouTube API response, structures the data into DataFrames,\n",
    "    and merges them to create a comprehensive dataset.\n",
    "\n",
    "    Args:\n",
    "        response (dict): The response object returned by the YouTube API containing video details.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - pd.DataFrame: A DataFrame containing merged video and channel details.\n",
    "            - str or None: The next page token if available, otherwise None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize lists to store video and channel details\n",
    "        videoDetails = []\n",
    "        videoIds = []\n",
    "        channelIds = []\n",
    "        channelDetails = []\n",
    "        \n",
    "        '''\n",
    "        Video Search Block: Extract basic video details from the response.\n",
    "        '''\n",
    "        for i in range(len(response['items'])):\n",
    "            # Extract publication time and convert to components\n",
    "            publishedOn = response['items'][i].get('snippet', '0000-00-00T00:00:00Z').get('publishTime', '0000-00-00T00:00:00Z')\n",
    "            publishTime = re.split(r'[TZ-]', publishedOn)\n",
    "            total_seconds = 0\n",
    "            if publishedOn != '0000-00-00T00:00:00Z':\n",
    "                try:\n",
    "                    dt = datetime.datetime.strptime(publishedOn, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                except ValueError:\n",
    "                    dt = datetime.datetime.strptime(publishedOn, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                epoch = datetime.datetime(1970, 1, 1)\n",
    "                total_seconds = int((dt - epoch).total_seconds())\n",
    "            \n",
    "            # Append extracted video details\n",
    "            videoDetails.append({\n",
    "                'channelId': response['items'][i]['snippet']['channelId'],\n",
    "                'channelName': response['items'][i]['snippet']['channelTitle'],\n",
    "                'videoId': response['items'][i]['id']['videoId'],\n",
    "                'videoTitle': response['items'][i]['snippet']['title'],\n",
    "                'videoPublishYear': publishTime[0],  # Extracted year\n",
    "                'videoPublishMonth': publishTime[1],  # Extracted month\n",
    "                'videoPublishDay': publishTime[2],  # Extracted day\n",
    "                'videoPublishTime': publishTime[3],  # Extracted time\n",
    "                'videoPublishedOn': publishedOn,\n",
    "                'videoPublishedOnInSeconds': total_seconds\n",
    "            })\n",
    "            \n",
    "            # Collect video and channel IDs\n",
    "            videoIds.append(response['items'][i]['id']['videoId'])\n",
    "            channelIds.append(response['items'][i]['snippet']['channelId'])\n",
    "        \n",
    "        # Extract next page token if available\n",
    "        nextPageToken = response.get(\"nextPageToken\", None)\n",
    "        \n",
    "        '''\n",
    "        Video Block: Fetch additional details about each video using its ID.\n",
    "        '''\n",
    "        try:\n",
    "            request = youtube.videos().list(\n",
    "                part='id,statistics,snippet,contentDetails,localizations,status,liveStreamingDetails,paidProductPlacementDetails,player,recordingDetails,topicDetails',\n",
    "                id=videoIds\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except Exception as e:\n",
    "            # Log any errors encountered during the API call\n",
    "            print(f\"Error during videos().list(): {e}\")\n",
    "            return None\n",
    "        \n",
    "        for i in range(len(response['items'])):\n",
    "            video = response['items'][i]\n",
    "\n",
    "            # Video id\n",
    "            videoDetails[i]['videoUniqueId'] = video.get('id',None)\n",
    "            \n",
    "            # Video statistics\n",
    "            statistics = video.get('statistics', {})\n",
    "            videoDetails[i]['videoViewCount'] = statistics.get('viewCount', 0)\n",
    "            videoDetails[i]['videoLikeCount'] = statistics.get('likeCount', 0)\n",
    "            videoDetails[i]['videoFavoriteCount'] = statistics.get('favoriteCount', 0)\n",
    "            videoDetails[i]['videoCommentCount'] = statistics.get('commentCount', 0)\n",
    "            \n",
    "            # Video snippet details\n",
    "            snippet = video.get('snippet', {})\n",
    "            videoDetails[i]['videoDescription'] = snippet.get('description', None)\n",
    "            videoDetails[i]['videoTags'] = snippet.get('tags', [])\n",
    "            videoDetails[i]['videoCategoryId'] = snippet.get('categoryId', None)\n",
    "            videoDetails[i]['videoLiveBroadcastContent'] = snippet.get('liveBroadcastContent', None)\n",
    "            videoDetails[i]['videoDefaultLanguage'] = snippet.get('defaultLanguage', None)\n",
    "            videoDetails[i]['videoDefaultAudioLanguage'] = snippet.get('defaultAudioLanguage', None)\n",
    "            \n",
    "            # Video duration (convert ISO 8601 to seconds)\n",
    "            duration = video.get('contentDetails', {}).get('duration', None)\n",
    "            if duration:\n",
    "                # Match ISO 8601 duration format\n",
    "                match = re.match(r\"PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?\", duration)\n",
    "                \n",
    "                if match:\n",
    "                    # Extract hours, minutes, and seconds; fallback to 0 if missing\n",
    "                    hours = int(match.group(1) or 0)\n",
    "                    minutes = int(match.group(2) or 0)\n",
    "                    seconds = int(match.group(3) or 0)\n",
    "                    \n",
    "                    # Calculate total duration in seconds\n",
    "                    total_duration_in_seconds = hours * 3600 + minutes * 60 + seconds\n",
    "                    \n",
    "                    # Assign duration and classify content type\n",
    "                    videoDetails[i]['videoDuration'] = duration\n",
    "                    videoDetails[i]['videoDurationInSeconds'] = total_duration_in_seconds\n",
    "                    \n",
    "                    if total_duration_in_seconds <= 60:\n",
    "                        videoDetails[i]['videoContentType'] = 'Short'\n",
    "                    elif total_duration_in_seconds <= 90 and video.get('snippet', {}).get('liveBroadcastContent', '') == 'short':\n",
    "                        videoDetails[i]['videoContentType'] = 'Short'\n",
    "                    else:\n",
    "                        videoDetails[i]['videoContentType'] = 'Video'\n",
    "                else:\n",
    "                    # Handle invalid duration format\n",
    "                    videoDetails[i]['videoDuration'] = None\n",
    "                    videoDetails[i]['videoDurationInSeconds'] = None\n",
    "                    videoDetails[i]['videoContentType'] = 'Unknown'\n",
    "            else:\n",
    "                # If duration is missing\n",
    "                videoDetails[i]['videoDuration'] = None\n",
    "                videoDetails[i]['videoDurationInSeconds'] = None\n",
    "                videoDetails[i]['videoContentType'] = 'Unknown'\n",
    "    \n",
    "            # Additional video details\n",
    "            content_details = video.get('contentDetails', {})\n",
    "            videoDetails[i]['videoDimension'] = content_details.get('dimension', None)\n",
    "            videoDetails[i]['videoDefinition'] = content_details.get('definition', None)\n",
    "            videoDetails[i]['videoCaption'] = content_details.get('caption', None)\n",
    "            videoDetails[i]['videoLicensedContent'] = content_details.get('licensedContent', False)\n",
    "            videoDetails[i]['videoProjection'] = content_details.get('projection', False)\n",
    "        \n",
    "        '''\n",
    "        Channel Block: Fetch details for channels associated with the videos.\n",
    "        '''\n",
    "        videoDetails = pd.DataFrame(videoDetails)\n",
    "        Unique_ChannelIds = list(set(videoDetails['channelId']))\n",
    "        try:\n",
    "            request = youtube.channels().list(\n",
    "                part='id,contentDetails,brandingSettings,contentOwnerDetails,localizations,snippet,statistics,status,topicDetails',\n",
    "                id=Unique_ChannelIds\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except Exception as e:\n",
    "            # Log any errors encountered during the API call\n",
    "            print(f\"Error during channels().list(): {e}\")\n",
    "            return None\n",
    "        \n",
    "        for i in range(len(response['items'])):\n",
    "            item = response['items'][i]\n",
    "            snippet = item.get('snippet', {})\n",
    "            publishedOn = snippet.get('publishedAt', '0000-00-00T00:00:00Z')\n",
    "            publishedAt = re.split(r'[TZ-]', publishedOn)\n",
    "            total_seconds = 0\n",
    "            if publishedOn != '0000-00-00T00:00:00Z':\n",
    "                try:\n",
    "                    dt = datetime.datetime.strptime(publishedOn, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                except ValueError:\n",
    "                    dt = datetime.datetime.strptime(publishedOn, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                epoch = datetime.datetime(1970, 1, 1)\n",
    "                total_seconds = int((dt - epoch).total_seconds())\n",
    "            \n",
    "            # Extract channel details\n",
    "            channelDetails.append({\n",
    "                'channelIdUnique': item['id'],\n",
    "                'channelTitleCheck': snippet.get('title', None),\n",
    "                'channelDescription': snippet.get('description', None),\n",
    "                'channelCustomUrl': snippet.get('customUrl', None),\n",
    "                'channelPublishYear': publishedAt[0],\n",
    "                'channelPublishMonth': publishedAt[1],\n",
    "                'channelPublishDay': publishedAt[2],\n",
    "                'channelPublishTime': publishedAt[3],\n",
    "                'channelPublishedOn': publishedOn,\n",
    "                'channelPublishedOnInSeconds': total_seconds,\n",
    "                'channelCountry': snippet.get('country', None),\n",
    "                'channelViewCount': item.get('statistics', {}).get('viewCount', 0),\n",
    "                'channelSubscriberCount': item.get('statistics', {}).get('subscriberCount', 0),\n",
    "                'channelVideoCount': item.get('statistics', {}).get('videoCount', 0),\n",
    "            })\n",
    "        \n",
    "        # Convert channel details to DataFrame\n",
    "        channelDetails = pd.DataFrame(channelDetails)\n",
    "        \n",
    "        '''\n",
    "        Result: Merge video and channel details into a single DataFrame.\n",
    "        '''\n",
    "        resultDataFrame = pd.merge(videoDetails, channelDetails, left_on='channelId', right_on='channelIdUnique', how='left')\n",
    "        return resultDataFrame, nextPageToken\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing VideoDataFrame(): {e}\")\n",
    "        return None, None\n",
    "\n",
    "def VideoDetailsStructuring(max_record_count, kw_list):\n",
    "    \"\"\"\n",
    "    Fetches and structures video details into a DataFrame, handling pagination if necessary.\n",
    "\n",
    "    Args:\n",
    "        max_record_count (int): The maximum number of video records to fetch.\n",
    "        kw_list (str): The keyword(s) to use for fetching video details.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A Pandas DataFrame containing video details. Returns an empty DataFrame on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize an empty DataFrame to store results\n",
    "        resultDataFrame = pd.DataFrame()\n",
    "\n",
    "        # Initialize the nextPageToken for pagination\n",
    "        nextPageToken = None\n",
    "\n",
    "        # Define the batch sizes for video fetching\n",
    "        record_fetching_batches = [50]  # Default batch size for YouTube API requests\n",
    "\n",
    "        # Adjust the batch sizes based on the max_record_count\n",
    "        if max_record_count > 50:\n",
    "            quotient = max_record_count // 50  # Number of full batches\n",
    "            remainder = [max_record_count % 50]  # Remaining records in the last batch\n",
    "            record_fetching_batches = record_fetching_batches * quotient\n",
    "            if remainder[0] > 0:\n",
    "                record_fetching_batches.extend(remainder)  # Add the remainder as a batch\n",
    "        else:\n",
    "            record_fetching_batches = [max_record_count]  # Single batch if max_record_count <= 50\n",
    "\n",
    "        # Case 1: Only one batch needed\n",
    "        if len(record_fetching_batches) == 1:\n",
    "            # Fetch video details for the single batch\n",
    "            response = VideoDetailExtraction(kw_list, record_fetching_batches[0])\n",
    "            if response is None:\n",
    "                print(\"Failed to fetch initial video details - VideoDetailExtraction() returned None, hence returned empty DataFrame.\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            # Process the response into a DataFrame and get the nextPageToken\n",
    "            resultDataFrame, nextPageToken = VideoDataFrame(response)\n",
    "            nextPageToken = None  # Reset the token as no further pages are needed\n",
    "            if resultDataFrame is None:\n",
    "                print(\"Failed to process video data frame - VideoDataFrame() returned None, hence returned empty DataFrame.\")\n",
    "                return pd.DataFrame()\n",
    "            return resultDataFrame\n",
    "\n",
    "        # Case 2: Multiple batches needed\n",
    "        elif len(record_fetching_batches) > 1:\n",
    "            # Fetch initial batch of video details\n",
    "            response = VideoDetailExtraction(kw_list, record_fetching_batches[0])\n",
    "            if response is None:\n",
    "                print(\"Failed to fetch initial video details - VideoDetailExtraction() returned None, hence returned empty DataFrame.\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            # Process the response into a DataFrame and get the nextPageToken\n",
    "            resultDataFrame, nextPageToken = VideoDataFrame(response)\n",
    "            if resultDataFrame is None:\n",
    "                print(\"Failed to process video data frame - VideoDataFrame() returned None, hence returned empty DataFrame.\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            # Loop through subsequent batches\n",
    "            for batch in record_fetching_batches[1:]:\n",
    "                # Fetch details for the next batch using nextPageToken\n",
    "                response = VideoDetailExtractionNextPageToken(kw_list, nextPageToken, batch)\n",
    "                if response is None:\n",
    "                    print(\"Failed to fetch next page of video details - VideoDetailExtractionNextPageToken() returned None, hence returned till now fetched videoDetails.\")\n",
    "                    break\n",
    "\n",
    "                # Process the response into a DataFrame\n",
    "                resultDataFrame_next, nextPageToken = VideoDataFrame(response)\n",
    "                if resultDataFrame_next is not None:\n",
    "                    # Concatenate the new DataFrame to the result DataFrame\n",
    "                    resultDataFrame = pd.concat([resultDataFrame, resultDataFrame_next], ignore_index=True)\n",
    "\n",
    "                # Break the loop if we've reached the max record count or no more pages are available\n",
    "                if len(resultDataFrame) >= max_record_count or not nextPageToken:\n",
    "                    break\n",
    "\n",
    "        return resultDataFrame  # Return the final result DataFrame\n",
    "    except Exception as e:\n",
    "        print(f\"Error during VideoDetailsStructuring(), hence returned empty DataFrame: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def RawFile(max_record_count):\n",
    "    \"\"\"\n",
    "    Processes video details, structures the data, and saves it as a JSON file.\n",
    "\n",
    "    Args:\n",
    "        max_record_count (int): The maximum number of records to process.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the file is successfully created and saved, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Call the function to structure video details and return a DataFrame.\n",
    "        # `kw_list` is assumed to be a global variable containing the search keyword(s).\n",
    "        dataframe = VideoDetailsStructuring(max_record_count, kw_list)\n",
    "        \n",
    "        # Check if the DataFrame is not empty before saving.\n",
    "        if not dataframe.empty:\n",
    "            # Count the number of records (rows) in the DataFrame\n",
    "            record_count = len(dataframe)\n",
    "            \n",
    "            # Generate a timestamp for the file name using the current time in IST (Indian Standard Time).\n",
    "            timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "        \n",
    "            # Create a filename using the generated timestamp to ensure uniqueness with number of records.\n",
    "            filename = f\"S_{timestamp}_{record_count}_records.json\"\n",
    "            \n",
    "            # Save the DataFrame to a JSON file with readable formatting.\n",
    "            dataframe.to_json(filename, orient=\"records\", indent=4)\n",
    "            print(f\"DataFrame saved as {filename}\")\n",
    "        else:\n",
    "            # Log a message if the DataFrame is empty.\n",
    "            print(\"No data to save since empty DataFrame returned.\")\n",
    "        \n",
    "        # Return True indicating the process was successful.\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        # Handle and log any errors that occur during the process.\n",
    "        print(f\"Error during raw file creation: {e}\")\n",
    "        \n",
    "        # Return False indicating the process failed.\n",
    "        return False\n",
    "\n",
    "def PushToGithub():\n",
    "    \"\"\"\n",
    "    Automates the process of identifying the latest .json file, copying it \n",
    "    to a GitHub repository, and pushing the changes to the same branch.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the process completes successfully and the file is pushed to GitHub, \n",
    "              False if an error occurs during any step.\n",
    "    \"\"\"\n",
    "    # List all files in the working directory\n",
    "    output_files = os.listdir('/kaggle/working')\n",
    "    \n",
    "    try:\n",
    "        # Filter and find the most recent .json file\n",
    "        json_files = [file for file in output_files if file.startswith(\"S_\") and file.endswith(\"_records.json\")]\n",
    "        if json_files:\n",
    "            LatestFiles = max(json_files, key=os.path.getctime)  # Get the latest file based on creation time\n",
    "        else:\n",
    "            raise ValueError(\"No JSON files found!\")  # Raise an error if no JSON files are found\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred at fetching recent .json file: {e}\")\n",
    "        return False  # Exit the function if there's an error in fetching JSON files\n",
    "    \n",
    "    # Define repository and destination paths\n",
    "    kaggle_repo_url = '/kaggle/working/DevOps-YouTube-Trends'\n",
    "    destination_path = '/kaggle/working/DevOps-YouTube-Trends/Source/Daily'\n",
    "\n",
    "    \n",
    "    print(LatestFiles)  # Print the latest JSON file name\n",
    "    try:\n",
    "        # Check if the repository already exists\n",
    "        if os.path.exists(kaggle_repo_url):\n",
    "            print(\"Already cloned and the repo file exists\")\n",
    "            repo = git.Repo(kaggle_repo_url)  # Access the existing repository\n",
    "            repo.config_writer().set_value(\"user\", \"name\", name).release()\n",
    "            repo.config_writer().set_value(\"user\", \"email\", email).release()\n",
    "            origin = repo.remote(name='origin')  # Get the remote repository\n",
    "            origin.pull()  # Pull the latest changes from the repository\n",
    "            print(\"Successfully pulled the git repo before push\")\n",
    "        else:\n",
    "            # Clone the repository if it doesn't exist\n",
    "            repo = git.Repo.clone_from(repo_url, kaggle_repo_url)\n",
    "            repo.config_writer().set_value(\"user\", \"name\", name).release()\n",
    "            repo.config_writer().set_value(\"user\", \"email\", email).release()\n",
    "            print(\"Successfully cloned the git repo\")\n",
    "        \n",
    "        # Check if the destination path exists, and copy the latest file\n",
    "        if os.path.exists(destination_path):\n",
    "            shutil.copyfile(f'/kaggle/working/{LatestFiles}', f'{destination_path}/{LatestFiles}')\n",
    "        else:\n",
    "            # Create the destination directory if it doesn't exist\n",
    "            os.makedirs(destination_path)\n",
    "            shutil.copyfile(f'/kaggle/working/{LatestFiles}', f'{destination_path}/{LatestFiles}')\n",
    "        \n",
    "        # Initialize the repository for git operations\n",
    "        repo = Repo(kaggle_repo_url)\n",
    "        \n",
    "        # Add the copied file to the staging area\n",
    "        repo.index.add([f\"{destination_path}/{LatestFiles}\"])\n",
    "        \n",
    "        timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "        # Commit the changes with a message including the timestamp and file name\n",
    "        repo.index.commit(f\"{timestamp} Added files from Kaggle notebook, {LatestFiles}\")\n",
    "        \n",
    "        # Push the changes to the remote repository\n",
    "        origin = repo.remote(name=\"origin\")\n",
    "        push_result = origin.push()\n",
    "        if push_result:\n",
    "            print(\"Output files successfully pushed to GitHub!\")\n",
    "        else:\n",
    "            print(\"Output files pushed to GitHub failed:(\")\n",
    "        return True  # Return True if the process completes successfully\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the git automation process\n",
    "        print(f\"An error occurred at git automation code: {e}\")\n",
    "        return False  # Return False if an error occurs\n",
    "        \n",
    "def main(max_record_count):\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the execution of raw data extraction and pushing data to GitHub.\n",
    "\n",
    "    Args:\n",
    "        max_record_count (int): The maximum number of records to process.\n",
    "    \"\"\"\n",
    "    # Call the RawFile function to process and extract raw data.\n",
    "    # This function likely handles fetching data, processing it, and storing it in a file.\n",
    "    RawFile(max_record_count)\n",
    "    \n",
    "    # Call the PushToGithub function to push the processed data to a GitHub repository.\n",
    "    # This function likely handles staging, committing, and pushing the file to the repository.\n",
    "    PushToGithub()\n",
    "\n",
    "# Entry point of the script\n",
    "if __name__ == \"__main__\":\n",
    "    # Importing necessary libraries\n",
    "    from googleapiclient.discovery import build  # For interacting with YouTube API\n",
    "    from IPython.display import JSON, display  # For displaying JSON responses in Jupyter Notebooks\n",
    "    import re  # For regular expressions\n",
    "    import datetime  # For date and time manipulations\n",
    "    # from dateutil.relativedelta import relativedelta  # For handling relative date differences\n",
    "    import pandas as pd  # For data manipulation and analysis\n",
    "    import os  # For interacting with the operating system\n",
    "    from kaggle_secrets import UserSecretsClient  # For securely managing API keys in Kaggle\n",
    "    import git  # For Git-related operations\n",
    "    from git import Repo  # For working with repositories\n",
    "    import shutil  # For file and directory operations\n",
    "    from pytz import timezone  # For handling time zones\n",
    "    from datetime import timedelta  # For handling time differences\n",
    "    \n",
    "    # Fetching secrets from Kaggle's secure environment\n",
    "\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    secret_value_0 = user_secrets.get_secret(\"api_key_youtube_analysis\")\n",
    "    secret_value_1 = user_secrets.get_secret(\"repo_owner_mail_youtube_analysis\")\n",
    "    secret_value_2 = user_secrets.get_secret(\"repo_owner_youtube_analysis\")\n",
    "    secret_value_3 = user_secrets.get_secret(\"repo_url_youtube_analysis\")\n",
    "\n",
    "    \n",
    "    # Assigning secrets to variables\n",
    "    api_key = secret_value_0\n",
    "    email = secret_value_1\n",
    "    name = secret_value_2\n",
    "    repo_url = secret_value_3\n",
    "    \n",
    "    # Setting up YouTube API details\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "    youtube = build(api_service_name, api_version, developerKey=api_key)  # Initialize YouTube API client\n",
    "    \n",
    "    # Setting the timezone to Indian Standard Time (IST)\n",
    "    ist = timezone('Asia/Kolkata')\n",
    "    \n",
    "    # Maximum number of records to fetch\n",
    "    max_record_count = 4000\n",
    "    \n",
    "    # Keyword list for searching YouTube videos\n",
    "    kw_list = \"devops\"\n",
    "    \n",
    "    # Call the main function with the maximum record count as an argument\n",
    "    main(max_record_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af3ed0e3",
   "metadata": {
    "_cell_guid": "af1353df-4a00-4b60-9f0a-7edb047c67ac",
    "_uuid": "50570b22-cadb-4dcf-a0c9-0627925591d9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-19T18:13:52.495308Z",
     "iopub.status.busy": "2025-03-19T18:13:52.494935Z",
     "iopub.status.idle": "2025-03-19T18:13:52.504071Z",
     "shell.execute_reply": "2025-03-19T18:13:52.502638Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.019127,
     "end_time": "2025-03-19T18:13:52.505943",
     "exception": false,
     "start_time": "2025-03-19T18:13:52.486816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution ended.\n"
     ]
    }
   ],
   "source": [
    "print(\"Execution ended.\")\n",
    "\n",
    "# Record the end time of execution\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Calculate the time taken for execution\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "# Get the current time in the 'Asia/Kolkata' timezone\n",
    "current_time = datetime.datetime.now(timezone('Asia/Kolkata')).strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "\n",
    "# Create a new row as a DataFrame\n",
    "new_row = pd.DataFrame([{\n",
    "    'ScriptFile': 'sourcedaily.ipynb',\n",
    "    'StartTime': start_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'EndTime': end_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'TimeTaken': str(time_taken),\n",
    "    'Date': current_time\n",
    "}])\n",
    "\n",
    "# Append the new row using pd.concat()\n",
    "FileExecution = pd.concat([FileExecution, new_row], ignore_index=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "# display(FileExecution)\n",
    "\n",
    "# Save the DataFrame to a JSON file\n",
    "FileExecution.to_json(f\"{current_time}_ScriptFileExecution.json\", orient=\"records\", indent=4)\n",
    "# print(FileExecution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fe1237",
   "metadata": {
    "papermill": {
     "duration": 0.006854,
     "end_time": "2025-03-19T18:13:52.520080",
     "exception": false,
     "start_time": "2025-03-19T18:13:52.513226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DataCleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "600b343e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T18:13:52.534463Z",
     "iopub.status.busy": "2025-03-19T18:13:52.534043Z",
     "iopub.status.idle": "2025-03-19T18:13:52.539842Z",
     "shell.execute_reply": "2025-03-19T18:13:52.538381Z"
    },
    "papermill": {
     "duration": 0.015108,
     "end_time": "2025-03-19T18:13:52.541694",
     "exception": false,
     "start_time": "2025-03-19T18:13:52.526586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution started...\n"
     ]
    }
   ],
   "source": [
    "# Recording the start time of execution\n",
    "start_time = datetime.datetime.now()\n",
    "# Code block for which execution time need to measure\n",
    "print(\"Execution started...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8959aa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T18:13:52.557073Z",
     "iopub.status.busy": "2025-03-19T18:13:52.556642Z",
     "iopub.status.idle": "2025-03-19T18:13:57.451920Z",
     "shell.execute_reply": "2025-03-19T18:13:57.450434Z"
    },
    "papermill": {
     "duration": 4.905367,
     "end_time": "2025-03-19T18:13:57.453869",
     "exception": false,
     "start_time": "2025-03-19T18:13:52.548502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already cloned and the repo file exists\n",
      "Successfully pulled the git repo before push\n",
      "DataFrame saved as DC_2025-03-19_23_43_56_393_records.json\n",
      "Destination directory already exists\n",
      "Push successful.\n"
     ]
    }
   ],
   "source": [
    "def Source_File_Extraction(repo_url, kaggle_repo_url, source_path):\n",
    "    \"\"\"\n",
    "    This function checks if a specified Git repository already exists in the local system.\n",
    "    If the repository exists, it pulls the latest changes from the remote repository.\n",
    "    If the repository doesn't exist, it clones the repository from the provided URL.\n",
    "    \n",
    "    After ensuring the repository is up-to-date, it searches for a JSON file that starts with \"S_\" \n",
    "    and ends with \"records.json\" in the specified source directory, loads the file using pandas, \n",
    "    and returns the data as a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - repo_url (str): The URL of the Git repository to clone if not already present.\n",
    "    - kaggle_repo_url (str): The local path where the repository is stored or will be cloned to.\n",
    "    - source_path (str): The directory where the JSON file is stored.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The data from the JSON file as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    if os.path.exists(kaggle_repo_url):\n",
    "        print(\"Already cloned and the repo file exists\")\n",
    "        repo = git.Repo(kaggle_repo_url)  # Access the existing repository\n",
    "        repo.config_writer().set_value(\"user\", \"name\", name).release()\n",
    "        repo.config_writer().set_value(\"user\", \"email\", email).release()\n",
    "        origin = repo.remote(name='origin')  # Get the remote repository\n",
    "        origin.pull()  # Pull the latest changes from the repository\n",
    "        print(\"Successfully pulled the git repo before push\")\n",
    "    else:\n",
    "        # Clone the repository if it doesn't exist\n",
    "        repo = git.Repo.clone_from(repo_url, kaggle_repo_url)\n",
    "        repo.config_writer().set_value(\"user\", \"name\", name).release()\n",
    "        repo.config_writer().set_value(\"user\", \"email\", email).release()\n",
    "        print(\"Successfully cloned the git repo\")\n",
    "    \n",
    "    # List all files in the source path and find the relevant JSON file\n",
    "    output_files = os.listdir(source_path)\n",
    "    Source_File = max([i for i in output_files if i.startswith(\"S_\") and i.endswith('records.json')])\n",
    "    \n",
    "    # Read the found JSON file into a pandas DataFrame\n",
    "    Source_File = pd.read_json(f'{source_path}/{Source_File}')\n",
    "    \n",
    "    return Source_File\n",
    "\n",
    "def DataCleaning(Target_File):\n",
    "    \"\"\"\n",
    "    Cleans the input DataFrame by performing the following operations:\n",
    "    1. Drops irrelevant columns.\n",
    "    2. Removes duplicate rows based on videoId, videoTitle, and channelId.\n",
    "    3. Filters videos based on language (only those with 'videoDefaultAudioLanguage' starting with 'en').\n",
    "    4. Translates non-ASCII characters, removes emojis, and decodes HTML entities from 'channelName' and 'videoTitle'.\n",
    "    5. Fills missing values in 'channelCountry' with 'Unknown'.\n",
    "    6. Keeps the latest or highest view count record when duplicates are found.\n",
    "\n",
    "    Args:\n",
    "    - Target_File (pd.DataFrame): The DataFrame to clean.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop irrelevant columns\n",
    "    Target_File = Target_File.drop(['videoDescription', 'videoLiveBroadcastContent', 'videoFavoriteCount',\n",
    "                                    'videoTags', 'videoUniqueId', 'channelIdUnique', 'channelTitleCheck', 'channelDescription'], axis=1, errors='ignore')\n",
    "\n",
    "    # Filter only English audio language\n",
    "    Target_File_EN = Target_File[Target_File['videoDefaultAudioLanguage'].str.startswith(\"en\", na=False)].reset_index(drop=True)\n",
    "\n",
    "    # Translate, remove emojis, and clean non-ASCII characters\n",
    "    for col in ['channelName', 'videoTitle']:\n",
    "        Target_File_EN[col] = Target_File_EN[col].apply(lambda x: GoogleTranslator(source='auto', target='en').translate(x) if not x.isascii() else x)\n",
    "        Target_File_EN[col] = Target_File_EN[col].apply(lambda x: emoji.replace_emoji(x, replace=''))\n",
    "        Target_File_EN[col] = Target_File_EN[col].apply(lambda x: html.unescape(x))\n",
    "        Target_File_EN[col] = Target_File_EN[col].apply(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', x))\n",
    "\n",
    "    # Fill missing channelCountry values with 'Unknown'\n",
    "    Target_File_EN['channelCountry'] = Target_File_EN['channelCountry'].fillna('Unknown')\n",
    "\n",
    "    # Remove duplicates based on 'videoId' and keep the latest or highest view count record\n",
    "    Target_File_EN = Target_File_EN.sort_values(by=['videoViewCount', 'videoPublishedOn'], ascending=[False, False])\n",
    "    Target_File_EN = Target_File_EN.drop_duplicates(subset=['videoId'], keep='first').reset_index(drop=True)\n",
    "\n",
    "    # Drop unnecessary column\n",
    "    if 'videoDefaultLanguage' in Target_File_EN.columns:\n",
    "        Target_File_EN = Target_File_EN.drop(['videoDefaultLanguage'], axis=1)\n",
    "\n",
    "    return Target_File_EN\n",
    "\n",
    "\n",
    "def GitHubPush(Target_File_EN):\n",
    "    \"\"\"\n",
    "    This function handles the process of saving a cleaned and processed DataFrame as a JSON file, \n",
    "    pushing it to a GitHub repository. It ensures that the file is properly named with a timestamp \n",
    "    and number of records, creates necessary directories, and commits the changes to the repository.\n",
    "    \n",
    "    Args:\n",
    "    - Target_File_EN (pd.DataFrame): The DataFrame that contains the processed data to be saved and pushed.\n",
    "    \n",
    "    Returns:\n",
    "    - None: This function performs file handling and Git operations but does not return anything.\n",
    "    \"\"\"\n",
    "\n",
    "    # Count the number of records in the DataFrame\n",
    "    record_count = len(Target_File_EN)\n",
    "    \n",
    "    # Generate a timestamp for the file name using the current time in IST (Indian Standard Time).\n",
    "    timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "    \n",
    "    # Create a filename using the generated timestamp and number of records to ensure uniqueness.\n",
    "    filename = f\"DC_{timestamp}_{record_count}_records.json\"\n",
    "    \n",
    "    # Save the DataFrame to a JSON file in a readable format (with indentation)\n",
    "    Target_File_EN.to_json(filename, orient=\"records\", indent=4)\n",
    "    print(f\"DataFrame saved as {filename}\")\n",
    "    \n",
    "    # Check if the destination directory exists\n",
    "    if not os.path.exists(destination_path):\n",
    "        # If the directory does not exist, create it\n",
    "        os.makedirs(destination_path)\n",
    "        print('Created the destination directory, DataCleaning/Daily')\n",
    "        # Copy the saved file into the newly created directory\n",
    "        shutil.copyfile(f'/kaggle/working/{filename}', f'{destination_path}/{filename}')\n",
    "    else:\n",
    "        print('Destination directory already exists')\n",
    "        # Copy the file to the existing directory\n",
    "        shutil.copyfile(f'/kaggle/working/{filename}', f'{destination_path}/{filename}')\n",
    "    \n",
    "    # Initialize the repository for git operations using the local GitHub repository URL\n",
    "    repo = Repo(kaggle_repo_url)\n",
    "    \n",
    "    # Add the copied file to the staging area for git commit\n",
    "    repo.index.add([f\"{destination_path}/{filename}\"])\n",
    "    \n",
    "    # Create a timestamp for the commit message\n",
    "    timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    # Commit the changes with a message that includes the timestamp and the filename\n",
    "    repo.index.commit(f\"{timestamp} Added files from Kaggle notebook, {filename}\")\n",
    "    \n",
    "    # Push the changes to the remote repository\n",
    "    origin = repo.remote(name=\"origin\")\n",
    "    push_result = origin.push()\n",
    "    \n",
    "    # Check if the push was successful and print the result\n",
    "    if push_result:\n",
    "        print(\"Push successful.\")\n",
    "    else:\n",
    "        print(\"Push failed.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    The main function orchestrates the entire data pipeline by:\n",
    "    1. Extracting the source data from the given repository URL.\n",
    "    2. Cleaning the extracted data using the DataCleaning function.\n",
    "    3. Pushing the final cleaned file to a GitHub repository.\n",
    "    \n",
    "    This function executes the steps in sequence to process and upload data.\n",
    "    \n",
    "    Args:\n",
    "    - None: This function does not accept any arguments. It uses predefined repository URLs and paths.\n",
    "    \n",
    "    Returns:\n",
    "    - None: This function does not return anything but performs data processing and Git operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Extract the source file from the repository based on the provided URL and path.\n",
    "    Source_File = Source_File_Extraction(repo_url, kaggle_repo_url, source_path)\n",
    "    \n",
    "    # Step 2: Clean the extracted data using the DataCleaning function.\n",
    "    Cleaned_File = DataCleaning(Source_File)\n",
    "    \n",
    "    # Optional: Uncomment to display the cleaned file sorted by video duration.\n",
    "    # display(Cleaned_File.sort_values(by='videoDurationInSeconds', ascending=True))\n",
    "    \n",
    "    # Step 3: Push the processed and feature-engineered data to GitHub using GitHubPush function.\n",
    "    GitHubPush(Cleaned_File)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    This script is the entry point for the data cleaning pipeline.\n",
    "    It performs the following tasks:\n",
    "    1. Imports necessary libraries for data processing, file handling, and Git operations.\n",
    "    2. Retrieves user secrets for repository URL.\n",
    "    3. Sets up paths for different directories (source, destination, etc.).\n",
    "    4. Configures pandas to display all columns and rows without truncation.\n",
    "    5. Calls the main function to execute the pipeline.\n",
    "\n",
    "    The script is designed to be executed as the main module in a Python environment.\n",
    "    It ensures that all necessary operations are performed, including fetching source data, \n",
    "    cleaning, and pushing the final data to a GitHub repository.\n",
    "    \"\"\"\n",
    "\n",
    "    # Import necessary libraries\n",
    "    import os  \n",
    "    import git  # Git library for interacting with repositories\n",
    "    from git import Repo  # GitHub repository interaction\n",
    "    import time  # For time-related operations\n",
    "    import datetime  # For working with date and time\n",
    "    from pytz import timezone  # For timezone management\n",
    "    import pytz  # Timezone handling\n",
    "    import pandas as pd  # For data manipulation and analysis\n",
    "    import deep_translator  # For translation services\n",
    "    from deep_translator import GoogleTranslator  # Google Translate API integration\n",
    "    import shutil  # For file operations like copying or removing\n",
    "    import emoji  # For handling emojis in the data\n",
    "    import re  # For regular expression operations\n",
    "    import html  # For HTML parsing and escaping\n",
    "    from kaggle_secrets import UserSecretsClient  # For accessing Kaggle's secret management system\n",
    "    \n",
    "    # Retrieve secret value for repository URL from Kaggle secrets storage\n",
    "    user_secrets = UserSecretsClient()\n",
    "    secret_value_0 = user_secrets.get_secret(\"repo_owner_mail_youtube_analysis\")\n",
    "    secret_value_1 = user_secrets.get_secret(\"repo_owner_youtube_analysis\")\n",
    "    secret_value_2 = user_secrets.get_secret(\"repo_url_youtube_analysis\")\n",
    "\n",
    "    email = secret_value_0  # URL for the GitHub repository used in this pipeline\n",
    "    name = secret_value_1\n",
    "    repo_url = secret_value_2\n",
    "    \n",
    "    # Set timezone to Indian Standard Time (IST)\n",
    "    ist = timezone('Asia/Kolkata')\n",
    "    \n",
    "    # Define paths for different directories\n",
    "    kaggle_repo_url = '/kaggle/working/DevOps-YouTube-Trends'  # Path to the working repository on Kaggle\n",
    "    destination_path = '/kaggle/working/DevOps-YouTube-Trends/DataCleaning/Daily'  # Path to store cleaned data\n",
    "    source_path = '/kaggle/working/DevOps-YouTube-Trends/Source/Daily'  # Path to source raw data\n",
    "    \n",
    "    # Configure pandas to display all columns and rows without truncation for easier debugging\n",
    "    pd.set_option(\"display.max_columns\", None)  # Prevent truncating columns\n",
    "    pd.set_option(\"display.max_rows\", None)  # Prevent truncating rows\n",
    "    \n",
    "    # Call the main function to execute the data pipeline\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1db19b2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T18:13:57.469814Z",
     "iopub.status.busy": "2025-03-19T18:13:57.469421Z",
     "iopub.status.idle": "2025-03-19T18:13:57.479892Z",
     "shell.execute_reply": "2025-03-19T18:13:57.478027Z"
    },
    "papermill": {
     "duration": 0.021,
     "end_time": "2025-03-19T18:13:57.481921",
     "exception": false,
     "start_time": "2025-03-19T18:13:57.460921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution ended.\n"
     ]
    }
   ],
   "source": [
    "print(\"Execution ended.\")\n",
    "\n",
    "# Record the end time of execution\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Calculate the time taken for execution\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "# Get the current time in the 'Asia/Kolkata' timezone\n",
    "current_time = datetime.datetime.now(timezone('Asia/Kolkata')).strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "\n",
    "# Create a new row as a DataFrame\n",
    "new_row = pd.DataFrame([{\n",
    "    'ScriptFile': 'dataCleaning.ipynb',\n",
    "    'StartTime': start_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'EndTime': end_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'TimeTaken': str(time_taken),\n",
    "    'Date': current_time\n",
    "}])\n",
    "\n",
    "# Append the new row using pd.concat()\n",
    "FileExecution = pd.concat([FileExecution, new_row], ignore_index=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "# display(FileExecution)\n",
    "\n",
    "# Save the DataFrame to a JSON file\n",
    "FileExecution.to_json(f\"{current_time}_ScriptFileExecution.json\", orient=\"records\", indent=4)\n",
    "# print(FileExecution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3d9b0",
   "metadata": {
    "papermill": {
     "duration": 0.008018,
     "end_time": "2025-03-19T18:13:57.497119",
     "exception": false,
     "start_time": "2025-03-19T18:13:57.489101",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "786ae01a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T18:13:57.515766Z",
     "iopub.status.busy": "2025-03-19T18:13:57.515285Z",
     "iopub.status.idle": "2025-03-19T18:13:57.521355Z",
     "shell.execute_reply": "2025-03-19T18:13:57.519991Z"
    },
    "papermill": {
     "duration": 0.018328,
     "end_time": "2025-03-19T18:13:57.523777",
     "exception": false,
     "start_time": "2025-03-19T18:13:57.505449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution started...\n"
     ]
    }
   ],
   "source": [
    "# Recording the start time of execution\n",
    "start_time = datetime.datetime.now()\n",
    "# Code block for which execution time need to measure\n",
    "print(\"Execution started...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0321721c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T18:13:57.542279Z",
     "iopub.status.busy": "2025-03-19T18:13:57.541892Z",
     "iopub.status.idle": "2025-03-19T18:13:59.224746Z",
     "shell.execute_reply": "2025-03-19T18:13:59.223550Z"
    },
    "papermill": {
     "duration": 1.694318,
     "end_time": "2025-03-19T18:13:59.226539",
     "exception": false,
     "start_time": "2025-03-19T18:13:57.532221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country data saved as /kaggle/working/RE_2025-03-19_23_43_58_country_details.json\n",
      "Latest JSON file: RE_2025-03-19_23_43_58_country_details.json\n",
      "Repository already cloned; pulling latest changes.\n",
      "Successfully pulled the latest changes.\n",
      "Output files successfully pushed to GitHub!\n"
     ]
    }
   ],
   "source": [
    "def fetch_country_data():\n",
    "    \"\"\"\n",
    "    Fetches country data from the REST Countries API and returns a dictionary\n",
    "    with ISO 3166-1 Alpha-2 codes as keys and details (country name, continent,\n",
    "    continent code, IT hub status) as values.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing country data with ISO Alpha-2 codes as keys.\n",
    "    \"\"\"\n",
    "    # Mapping of continent full names to continent codes\n",
    "    continent_codes = {\n",
    "        \"Africa\": \"AF\",\n",
    "        \"Asia\": \"AS\",\n",
    "        \"Europe\": \"EU\",\n",
    "        \"North America\": \"NA\",\n",
    "        \"South America\": \"SA\",\n",
    "        \"Oceania\": \"OC\",\n",
    "        \"Antarctica\": \"AN\"\n",
    "    }\n",
    "    \n",
    "    # List of ISO Alpha-2 codes for IT hub countries\n",
    "    it_hub_countries = {\n",
    "        'US', 'IN', 'CN', 'JP', 'KR', 'DE', 'GB', 'FR', 'CA', 'AU',\n",
    "        'SG', 'SE', 'FI', 'IE', 'IL', 'NL', 'CH', 'ES', 'IT', 'BR',\n",
    "        'ZA', 'RU', 'AE', 'TR', 'PL', 'VN', 'MY', 'PH', 'TH', 'ID',\n",
    "        'HK', 'TW'\n",
    "    }\n",
    "    \n",
    "    # REST Countries API endpoint\n",
    "    url = \"https://restcountries.com/v3.1/all\"\n",
    "    response = requests.get(url)\n",
    "    # print(response)\n",
    "    countries = response.json()\n",
    "    \n",
    "    country_mapping = {}\n",
    "    for country in countries:\n",
    "        country_code = country.get(\"cca2\", None)\n",
    "        if not country_code:\n",
    "            continue\n",
    "        \n",
    "        country_name = country.get(\"name\", {}).get(\"common\", \"Unknown\")\n",
    "        continents = country.get(\"continents\", [])\n",
    "        if continents:\n",
    "            continent = continents[0]\n",
    "            continent_code = continent_codes.get(continent, \"Unknown\")\n",
    "        else:\n",
    "            continent = \"Unknown\"\n",
    "            continent_code = \"Unknown\"\n",
    "        \n",
    "        it_hub_status = \"Yes\" if country_code in it_hub_countries else \"No\"\n",
    "        \n",
    "        country_mapping[country_code] = {\n",
    "            \"country_name\": country_name,\n",
    "            \"continent\": continent,\n",
    "            \"continent_code\": continent_code,\n",
    "            \"it_hub_country\": it_hub_status\n",
    "        }\n",
    "    \n",
    "    return country_mapping\n",
    "\n",
    "def save_country_data(country_data):\n",
    "    \"\"\"\n",
    "    Saves the country data to a JSON file with a timestamped filename in the current working directory.\n",
    "\n",
    "    Args:\n",
    "        country_data (dict): The country data to be saved.\n",
    "    \"\"\"\n",
    "    # Generate a timestamp in the format YYYY-MM-DD_HH:MM:SS\n",
    "    timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "    # Create the filename with the timestamp\n",
    "    filename = f\"RE_{timestamp}_country_details.json\"\n",
    "    # Define the file path in the current working directory\n",
    "    filepath = os.path.join(os.getcwd(), filename)\n",
    "    # Write the country data to the JSON file\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(country_data, file, indent=4, ensure_ascii=False)\n",
    "    print(f\"Country data saved as {filepath}\")\n",
    "\n",
    "def push_to_github(repo_url):\n",
    "    \"\"\"\n",
    "    Automates pushing the generated JSON file to a GitHub repository.\n",
    "    Finds the latest JSON file, clones or pulls the repository, copies the file,\n",
    "    commits, and pushes it.\n",
    "\n",
    "    Args:\n",
    "        repo_url (str): The URL of the GitHub repository.\n",
    "    \"\"\"\n",
    "    # List all files in the current working directory\n",
    "    output_files = os.listdir(os.getcwd())\n",
    "    try:\n",
    "        # Filter for JSON files that match the naming pattern\n",
    "        json_files = [file for file in output_files if file.startswith(\"RE\") and file.endswith(\"country_details.json\")]\n",
    "        if json_files:\n",
    "            # Get the most recently created JSON file\n",
    "            latest_file = max(json_files, key=os.path.getctime)\n",
    "        else:\n",
    "            raise ValueError(\"No JSON files found!\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error fetching the recent .json file: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Define the local path for the cloned repository\n",
    "    local_repo_path = os.path.join(os.getcwd(), \"DevOps-YouTube-Trends\")\n",
    "    # Define the destination path within the repository\n",
    "    destination_path = os.path.join(local_repo_path, \"Requirement\", \"Daily\")\n",
    "    \n",
    "    print(f\"Latest JSON file: {latest_file}\")\n",
    "    try:\n",
    "        if os.path.exists(local_repo_path):\n",
    "            print(\"Repository already cloned; pulling latest changes.\")\n",
    "            repo = git.Repo(local_repo_path)\n",
    "            repo.config_writer().set_value(\"user\", \"name\", name).release()\n",
    "            repo.config_writer().set_value(\"user\", \"email\", email).release()\n",
    "            origin = repo.remote(name='origin')\n",
    "            origin.pull()\n",
    "            print(\"Successfully pulled the latest changes.\")\n",
    "        else:\n",
    "            repo = git.Repo.clone_from(repo_url, local_repo_path)\n",
    "            repo.config_writer().set_value(\"user\", \"name\", name).release()\n",
    "            repo.config_writer().set_value(\"user\", \"email\", email).release()\n",
    "            print(\"Successfully cloned the repository.\")\n",
    "        \n",
    "        if not os.path.exists(destination_path):\n",
    "            os.makedirs(destination_path)\n",
    "        # Copy the latest JSON file to the destination path in the repository\n",
    "        shutil.copyfile(os.path.join(os.getcwd(), latest_file),\n",
    "                        os.path.join(destination_path, latest_file))\n",
    "        \n",
    "        repo = Repo(local_repo_path)\n",
    "        # Stage the new file for commit\n",
    "        repo.index.add([os.path.join(destination_path, latest_file)])\n",
    "        # Create a commit message with the current timestamp\n",
    "        timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "        repo.index.commit(f\"{timestamp} - Added {latest_file} from Kaggle notebook\")\n",
    "        origin = repo.remote(name=\"origin\")\n",
    "        # Push the commit to the remote repository\n",
    "        push_result = origin.push()\n",
    "        if push_result:\n",
    "            print(\"Output files successfully pushed to GitHub!\")\n",
    "        else:\n",
    "            print(\"Pushing to GitHub failed.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during git automation: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to fetch country data, save it to a JSON file, and push it to GitHub.\n",
    "    \"\"\"\n",
    "    # Fetch the country data\n",
    "    country_data = fetch_country_data()\n",
    "    # print(country_data)\n",
    "    # Save the country data to a JSON file\n",
    "    save_country_data(country_data)\n",
    "    # Push the JSON file to the GitHub repository\n",
    "    push_to_github(repo_url)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Import necessary libraries\n",
    "    import requests\n",
    "    import json\n",
    "    import datetime\n",
    "    import os\n",
    "    import shutil\n",
    "    from pytz import timezone\n",
    "    import git\n",
    "    from git import Repo\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    \n",
    "    # Set the Indian Standard Time (IST) timezone\n",
    "    ist = timezone('Asia/Kolkata')\n",
    "\n",
    "    # Initialize the UserSecretsClient to retrieve the GitHub repository URL\n",
    "    user_secrets = UserSecretsClient()\n",
    "    secret_value_0 = user_secrets.get_secret(\"repo_owner_mail_youtube_analysis\")\n",
    "    secret_value_1 = user_secrets.get_secret(\"repo_owner_youtube_analysis\")\n",
    "    secret_value_2 = user_secrets.get_secret(\"repo_url_youtube_analysis\")\n",
    "\n",
    "    email = secret_value_0\n",
    "    name = secret_value_1\n",
    "    repo_url = secret_value_2\n",
    "    # Execute the main function\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe20f6aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T18:13:59.242213Z",
     "iopub.status.busy": "2025-03-19T18:13:59.241833Z",
     "iopub.status.idle": "2025-03-19T18:13:59.250500Z",
     "shell.execute_reply": "2025-03-19T18:13:59.249389Z"
    },
    "papermill": {
     "duration": 0.018708,
     "end_time": "2025-03-19T18:13:59.252465",
     "exception": false,
     "start_time": "2025-03-19T18:13:59.233757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution ended.\n"
     ]
    }
   ],
   "source": [
    "print(\"Execution ended.\")\n",
    "\n",
    "# Record the end time of execution\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Calculate the time taken for execution\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "# Get the current time in the 'Asia/Kolkata' timezone\n",
    "current_time = datetime.datetime.now(timezone('Asia/Kolkata')).strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "\n",
    "# Create a new row as a DataFrame\n",
    "new_row = pd.DataFrame([{\n",
    "    'ScriptFile': 'country-codes-iso-3166-1-alpha-2-continent-code.ipynb',\n",
    "    'StartTime': start_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'EndTime': end_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'TimeTaken': str(time_taken),\n",
    "    'Date': current_time\n",
    "}])\n",
    "\n",
    "# Append the new row using pd.concat()\n",
    "FileExecution = pd.concat([FileExecution, new_row], ignore_index=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "# display(FileExecution)\n",
    "\n",
    "# Save the DataFrame to a JSON file\n",
    "FileExecution.to_json(f\"{current_time}_ScriptFileExecution.json\", orient=\"records\", indent=4)\n",
    "# print(FileExecution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826073b5",
   "metadata": {
    "papermill": {
     "duration": 0.006818,
     "end_time": "2025-03-19T18:13:59.266441",
     "exception": false,
     "start_time": "2025-03-19T18:13:59.259623",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c842cbf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T18:13:59.281498Z",
     "iopub.status.busy": "2025-03-19T18:13:59.281115Z",
     "iopub.status.idle": "2025-03-19T18:13:59.286530Z",
     "shell.execute_reply": "2025-03-19T18:13:59.285335Z"
    },
    "papermill": {
     "duration": 0.015072,
     "end_time": "2025-03-19T18:13:59.288408",
     "exception": false,
     "start_time": "2025-03-19T18:13:59.273336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution started...\n"
     ]
    }
   ],
   "source": [
    "# Recording the start time of execution\n",
    "start_time = datetime.datetime.now()\n",
    "# Code block for which execution time need to measure\n",
    "print(\"Execution started...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1a42953",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T18:13:59.304816Z",
     "iopub.status.busy": "2025-03-19T18:13:59.304401Z",
     "iopub.status.idle": "2025-03-19T18:14:03.101684Z",
     "shell.execute_reply": "2025-03-19T18:14:03.100411Z"
    },
    "papermill": {
     "duration": 3.80802,
     "end_time": "2025-03-19T18:14:03.103763",
     "exception": false,
     "start_time": "2025-03-19T18:13:59.295743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository already exists locally.\n",
      "Successfully pulled the latest changes.\n",
      "Already cloned and the repo file exists\n",
      "Successfully pulled the git repo before push\n",
      "DataFrame saved as FE_2025-03-19_23_44_02_393_records.json\n",
      "Push successful.\n"
     ]
    }
   ],
   "source": [
    "def DataCleaning_File_Extraction(repo_url, kaggle_repo_url, DataCleaning_path):\n",
    "    \"\"\"\n",
    "    Extracts and processes a data cleaning file from a specified GitHub repository.\n",
    "\n",
    "    This function checks if the repository is already cloned locally. If found, it pulls the latest changes;\n",
    "    otherwise, it clones the repository. It then searches for a JSON file in the specified directory that starts\n",
    "    with 'DC_' and ends with 'records.json', reads it into a pandas DataFrame, and returns the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    repo_url (str): The URL of the GitHub repository to clone or update.\n",
    "    kaggle_repo_url (str): The local path where the repository is cloned.\n",
    "    DataCleaning_path (str): The directory path where the data cleaning files are stored.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A pandas DataFrame containing the extracted data from the JSON file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the repository already exists locally\n",
    "    if os.path.exists(kaggle_repo_url):\n",
    "        print(\"Repository already exists locally.\")\n",
    "        repo = git.Repo(kaggle_repo_url)  # Access the existing repository\n",
    "        repo.config_writer().set_value(\"user\", \"name\", name).release()\n",
    "        repo.config_writer().set_value(\"user\", \"email\", email).release()\n",
    "        origin = repo.remote(name='origin')  # Get the remote repository reference\n",
    "        origin.pull()  # Pull the latest updates from the remote repository\n",
    "        print(\"Successfully pulled the latest changes.\")\n",
    "    else:\n",
    "        # Clone the repository if it does not exist\n",
    "        repo = git.Repo.clone_from(repo_url, kaggle_repo_url)\n",
    "        repo.config_writer().set_value(\"user\", \"name\", name).release()\n",
    "        repo.config_writer().set_value(\"user\", \"email\", email).release()\n",
    "        print(\"Successfully cloned the repository.\")\n",
    "\n",
    "    # List all files in the specified directory and filter for the relevant JSON file\n",
    "    output_files = os.listdir(DataCleaning_path)\n",
    "    DataCleaning_File = max(\n",
    "        [file for file in output_files if file.startswith(\"DC_\") and file.endswith('records.json')]\n",
    "    )\n",
    "\n",
    "    # Read the identified JSON file into a pandas DataFrame\n",
    "    DataCleaning_File = pd.read_json(os.path.join(DataCleaning_path, DataCleaning_File))\n",
    "\n",
    "    return DataCleaning_File\n",
    "\n",
    "def Requirement_File_Extraction(repo_url, kaggle_repo_url, requirement_path):\n",
    "    \"\"\"\n",
    "    Ensures the repository is up-to-date by either pulling the latest changes or cloning it.\n",
    "    Then, extracts and returns the most recent JSON file starting with \"RE_\" and ending with \n",
    "    \"country_details.json\" from the specified requirement directory as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - repo_url (str): Git repository URL to clone if not present.\n",
    "    - kaggle_repo_url (str): Local directory path for the repository.\n",
    "    - requirement_path (str): Directory containing the JSON files.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Data from the most recent JSON file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the repository already exists locally\n",
    "    if os.path.exists(kaggle_repo_url):\n",
    "        print(\"Already cloned and the repo file exists\")\n",
    "        \n",
    "        # Access the existing repository and pull the latest changes\n",
    "        repo = git.Repo(kaggle_repo_url)\n",
    "        repo.config_writer().set_value(\"user\", \"name\", name).release()\n",
    "        repo.config_writer().set_value(\"user\", \"email\", email).release()\n",
    "        origin = repo.remote(name='origin')\n",
    "        origin.pull()  # Pull the latest changes\n",
    "        print(\"Successfully pulled the git repo before push\")\n",
    "    else:\n",
    "        # Clone the repository if it doesn't exist locally\n",
    "        repo = git.Repo.clone_from(repo_url, kaggle_repo_url)\n",
    "        repo.config_writer().set_value(\"user\", \"name\", name).release()\n",
    "        repo.config_writer().set_value(\"user\", \"email\", email).release()\n",
    "        print(\"Successfully cloned the git repo\")\n",
    "    \n",
    "    # List all files in the requirement directory\n",
    "    output_files = os.listdir(requirement_path)\n",
    "    \n",
    "    # Find the most recent JSON file that starts with \"RE_\" and ends with \"country_details.json\"\n",
    "    Requirement_File = max([i for i in output_files if i.startswith(\"RE_\") and i.endswith('country_details.json')])\n",
    "    \n",
    "    # Read the found JSON file into a pandas DataFrame\n",
    "    Requirement_File = pd.read_json(f'{requirement_path}/{Requirement_File}')\n",
    "    \n",
    "    return Requirement_File\n",
    "\n",
    "def videoDurationClassification(videoDurationInSeconds):\n",
    "    \"\"\"\n",
    "    Classifies the video duration into categories based on its length in seconds.\n",
    "\n",
    "    Args:\n",
    "    - videoDurationInSeconds (int): Duration of the video in seconds.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string indicating the classification of the video duration.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Classifying the video duration into different categories\n",
    "    if 0 <= videoDurationInSeconds <= 60:\n",
    "        return \"Very Short\"  # Video duration between 0 and 60 seconds\n",
    "    elif 61 <= videoDurationInSeconds <= 120:\n",
    "        return \"Short\"  # Video duration between 61 and 120 seconds\n",
    "    elif 121 <= videoDurationInSeconds <= 300:\n",
    "        return \"Medium\"  # Video duration between 121 and 300 seconds (2-5 minutes)\n",
    "    elif 301 <= videoDurationInSeconds <= 600:\n",
    "        return \"Long\"  # Video duration between 301 and 600 seconds (5-10 minutes)\n",
    "    elif 601 <= videoDurationInSeconds <= 3600:\n",
    "        return \"Very Long\"  # Video duration between 601 seconds (10 minutes) and 3600 seconds (1 hour)\n",
    "    elif 3601 <= videoDurationInSeconds <= 10800:\n",
    "        return \"Extended\"  # Video duration between 3601 seconds (1 hour) and 10800 seconds (3 hours)\n",
    "    elif videoDurationInSeconds > 10800:\n",
    "        return \"Ultra Long\"  # Video duration greater than 10800 seconds (3 hours)\n",
    "    else:\n",
    "        return \"Invalid video duration\"  # Invalid value for video duration\n",
    "\n",
    "def parse_datetime(value):\n",
    "    \"\"\"\n",
    "    Parses a datetime string into a Pandas datetime object based on specific formats.\n",
    "    \n",
    "    Handles two datetime formats:\n",
    "    1. \"%Y-%m-%dT%H:%M:%SZ\" for ISO 8601 format without fractional seconds.\n",
    "    2. \"%Y-%m-%dT%H:%M:%S.%fZ\" for ISO 8601 format with fractional seconds.\n",
    "\n",
    "    Args:\n",
    "    - value (str): The input datetime string to be parsed.\n",
    "\n",
    "    Returns:\n",
    "    - pd.Timestamp or pd.NaT: A Pandas Timestamp object if the format matches, otherwise pd.NaT.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check for the presence of \"Z\" and determine the format based on whether the string contains a decimal point\n",
    "    if \"Z\" in value and \".\" not in value:\n",
    "        return pd.to_datetime(value, format=\"%Y-%m-%dT%H:%M:%SZ\")  # Format without fractional seconds\n",
    "    elif \"Z\" in value and \".\" in value:\n",
    "        return pd.to_datetime(value, format=\"%Y-%m-%dT%H:%M:%S.%fZ\")  # Format with fractional seconds\n",
    "    else:\n",
    "        return pd.NaT  # Return Not a Time (NaT) if the format doesn't match\n",
    "\n",
    "def normalize(series):\n",
    "    \"\"\"\n",
    "    Normalizes a Pandas Series using Min-Max Scaling.\n",
    "\n",
    "    Parameters:\n",
    "    series (pd.Series): The input series to be normalized.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A NumPy array containing normalized values between 0 and 1.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()  # Initialize the MinMaxScaler\n",
    "    return scaler.fit_transform(series.values.reshape(-1, 1)).flatten()  # Normalize and return a 1D array\n",
    "\n",
    "def calculate_channel_growth(Cleaned_File):\n",
    "    \"\"\"\n",
    "    Calculates the growth score for channels and engagement score for videos based on various metrics.\n",
    "    \n",
    "    The growth score for each channel is calculated using the normalized values of:\n",
    "    - View count\n",
    "    - Subscriber count\n",
    "    - Video count\n",
    "    - Channel age (in years)\n",
    "    \n",
    "    The engagement score for each video is calculated using:\n",
    "    - Views per day\n",
    "    - Like-to-view ratio\n",
    "    - Comment-to-view ratio\n",
    "\n",
    "    Args:\n",
    "    - Cleaned_File (pd.DataFrame): The dataframe containing channel and video data to calculate growth and engagement scores.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The cleaned dataframe with the added columns for growth and engagement scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the current IST time (Indian Standard Time) to calculate age-based metrics\n",
    "    utc_timestamp = int(time.time())\n",
    "    zone = pytz.timezone('Asia/Kolkata')\n",
    "    current_ist_time = datetime.datetime.fromtimestamp(utc_timestamp, zone).replace(tzinfo=None)\n",
    "    # Channel Age Calculation (in years)\n",
    "    channelPublishedOn = Cleaned_File[\"channelPublishedOn\"].apply(parse_datetime)\n",
    "    Cleaned_File['channelAgeInYears'] = (current_ist_time - channelPublishedOn).dt.total_seconds() / (365 * 24 * 60 * 60)\n",
    "    \n",
    "    # Min-Max normalization for channel metrics\n",
    "    Cleaned_File['channelNormalizedViewCount'] = normalize(Cleaned_File['channelViewCount'])\n",
    "    Cleaned_File['channelNormalizedSubscriberCount'] = normalize(Cleaned_File['channelSubscriberCount'])\n",
    "    Cleaned_File['channelNormalizedVideoCount'] = normalize(Cleaned_File['channelVideoCount'])\n",
    "    Cleaned_File['channelNormalizedChannelAge'] = normalize(Cleaned_File['channelAgeInYears'])\n",
    "    \n",
    "    # Define weights for each metric in the growth score calculation\n",
    "    weight_views = 50\n",
    "    weight_subscribers = 30\n",
    "    weight_videos = 20\n",
    "    \n",
    "    # Growth Score Calculation for the channel\n",
    "    Cleaned_File['channelGrowthScore'] = (\n",
    "        (Cleaned_File['channelNormalizedViewCount'] * weight_views) +\n",
    "        (Cleaned_File['channelNormalizedSubscriberCount'] * weight_subscribers) +\n",
    "        (Cleaned_File['channelNormalizedVideoCount'] * weight_videos)\n",
    "    ) / (Cleaned_File['channelNormalizedChannelAge'] + 1e-6)  # Avoid division by zero\n",
    "    \n",
    "    # Video Age Calculation (in days)\n",
    "    videoPublishedOn = Cleaned_File[\"videoPublishedOn\"].apply(parse_datetime)\n",
    "    Cleaned_File[\"videoAgeInDays\"] = (current_ist_time - videoPublishedOn).dt.total_seconds() / (24 * 60 * 60)\n",
    "    \n",
    "    # Engagement Metrics for videos\n",
    "    Cleaned_File[\"videoViewsPerDay\"] = Cleaned_File[\"videoViewCount\"] / (Cleaned_File[\"videoAgeInDays\"] + 1e-6)  # Avoid division by zero\n",
    "    Cleaned_File[\"videoLikeToViewRatio\"] = Cleaned_File[\"videoLikeCount\"] / (Cleaned_File[\"videoViewCount\"] + 1e-6)\n",
    "    Cleaned_File[\"videoCommentToViewRatio\"] = Cleaned_File[\"videoCommentCount\"] / (Cleaned_File[\"videoViewCount\"] + 1e-6)\n",
    "    \n",
    "    # Engagement Score Calculation for the video\n",
    "    Cleaned_File[\"videoEngagementScore\"] = (\n",
    "        (Cleaned_File[\"videoViewsPerDay\"] * 50) +\n",
    "        (Cleaned_File[\"videoLikeToViewRatio\"] * 100 * 30) +\n",
    "        (Cleaned_File[\"videoCommentToViewRatio\"] * 100 * 20)\n",
    "    )\n",
    "    \n",
    "    # Return the dataframe with added growth and engagement scores\n",
    "    return Cleaned_File\n",
    "\n",
    "def HierarchicalWeightRanking(Cleaned_File):\n",
    "    \"\"\"\n",
    "    Assigns rankings to channels and videos based on their growth and engagement scores.\n",
    "    \n",
    "    - Channels are ranked only once to prevent duplication.\n",
    "    - Videos are ranked individually.\n",
    "    \n",
    "    Parameters:\n",
    "    Cleaned_File (pd.DataFrame): The input DataFrame containing channel and video data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with added ranking columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Rank channels uniquely (consider only one entry per channel)\n",
    "    channel_ranking_df = Cleaned_File.drop_duplicates(subset=['channelId']).copy()\n",
    "\n",
    "    # Sort by channel growth-related features\n",
    "    sort_orderby_columns = ['channelGrowthScore', 'channelNormalizedViewCount', 'channelViewCount',\n",
    "                            'channelNormalizedSubscriberCount', 'channelSubscriberCount', \n",
    "                            'channelNormalizedVideoCount', 'channelVideoCount', \n",
    "                            'channelNormalizedChannelAge', 'channelAgeInYears']\n",
    "    ascending_bool = [False] * len(sort_orderby_columns)\n",
    "\n",
    "    channel_ranking_df = channel_ranking_df.sort_values(by=sort_orderby_columns, ascending=ascending_bool)\n",
    "    channel_ranking_df[\"channelGrowthScoreRank\"] = range(1, len(channel_ranking_df) + 1)\n",
    "\n",
    "    # Merge the unique channel ranks back to the original DataFrame\n",
    "    Cleaned_File = Cleaned_File.merge(channel_ranking_df[['channelId', 'channelGrowthScoreRank']], on='channelId', how='left')\n",
    "\n",
    "    # Rank videos normally (since they are unique)\n",
    "    sort_orderby_columns = ['videoEngagementScore', 'videoViewsPerDay', 'videoViewCount', \n",
    "                            'videoLikeToViewRatio', 'videoLikeCount', 'videoCommentToViewRatio', \n",
    "                            'videoCommentCount', 'videoAgeInDays']\n",
    "    ascending_bool = [False, False, False, False, False, False, False, True]\n",
    "\n",
    "    Cleaned_File = Cleaned_File.sort_values(by=sort_orderby_columns, ascending=ascending_bool)\n",
    "    Cleaned_File[\"videoEngagementScoreRank\"] = range(1, len(Cleaned_File) + 1)\n",
    "\n",
    "    return Cleaned_File\n",
    "\n",
    "\n",
    "def FeatureEngineering(Cleaned_File):\n",
    "    \"\"\"\n",
    "    This function performs feature engineering to enhance the dataset for analysis by creating new features \n",
    "    and transforming existing ones, such as categorizing video duration, calculating channel growth and \n",
    "    video engagement scores, and enriching geographic details.\n",
    "\n",
    "    The key steps include:\n",
    "    - Extracting the day of the week from the video publish timestamp.\n",
    "    - Classifying video durations into predefined categories.\n",
    "    - Calculating channel growth and video engagement scores.\n",
    "    - Ranking channels and videos based on their growth and engagement scores.\n",
    "    - Merging geographic details like country, continent, and IT hub information with the dataset.\n",
    "    \n",
    "    Args:\n",
    "    - Cleaned_File (pd.DataFrame): The input dataframe containing video and channel data for feature engineering.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The transformed dataframe with newly engineered features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Feature: videoPublishedWeekDay - Derive the day of the week from the videoPublishedOn timestamp.\n",
    "    Cleaned_File['videoPublishedWeekDay'] = pd.to_datetime(Cleaned_File[\"videoPublishedOn\"]).dt.day_name()\n",
    "    \n",
    "    # Feature: videoDurationClassification - Categorize videos based on their duration in seconds into predefined segments.\n",
    "    # Categories:\n",
    "    #     Very Short (0 - 60 sec), Short (61 sec - 2 min), Medium (2 min 1 sec - 5 min),\n",
    "    #     Long (5 min 1 sec - 10 min), Very Long (10 min 1 sec - 1 hour),\n",
    "    #     Extended (1 hour 1 sec - 3 hours), Ultra Long (3 hours 1 sec and above)\n",
    "    Cleaned_File['videoDurationClassification'] = Cleaned_File['videoDurationInSeconds'].apply(videoDurationClassification)\n",
    "    \n",
    "    # Feature: channelGrowth metric - Calculate channel growth using factors such as views, subscribers, video count, and age.\n",
    "    # Normalization of key columns: channelPublishedOn, channelViewCount, channelSubscriberCount, and channelVideoCount\n",
    "    Cleaned_File = calculate_channel_growth(Cleaned_File)\n",
    "    \n",
    "    # Feature: videoEngagementScore - Calculate the video engagement score using video views, likes, and comments.\n",
    "    # Normalization of key columns: videoPublishedOn, videoViewCount, videoLikeCount, and videoCommentCount\n",
    "    Cleaned_File = calculate_channel_growth(Cleaned_File)  # This also handles the video engagement scores\n",
    "    \n",
    "    # Feature: channelGrowthScoreRank - Rank channels based on their growth score.\n",
    "    # Feature: videoEngagementScoreRank - Rank videos based on their engagement score.\n",
    "    Cleaned_File = HierarchicalWeightRanking(Cleaned_File)\n",
    "    \n",
    "    # Feature: Geographic Classification - Enrich dataset with geographic details (country, continent, IT hub classification).\n",
    "    # This merges additional country and continent details from an external source based on the channel's country.\n",
    "    \n",
    "    # Fetch geographic details (ISO codes, country names, continent, etc.) from an external file\n",
    "    Country_Details_ISO = Requirement_File_Extraction(repo_url, kaggle_repo_url, requirement_path).transpose()\n",
    "    Country_Details_ISO = Country_Details_ISO.reset_index()\n",
    "    Country_Details_ISO.rename(columns={'index': 'country_code'}, inplace=True)\n",
    "    \n",
    "    # Merge geographic details (from Country_Details_ISO) with the cleaned file\n",
    "    resultDataFrame = pd.merge(Cleaned_File, Country_Details_ISO, left_on='channelCountry', right_on='country_code', how='left')\n",
    "    \n",
    "    # Fill missing geographic data with 'Unknown' (in case a country code doesn't match)\n",
    "    cols_to_fill = ['country_code', 'country_name', 'continent', 'continent_code', 'it_hub_country']\n",
    "    resultDataFrame[cols_to_fill] = resultDataFrame[cols_to_fill].fillna('Unknown')\n",
    "    \n",
    "    # Return the enriched dataframe with new features\n",
    "    return resultDataFrame\n",
    "\n",
    "def GitHubPush(Feature_File):\n",
    "    \"\"\"\n",
    "    Saves a DataFrame as a JSON file and pushes it to a GitHub repository.\n",
    "\n",
    "    This function:\n",
    "    - Counts the number of records in the DataFrame.\n",
    "    - Generates a unique filename using a timestamp in IST (Indian Standard Time) and the record count.\n",
    "    - Saves the DataFrame as a JSON file in a readable format.\n",
    "    - Checks if the destination directory exists; if not, creates it.\n",
    "    - Copies the saved file to the destination directory.\n",
    "    - Commits and pushes the file to a GitHub repository.\n",
    "\n",
    "    Parameters:\n",
    "    Feature_File (pd.DataFrame): The DataFrame to be saved and pushed.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Count the number of records in the DataFrame\n",
    "    record_count = len(Feature_File)\n",
    "    \n",
    "    # Generate a timestamp for the filename using the current time in IST (Indian Standard Time)\n",
    "    timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "    \n",
    "    # Create a unique filename using the timestamp and number of records\n",
    "    filename = f\"FE_{timestamp}_{record_count}_records.json\"\n",
    "    \n",
    "    # Save the DataFrame as a JSON file with indentation for readability\n",
    "    Feature_File.to_json(filename, orient=\"records\", indent=4)\n",
    "    print(f\"DataFrame saved as {filename}\")\n",
    "    \n",
    "    # Check if the destination directory exists\n",
    "    if not os.path.exists(destination_path):\n",
    "        # Create the directory if it does not exist\n",
    "        os.makedirs(destination_path)\n",
    "        print(\"Created the destination directory: FeatureEngineering/Daily\")\n",
    "    \n",
    "    # Copy the saved file to the destination directory\n",
    "    shutil.copyfile(f'/kaggle/working/{filename}', f'{destination_path}/{filename}')\n",
    "    \n",
    "    # Initialize the local Git repository\n",
    "    repo = Repo(kaggle_repo_url)\n",
    "    \n",
    "    # Add the copied file to the Git staging area\n",
    "    repo.index.add([f\"{destination_path}/{filename}\"])\n",
    "    \n",
    "    # Create a commit message including the timestamp and filename\n",
    "    timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    repo.index.commit(f\"{timestamp} Added files from Kaggle notebook, {filename}\")\n",
    "    \n",
    "    # Push the committed changes to the remote GitHub repository\n",
    "    origin = repo.remote(name=\"origin\")\n",
    "    push_result = origin.push()\n",
    "    \n",
    "    # Check if the push was successful and print the result\n",
    "    if push_result:\n",
    "        print(\"Push successful.\")\n",
    "    else:\n",
    "        print(\"Push failed.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the data extraction, feature engineering, and GitHub push process.\n",
    "\n",
    "    Steps:\n",
    "    1. Extracts the cleaned data file from the repository using the provided URL and path.\n",
    "    2. Applies feature engineering to enhance the cleaned data.\n",
    "    3. Pushes the processed and feature-engineered data to GitHub.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Extract the cleaned data file from the repository using the provided URL and path.\n",
    "    DataCleaning_File = DataCleaning_File_Extraction(repo_url, kaggle_repo_url, DataCleaning_path)\n",
    "    \n",
    "    # Optional: Uncomment the following line to display the cleaned file sorted by video duration.\n",
    "    # display(DataCleaning_File.sort_values(by='videoDurationInSeconds', ascending=True))\n",
    "    \n",
    "    # Step 2: Apply feature engineering transformations to the cleaned data.\n",
    "    Feature_File = FeatureEngineering(DataCleaning_File)\n",
    "    \n",
    "    # Optional: Uncomment the following line to display the feature-engineered file.\n",
    "    # display(Feature_File)\n",
    "    \n",
    "    # Step 3: Push the processed and feature-engineered data to GitHub.\n",
    "    GitHubPush(Feature_File)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Entry point for the data pipeline execution. \n",
    "\n",
    "    This script:\n",
    "    - Imports necessary libraries for file handling, Git operations, time management, data manipulation, and Kaggle secret access.\n",
    "    - Retrieves the GitHub repository URL from Kaggle secrets.\n",
    "    - Sets up the Indian Standard Time (IST) timezone for consistent timestamping.\n",
    "    - Defines paths for various directories used in the pipeline, including repositories, data cleaning, and feature engineering storage.\n",
    "    - Configures pandas to display all columns and rows for better debugging.\n",
    "    - Calls the `main()` function to execute the full data pipeline, including data extraction, feature engineering, and pushing data to GitHub.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Import necessary libraries\n",
    "    import os  # For file and directory operations\n",
    "    import git  # For interacting with Git repositories\n",
    "    from git import Repo  # For handling GitHub repository interactions\n",
    "    import time  # For time-related operations\n",
    "    import datetime  # For date and time manipulations\n",
    "    import pytz  # For timezone handling\n",
    "    from pytz import timezone  # To manage different timezones\n",
    "    import pandas as pd  # For data manipulation and analysis\n",
    "    import shutil  # For file operations like copying and removing files\n",
    "    from kaggle_secrets import UserSecretsClient  # For securely accessing secrets in Kaggle\n",
    "    from sklearn.preprocessing import MinMaxScaler # Using MinMaxScaler for efficient and consistent normalization\n",
    "\n",
    "    # Retrieve the GitHub repository URL stored in Kaggle's secret management system\n",
    "    user_secrets = UserSecretsClient()\n",
    "    secret_value_0 = user_secrets.get_secret(\"repo_owner_mail_youtube_analysis\")\n",
    "    secret_value_1 = user_secrets.get_secret(\"repo_owner_youtube_analysis\")\n",
    "    secret_value_2 = user_secrets.get_secret(\"repo_url_youtube_analysis\")\n",
    "\n",
    "    \n",
    "    email = secret_value_0  # URL for the GitHub repository used in this pipeline\n",
    "    name = secret_value_1\n",
    "    repo_url = secret_value_2\n",
    "\n",
    "    # Set the timezone to Indian Standard Time (IST) for consistent timestamping\n",
    "    ist = timezone('Asia/Kolkata')\n",
    "\n",
    "    # Define paths for various directories used in the data pipeline\n",
    "    kaggle_repo_url = '/kaggle/working/DevOps-YouTube-Trends'  # Local path to the cloned GitHub repository\n",
    "    destination_path = '/kaggle/working/DevOps-YouTube-Trends/FeatureEngineering/Daily'  # Directory for storing feature-engineered data\n",
    "    DataCleaning_path = '/kaggle/working/DevOps-YouTube-Trends/DataCleaning/Daily'  # Directory for cleaned data files\n",
    "    requirement_path = '/kaggle/working/DevOps-YouTube-Trends/Requirement/Daily'  # Directory for requirement-related files\n",
    "\n",
    "    # Configure pandas display settings to show all columns and rows for better visibility during debugging\n",
    "    pd.set_option(\"display.max_columns\", None)  # Display all columns without truncation\n",
    "    pd.set_option(\"display.max_rows\", None)  # Display all rows without truncation\n",
    "\n",
    "    # Execute the main function to run the data pipeline\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "503f7d7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T18:14:03.121292Z",
     "iopub.status.busy": "2025-03-19T18:14:03.120889Z",
     "iopub.status.idle": "2025-03-19T18:14:03.130707Z",
     "shell.execute_reply": "2025-03-19T18:14:03.129394Z"
    },
    "papermill": {
     "duration": 0.021403,
     "end_time": "2025-03-19T18:14:03.132899",
     "exception": false,
     "start_time": "2025-03-19T18:14:03.111496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution ended.\n"
     ]
    }
   ],
   "source": [
    "print(\"Execution ended.\")\n",
    "\n",
    "# Record the end time of execution\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Calculate the time taken for execution\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "# Get the current time in the 'Asia/Kolkata' timezone\n",
    "current_time = datetime.datetime.now(timezone('Asia/Kolkata')).strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "\n",
    "# Create a new row as a DataFrame\n",
    "new_row = pd.DataFrame([{\n",
    "    'ScriptFile': 'feature-engineering.ipynb',\n",
    "    'StartTime': start_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'EndTime': end_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'TimeTaken': str(time_taken),\n",
    "    'Date': current_time\n",
    "}])\n",
    "\n",
    "# Append the new row using pd.concat()\n",
    "FileExecution = pd.concat([FileExecution, new_row], ignore_index=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "# display(FileExecution)\n",
    "\n",
    "# Save the DataFrame to a JSON file\n",
    "FileExecution.to_json(f\"{current_time}_ScriptFileExecution.json\", orient=\"records\", indent=4)\n",
    "# print(FileExecution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbb31120",
   "metadata": {
    "_cell_guid": "dec5041e-9fba-47fe-b724-7594fccbb395",
    "_uuid": "8667fa74-15b1-449f-8d3a-9df37e50b90f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-19T18:14:03.150631Z",
     "iopub.status.busy": "2025-03-19T18:14:03.150233Z",
     "iopub.status.idle": "2025-03-19T18:14:04.137539Z",
     "shell.execute_reply": "2025-03-19T18:14:04.135971Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.99818,
     "end_time": "2025-03-19T18:14:04.139663",
     "exception": false,
     "start_time": "2025-03-19T18:14:03.141483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-19_23_44_03_ScriptFileExecution.json\n",
      "Already cloned and the repo file exists\n",
      "Successfully pulled the git repo before push\n",
      "Changes committed successfully.\n",
      "Execution Tracking file successfully pushed to GitHub!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Automates the process of identifying the latest .json file, copying it \n",
    "to a GitHub repository, and pushing the changes.\n",
    "\n",
    "Args:\n",
    "    None\n",
    "\n",
    "Returns:\n",
    "    bool: True if the process completes successfully and the file is pushed to GitHub, \n",
    "          False if an error occurs during any step.\n",
    "\"\"\"\n",
    "# List all files in the working directory\n",
    "output_files = os.listdir('/kaggle/working')\n",
    "\n",
    "try:\n",
    "    # Filter and find the most recent .json file\n",
    "    json_files = [file for file in output_files if file.endswith(\"ScriptFileExecution.json\")]\n",
    "    if json_files:\n",
    "        LatestFiles = max(json_files, key=os.path.getctime)  # Get the latest file based on creation time\n",
    "    else:\n",
    "        raise ValueError(\"No JSON files found!\")  # Raise an error if no JSON files are found\n",
    "except ValueError as e:\n",
    "    print(f\"An error occurred at fetching recent .json file: {e}\")\n",
    "\n",
    "# Define repository and destination paths\n",
    "kaggle_repo_url = '/kaggle/working/DevOps-YouTube-Trends'\n",
    "destination_path = '/kaggle/working/DevOps-YouTube-Trends/ExecutionTracker/Daily'\n",
    "\n",
    "print(LatestFiles)  # Print the latest JSON file name\n",
    "try:\n",
    "    # Check if the repository already exists\n",
    "    if os.path.exists(kaggle_repo_url):\n",
    "        print(\"Already cloned and the repo file exists\")\n",
    "        repo = git.Repo(kaggle_repo_url)  # Access the existing repository\n",
    "        repo.config_writer().set_value(\"user\", \"name\", name).release()\n",
    "        repo.config_writer().set_value(\"user\", \"email\", email).release()\n",
    "        origin = repo.remote(name='origin')  # Get the remote repository\n",
    "        origin.pull()  # Pull the latest changes from the repository\n",
    "        print(\"Successfully pulled the git repo before push\")\n",
    "    else:\n",
    "        # Clone the repository if it doesn't exist\n",
    "        repo = git.Repo.clone_from(repo_url, kaggle_repo_url)\n",
    "        repo.config_writer().set_value(\"user\", \"name\", name).release()\n",
    "        repo.config_writer().set_value(\"user\", \"email\", email).release()\n",
    "        print(\"Successfully cloned the git repo\")\n",
    "    \n",
    "    # Check if the destination path exists, and copy the latest file\n",
    "    if os.path.exists(destination_path):\n",
    "        shutil.copyfile(f'/kaggle/working/{LatestFiles}', f'{destination_path}/{LatestFiles}')\n",
    "    else:\n",
    "        # Create the destination directory if it doesn't exist\n",
    "        os.makedirs(destination_path)\n",
    "        shutil.copyfile(f'/kaggle/working/{LatestFiles}', f'{destination_path}/{LatestFiles}')\n",
    "    \n",
    "    # Initialize the repository for git operations\n",
    "    repo = Repo(kaggle_repo_url)\n",
    "    # Add the copied file to the staging area\n",
    "    repo.index.add([f\"{destination_path}/{LatestFiles}\"])\n",
    "    \n",
    "    # Create a timestamp for the commit message\n",
    "    ist = timezone('Asia/Kolkata')  # IST timezone\n",
    "    timestamp = datetime.datetime.now(ist).strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    \n",
    "    # Commit the changes with a message including the timestamp and file name\n",
    "    if repo.is_dirty(untracked_files=True):\n",
    "        repo.index.commit(f\"{timestamp} Added files from Kaggle notebook, {LatestFiles}\")\n",
    "        print(\"Changes committed successfully.\")\n",
    "    else:\n",
    "        # If no changes are detected, create an empty commit\n",
    "        repo.git.commit(m=\"Empty commit to trigger contribution\", allow_empty=True)\n",
    "        print(\"Empty commit created as no changes were detected.\")\n",
    "    \n",
    "    # Push the changes to the remote repository\n",
    "    origin = repo.remote(name=\"origin\")\n",
    "    origin.push()\n",
    "    print(\"Execution Tracking file successfully pushed to GitHub!\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle any errors that occur during the git automation process\n",
    "    print(f\"An error occurred at git automation code: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ad5ea",
   "metadata": {
    "papermill": {
     "duration": 0.007275,
     "end_time": "2025-03-19T18:14:04.154873",
     "exception": false,
     "start_time": "2025-03-19T18:14:04.147598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 33.931726,
   "end_time": "2025-03-19T18:14:05.084567",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-19T18:13:31.152841",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
