[
    {
        "channelId":"UCkw4JCwteGrDHIsyIIKo4tQ",
        "channelName":"edureka!",
        "videoId":"hQcFE0RD0cQ",
        "videoTitle":"DevOps Tutorial for Beginners | Learn DevOps in 7 Hours - Full Course | DevOps Training | Edureka",
        "videoPublishYear":2019,
        "videoPublishMonth":6,
        "videoPublishDay":30,
        "videoPublishTime":"08:30:02",
        "videoPublishedOn":"2019-06-30T08:30:02Z",
        "videoPublishedOnInSeconds":1561883402,
        "videoViewCount":3551395,
        "videoLikeCount":46175,
        "videoCommentCount":318,
        "videoCategoryId":27,
        "videoDefaultAudioLanguage":"en",
        "videoDuration":"PT6H47M13S",
        "videoDurationInSeconds":24433,
        "videoContentType":"Video",
        "videoDimension":"2d",
        "videoDefinition":"hd",
        "videoCaption":"true",
        "videoLicensedContent":true,
        "videoProjection":"rectangular",
        "channelCustomUrl":"@edurekain",
        "channelPublishYear":2012,
        "channelPublishMonth":6,
        "channelPublishDay":29,
        "channelPublishTime":"06:12:26",
        "channelPublishedOn":"2012-06-29T06:12:26Z",
        "channelPublishedOnInSeconds":1340950346,
        "channelCountry":"IN",
        "channelViewCount":448778073,
        "channelSubscriberCount":4360000,
        "channelVideoCount":11637,
        "videoPublishedWeekDay":"Sunday",
        "videoDurationClassification":"Ultra Long",
        "channelAgeInYears":12.8160064371,
        "channelNormalizedViewCount":0.5251010693,
        "channelNormalizedSubscriberCount":0.3694912583,
        "channelNormalizedVideoCount":0.6599364791,
        "channelNormalizedChannelAge":0.6614709065,
        "channelGrowthScore":76.4031250633,
        "videoAgeInDays":2120.7467939815,
        "videoViewsPerDay":1674.5964244312,
        "videoLikeToViewRatio":0.013001933,
        "videoCommentToViewRatio":0.0000895423,
        "videoEngagementScore":83769.0061052485,
        "channelGrowthScoreRank":4,
        "videoEngagementScoreRank":1,
        "country_code":"IN",
        "country_name":"India",
        "continent":"Asia",
        "continent_code":"AS",
        "it_hub_country":"Yes",
        "videoTranscript":"Welcome everyone to a Edureka YouTube channel. My name is Saurabh and today I'll be taking you through this entire session on Devops\nfull course. So we have designed this crash course in such a way that it starts from the\nbasic topics and also covers the advanced ones. So we'll be covering all the stages\nand tools involved in Devops. So this is how the modules are structured. We'll start by\nunderstanding. What is the meaning of devops? What was the methodology before devops? Right?\nSo all those questions will be answered in the first module. Then we are going to talk\nabout what is git how it works. And what is the meaning of Version Control and how we\ncan achieve that with the help of git, that session will be taken by Miss Reyshma. Post that I'll be teaching you how you can create really cool digital pipelines with the help\nof Jenkins Maven and git and GitHub. After that. I'll be talking about the most famous\nsoftware containerization platform, which is docker and post that Vardhan we'll be\nteaching you how you can Kubernetes for orchestrating Docker container clusters. After that, We\nare going to talk about configuration management using ansible and puppet. Now, both of these\ntools are really famous in the market ansible is pretty trending whereas puppet is very\nmature it is there in the market since 2005 finally. I'll be teaching you how you can\nperform continuous monitoring with the help of Nagios. So let's start the session guys.\nWill Begin by understanding what is devops? So this is what we'll be discussing today.\nWe'll Begin by understanding why we need devops everything exists for a reason. So we'll try\nto figure out that reason we are going to see what are the various limitations that\nthe traditional software delivery methodologies and how it devops overcomes all of those limitations.\nThen we are going to focus on what exactly is the devops methodology and what are the\nvarious stages and tools involved in devops. And then finally in the hands on part I will\ntell you how you can create a docker image how you can build it test it and even push\nit onto Docker Hub in an automated fashion using Jenkins. So I hope you all are clear with the\nagenda. So let's move forward guys and we'll see why we need DevOps. So guys, let's start\nwith the waterfall model. Now before devops organizations were using this particular software\ndevelopment methodology. It was first documented in the year 1970 by Royce and was the first\npublic documented life cycle model. The waterfall model describes a development method that\nis linear and sequential waterfall development has distinct goals for each phase of development.\nNow, you must be thinking why the name waterfall model because it's pretty similar to a waterfall.\nNow what happens in a waterfall once the water has flowed over the edge of the cliff. It\ncannot turn back the same is the case for waterfall development strategy as well. An\napplication will go to the next stage only when the previous stage is complete. So let\nus focus on what are the various stages involved in waterfall methodology. So notice the diagram\nthat is there in front of your screen. If you notice it's almost like a waterfall or\nyou can even visualize it as a ladder as well. So first what happens the client gives requirement\nfor an application. So you gather that requirement and you try to analyze it then what happens\nyou design the application how the application is going to look like. Then you start writing\nthe code for the application and you build it when I say build it involves multiple think\ncompiling your application, you know unit testing then even it involves packaging is\nwell after that it is deployed onto the test servers for testing and then deployed onto\nthe broad service for release. And once the application is life. It is monitored. Now.\nI know this small looks perfect and trust me guys. It was at that time, but think about\nit what will happen if we use it. Now fine. Let me give you a few disadvantages of this\nmodel. So here are a few disadvantages. So first one is once the application is in the\ntesting stage. It is very difficult to go back and change something that was not well\nthought out in the concept stage now what I mean by that suppose you have written the\ncode for the entire application but in testing there's some bug in that particular application\nnow in order to remove that bug you need to go through the entire source code of the application\nwhich used to take a lot of time, right? So that is Very big limitation of waterfall model\napart from that. No working software is produced until late during the life cycle. We saw that\nwhen we are discussing about various stages of what for more there are high amount of\nrisk and uncertainty which means that once your product is life it is there in the market\nthen if there is any bug or any downtime, then you have to go through the entire source\ncode of the application again, you have to go through that entire process of waterfall\nmodel that we just saw in order to produce a working software again, right? So that's\nhow it used to take. A lot of time. There's a lot of risk and uncertainty and imagine\nif you have upgraded some software stack in your production environment and that led to\nthe failure of your application now to go back to the previous table version used to\nalso take a lot of time now, it is not a good model for complex and object oriented projects\nand it is not suitable for the Project's where requirements are at a moderate to high risk\nof changing. So what I mean by that suppose your client has given you a requirement for\na web application today now you have taken Own sweet time and you are in a condition\nthe release the application say after one year now after one year, the market has changed.\nThe client does not want a web application. He's looking for a mobile application now,\nso this type of model is not suitable where requirements are at a moderate to high risk\nof changing. So there's a question popped in my screen is from Jessica. She's asking\nso all the iteration in the waterfall model goes through all the stages. Well, there are\nno I tration as such Jessica. First of all, it is not agile methodology or devops. It\nis waterfall model, right? There are no I trations once the stage is complete then only\nit will be good. It will be going to the next stage. So there are no I trations as such\nif you're talking about the application and it is life and then there is some bug or there\nis some downtime then at that time based on the kind of box, which is there in the application\nSuppose. There might be a bug because of some flawed version of a software stack installed\nin your production environment. Probably some upgraded version because if that your application\nis not working properly. You need to roll back to the previous table version of the\nsoftware stack in your production environment. So that can be one bug apart from that. There\nmight be bugs related to the code in which you have to check the entire source code of\nthe application again. Now if you look at it to roll back and incorporate the feedback\nthat you have got is used to take a lot of time. Right? So I hope this answers your question.\nAll right, she's finally the answer any of the questions any other doubt you have guys\nyou can just go ahead and ask me find so there are no questions right now. So I hope you\nhave understood what was the relation with waterfall model. What are the various limitations\nof this waterfall model. Now we are going to focus on the next methodology that is called\nthe agile methodology. Now agile methodology is a practice that promotes continuous iteration\nof development and testing throughout the software development life cycle of the project.\nSo the development and the testing of an application used to happen continuously with the agile\nmethodology. So what I mean by that if you focus on a diagram that is there in front\nof your screen, so here we get the feedback from the testing that we have done in the\nprevious iteration. We design the application again, then we develop it there again. We\ntest it then we discover few things that we can incorporate in the application. We again\ndesign it develop it and there are multiple I trations involved in development and testing\nof a particular application cinestyle. Methodology. Each project is broken up into several I trations\nand all I tration should be of the same time duration and generally it is between 2 to\n8 weeks and at the end of each iteration of working for dr. Should be delivered. So this\nis what agile methodology in a nutshell is now let me go ahead and compare this with\nthe waterfall model. Now if you notice in the diagram that is there in front of your\nscreen, so waterfall model is pretty linear and it's pretty straight as you can see from\nthe diagram that we analyze requirements. We plan it design. It build it test it. And\nthen finally we deploy it onto the processor was for release, but when I talk about the\nagile methodology over here the design build and testing part is happening continously.\nWe are writing the code. We are building the application. We are testing it continuously\nand there are several iterations involved in this particular stage. And once the final\ntesting is done. It is then deployed onto the broad service for release, right? So agile\nmethodology basically breaks down the entire software delivery life cycle into small sprains\nor iterations that we call it due to which the development and the testing part of the\nsoftware delivery life cycle used to happen continously. Let's move forward and we are\ngoing to focus on what are the various limitations of agile methodology the first and the biggest\nlimitation of agile methodology is that the deaf part of the team was pretty agile right\nthe development and testing used to happen continuously. But when I talk about deployment\nthen that was not continuous there were still a lot of conflicts happening between the Devon\nthe off side of the company the dev team wants agility. Whereas the Ops Team want stability\nand there's a very common conflict that happens and a lot of you can actually relate to it\nthat the code works fine in the developers laptop, but when it reaches to production\nthere is some bug in the application or it does not work any production at all. So this\nis because if you know some inconsistency in the Computing environment And that has\ncaused that and due to which the operations team and the dev team used to fight a lot.\nThere are a lot of conflicts guys at that time happening. So agile methodology made\nthe deaf part of the company pretty agile, but when I talk about the off side of the\ncompany, they needed some solution in order to solve the problem that I've just discussed\nright? So I hope you are able to understand what kind of a problem I'm focusing on. If\nyou go back to the previous diagram as well so over here if you notice only the design\nbuild and test or you can say Development building and testing part is continuous, right\nthe deployment is still linear. You need to deploy it manually on to the various products\novers. That's what you was happening in the agile methodology. Right? So the error that\nI was talking about you too busy. Our application is not working fine. I mean once your application\nis life and do you do some software stack in the production environment? It doesn't\nwork properly now to go back and change something in the production environment used to take\na lot of time. For example, you know, you have upgraded some particular software stack\nand because of that your application is Doll working it fails to work now to go back to\nthe previous table version of the software stack the operations team was taking a lot\nof time because they have to go through the login scripts that they have written on in\norder to provision the infrastructure. So let me just give you a quick recap of the\nthings that we have discussed till now, we have discussed quite a lot of history. We\nstarted with the waterfall model the traditional waterfall model be understood what are its\nvarious stages and what are the limitations of this waterfall mode? Then we went ahead\nand understood what exactly the design methodology and how is it different from the waterfall\nmodel and what are the various limitations of the agile methodology? So this is what\nwe have discussed till now now we are going to look at the solution to all the problems\nthat we have just discussed and the solution is none other than divorce divorce is basically\na software development strategy which Bridges the gap between the deaf side and the offside\nof the company. So devops is basically a term for a group of Concepts that while not all\nnew half catalyze into a movement and a rapidly spreading. Well, the technical community like\nany new and popular term people may have confused and sometimes contradictory impressions of\nwhat it is. So let me tell you guys devops is not a technology. It is a methodology.\nSo basically devops is a practice that equated to the study of building evolving and operating\nrapidly changing systems at scale. Now. Let me put this in simpler terms. So devops is\nthe practice of operations and development Engineers participating together in the entire\nsoftware life cycle from design through the development process to production support\nand you can also say that devops is also characterized by operation staff making use many of the\nsame techniques as Developers for this system work. I'll explain you that how is this definition\nrelevant because all we are saying here is devops is characterized by operation staff\nmaking use many of the same techniques as Developers for their systems work seven. I\nwill explain you infrastructure as code you will understand why I am using this particular\ndefinition. So as you know, that devops is a software development strategy which Bridges\nthe gap between the dev part in the upside of the company and helps us to deliver good\nquality software in time and how this happens this happens because of various stages and\ntools involved in Des Moines. So here is a diagram which is nothing but an infinite Loop\nbecause everything happens continuously in Dev Ops guys, everything starting from coding\ntesting deployment monitoring everything is happening continuously, and these are the\nvarious tools which are involved in the devops methodologic, right? So not only the knowledge\nof these tools are important for a divorce engineer, but also how to use these tools.\nHow can I architect my software delivery lifecycle such that I get the maximum output right?\nSo it doesn't mean that you know, if I have a good knowledge of Jenkins or gate or docker\nthen I become a divorce engineer. No that is not true. You should know how to use them.\nYou should know where to use them to get the maximum output. So I hope you have got my\npoint what I'm trying to say here in the next slide. Be discussing about various stages\nthat are involved in devops fine. So let's move forward guys and we are going to focus\non various stages involved in divorce. So these are the various stages involved in devops.\nLet me just take you through all these stages one by one starting from Version Control.\nSo I'll be discussing all of these stages one by one as well. But let me just give you\nan entire picture of these stages in one slide first. So Version Control is basically maintaining\ndifferent versions of the code what I mean by that Suppose there are multiple developers\nwriting a code for a particular application. So how will I know that which developer has\nmade which commits at what time and which commits is actually causing the error and\nhow will I revert back to the previous commit so I hope you are getting my point my point\nhere is how will I manage that source code suppose developer a has made a commit and\nthat commit is causing some error. Now how will I know the developer a has made that\ncommit and at what time he made that comment and very the code was that editing happened,\nright? So all of these questions can be answered once you use Version Control tools like git\nsubversion. XXXX of we are going to focus on getting our course. So then we have continuous\nintegration. So continuous integration is basically building your application continuously\nwhat I mean by that suppose any developer made a change the source code a continuous\nintegration server should be able to pull that code. I am prepare a built now when I\nsay build people have this misconception of you know, only compiling the source code.\nIt is not true guys includes everything starting from compiling your source code validating\nyour source code code review unit, testing integration, testing, etc, etc. And even packaging\nyour application as well. Then comes continuous delivery. Now the same continuous integration\ntool that we are using suppose Jenkins. Now what Jenkins will do once the application\nis built. It will be deployed onto the test servers for testing to perform, you know,\nuser acceptance test or end user testing whether you call it there will be using tools like\nselenium right for performing automation testing. And once that is done it will be then deployed\nonto the process servers for release, right that is called continuous deployment and here\nwe'll be using configuration management and Tools so this is basically to provision your\ninfrastructure to provision your Prada environment and let me tell you guys continuous deployment\nis something which is not a good practice because before releasing a product in the\nmarket, there might be multiple checks that you want to do before that right? There might\nbe multiple other testings that you want to do. So you don't want this to be automated\nright? That's why continuous deployment is something which is not preferred after continuous\ndelivery. We can go ahead and manually use configuration management tools like puppet\nchef ansible and salts tag, or we can even use Docker for a similar purpose and then\nwe can go ahead and deploy it onto the Crossovers for release. And once the application is live.\nIt is continuously monitored by tools like Nagi Os or Splunk, which will provide the\nrelevant feedback to the concern teams, right? So these are various stages involved in devops,\nright? So now let me just go back to clear if there are doubts. So this is our various\nstages are scheduled various jobs schedule. So we have Jenkins here. We have a continuous\nintegration server. So what Jenkins will do the moment any developer makes a change in\nthe source code it Take that code and then it will trigger a built using tools like Maven\nor and or Gradle. Once that is done. It will deploy it onto the test servers for testing\nfor end user testing using tools like selenium j-unit Etc. Then what happens it will automatically\ntake that tested application and deploy it onto the process servers for release, right?\nAnd then it is continuously monitored by tools. Like Nagi was plunky LK cetera et cetera.\nSo Jenkins is basically heart of devops life cycle. It gives you a nice 360 degree view\nof your entire software delivery life cycle. So with that UI you can go ahead and have\na look how your application is doing currently right? We're in which stage it is in right\nnow testing is done or not. All those things. You can go ahead and see in the Jenkins dashboard\nright? There might be multiple jobs running in the Jenkins dashboard that you can see\nand it gives you a very good picture of the entire software delivery life cycle. Uh, don't\nworry. I'm going to discuss all of these stages in detail when we move forward. We are going\nto discuss each of these stages one by one. Eating from source code management or even\ncall us Version Control. Now what happens in source code management? There are two types\nof source code management approaches one is called centralized Version Control. And another\none is called the distributed Version Control the source code management. Now imagine there\nare multiple developers writing a code for an application if there is some bug introduced\nhow will we know which commits has caused that error and how will I revert back to the\nprevious version of the code in order to solve these issues source code management tools\nwere introduced and there are two types of source code management tools one is called\ncentralized Version Control and another is distributed Version Control. So let's discuss\nthe centralized Version Control first. So centralized version control system uses a\ncentral server to store all the files and enables team collaboration. It works in a\nsingle repository to which users can directly access a central server. So this is what happens\nhere guys. So every developer has a working copy the working directory. So the moment\nthey want to make any change in the source code. They can go ahead and make a comment\nin the shared repository right and they can even update their working. By you know pulling\nthe code that is there in the repository as well. So the repository then the diagram that\nyour nose noticing indicates a central server that could be local or remote which is directly\nconnected to each of the programmers workstation. As you can see now every programmer can extract\nor update their workstation or the data present in the repository or can even make changes\nto the data or committed in the repository. Every operation is performed directly on the\ncentral server or the central repository, even though it seems pretty convenient to\nmaintain a single repository, but it has a lot of drawbacks. But before I tell you the\ndrawbacks, let me tell you what advantage we have here. So first of all, if anyone makes\na comment in the repository, then it will be a commit ID Associated to it and there\nwill always be a commit message. So, you know, which person has made that commit and at what\ntime and where in the code basically, right so you can always revert back but let me now\ndiscuss few disadvantages. First of all, it is not locally available. Meaning you always\nneed to be connected to a network to perform any action. It is always not available locally,\nright? So you need to be connected with the some sort of network. Basically since everything\nis centralized in case of the central server getting crashed or corrupted. It will result\nin losing the entire data of the project. Right? So that's a very serious issue guys.\nAnd that is one of the reasons why Industries don't prefer centralized Version Control System,\nthat's talk about the distributed version control system. Now now these systems do not\nnecessary rely on a central server to store all the versions of the project file. So in\ndistributed Version Control System, every contributor has a local copy or clone of the\nmain repository as you can see, I'm highlighting with my cursor right now that is everyone\nmaintains a local repository of their own which contains all the files and metadata\npresent in the main repository. As you can see then the diagram is well, every programmer\nmaintains a local repository on its own which is actually the copy or clone of the central\nrepository on their hard drive. They can commit and update the local repository without any\ninterference. They can update the local repositories with new data coming from the central server\nby an operation called pull and effect changes the main repository by an operation called\npush write operation called push from the local post re now. You must be thinking what\nadvantage we get here. What are the advantages of distributed version control over the centralized\nVersion Control now basically the act of cloning and entire repository gives you that Advantage.\nLet me tell you how now all operations apart from push-and-pull are very fast because the\ntool only needs to access the hard drive not a remote server, hence, you do not always\nneed an internet connection committing new change sets can be done locally without manipulating\nthe data on the main proposed three. Once you have a group of change sets ready. You\ncan push them all at once. So what you can do is you can ask the commit to your local\nrepository, which is there in your local hard drive. You can commit the changes. Are you\nwant in the source code you can you know, once you review it and then once you have\nquite a lot of It's ready. You can go ahead and push it onto the central server as well\nas the central server gets crashed at any point of time. The lost data can be easily\nrecovered from any one of the contributors local repository. This is one very big Advantage\napart from that since every contributor has a full copy of the project repository. They\ncan share changes with one another if they want to get some feedback before affecting\nthe changes in the main repository as well. So these are the various ways in which you\nknow distributed version control system is actually better than a centralized version\ncontrol system. So we saw the two types of phones code Management systems and I hope\nyou have understood it. We are going to discuss a one source code management tool called gate,\nwhich is very popular in the market right now almost all the companies actually use\nget for now. I'll move forward and we'll go into focus on a source code management tool\na distributed Version Control tool that is called as get now before I move forward guys.\nLet me make this thing clear. So when I say Version Control or source code management,\nit's one in the same thing. Let's talk about get now now git is a distributed Version Control\ntool. Boards distributed nonlinear workflows by providing data Assurance for developing\nquality software, right? So it's a pretty tough definition to follow but it will be\neasier for you to understand with the diagram that is there in front of your screen. So\nfor example, I am a developer and this is my working directory right now. What I want\nto do is I want to make some changes to my local repository because it is a distributed\nVersion Control System. I have my local repository as well. So what I'll do I'll perform a get\nadd operation now because of get add whatever was there in my working directory will be\npresent in the staging area. Now, you can visualize the staging area as something which\nis between the working directory and your local repository, right? And once you have\ndone get ad you can go ahead and perform git commit to make changes to your local repository.\nAnd once that is done you can go ahead and push your changes to the remote repository\nas well. After that you can even perform get pull to add whatever is there in your remote\nrepository to your local repository and perform get check out to our everything which was\nthere in your Capacity of working directory as well. All right, so let me just repeat\nit once more for you guys. So I have a working directory here. Now in order to add that to\nmy local repository. I need to First perform get add that will add it to my staging area\nstaging area is nothing but area between the working directory and the local repository\nafter guitar. I can go ahead and execute git commit which will add the changes to my local\nrepository. Once that is done. I can perform get push to push the changes that I've made\nin my local repository to the remote repository and in order to pull other changes which are\nthere in the remote repository of the local repository. You can perform get pull and finally\nget check out that will be added to your working directory as well and get more which is also\na pretty similar command now before we move forward guys. Let me just show you a few basic\ncommands of get so I've already installed get in my Center is virtual machine. So let\nme just quickly open my Center as virtual machine to show you a few basic operations\nthat you can perform with get device virtual machine, and I've told you that have already\ninstalled get now in order to check the version of get you can just Then he'd get - - version\nand you can see that I have two point seven point two here. Let me go ahead and clear\nmy terminal. So now let me first make a directory and let me call this as a deal breaker - repository\nand I'll move into this array core repository. So first thing that I need to do is initialize\nthis repository as an empty git repository. So for that all I have to type here is get\nin it and it will go ahead and initialize this R empty directory as a local git repository.\nSo it has been initialized now as you can see initialise empty git repository in home\nand Drake I drink - report dot kit or right then so over here. I'm just going to create\na file of python file. So let me just name that as a deer a card dot p y and I'm going\nto make some changes in this particular files. So I'll use G edit for that. I'm just going\nto write in here, uh normal print statement. Welcome to Ed Eureka close the parenthesis\nsave it. Close it. Let me get my terminal now if I hit an LS command so I can see that\nedeka dot py file is here. Now. If you can recall from the slides, I was telling you\nin order to add a particular file or a directory into the local git repository first. I need\nto add it to my staging area and how will I do that by using the guitar? Come on. So\nall I have to type here is get ad at the name of my file, which is edureka.py then here\nwe go. So it is done now now if I type in here git status it will give me the files\nwhich I need to commit. So this particular command gives me the status status as a little\ntell me model files. They need to commit to the local repository. So it says when you\nfile has been created that is in the record or py in the state and it is present in the\nstaging area and I need to come at this particular Phi. So all I have to type here is git commit\n- M and the message that I want so I'll just type in here first commit and here we go.\nSo it is successfully done now. So I've added a particular file to my local git repository.\nSo now what I'm going to show you is basically how to deal with the remote repositories.\nSo I have a remote git repository present on GitHub. So I have created a GitHub account.\nThe first thing that you need to do is create a GitHub account and then you can go ahead\nand create a new repository there and then I'll tell you how to add that particular repository\nto a local git repository. Let me just go to my browser once and me just zoom in a bit.\nAnd yeah, so this is my GitHub account guys. And what I'm going to do is I'm first going\nto go to this repository stab and I'm going to add one new repository. So I'll click on\nnew. I'm going to give a name to this repository. So whatever name that you want to give you\njust go ahead and do that. Let me just write it here. Get - tutorial - Dev Ops, whatever\nname that you feel like just go ahead and write that I'm going to keep it public if\nyou want any description you can go ahead and give that and I can also initialize it\nwith a readme create a posse and that's all you have to do in order to create a remote\nGitHub repository now over here. You can see that there's only one read me dot MD file.\nSo what I'm going to do, I'm just going to copy this particular SSH link and I'm going\nto perform git remote add origin and the link there are just copy. I'll paste it here and\nhere we go. So this has basically added my remote repository to my local repository.\nNow, what I can do is I can go ahead and pull whatever is there in my remote repository\nto my local git repository for that. All our to type here is git pull origin master and\nhere we go. Set is done. Now as you can see that I've pulled all the changes. So let me\nclear my terminal and hit an endless command. So you'll find read me dot MD present here\nright now. What I'm going to show you is basically how to push this array card or py file onto\nmy remote repository. So for that all I have to type here is git push origin master and\nhere we go. So it is done. Now. Let me just go ahead and refresh this particular repository\nand you'll find Erica py file here. Let me just go ahead and reload this so you can see\na record or py file where I've written welcome to edit a car. So it's that easy guys. Let\nme clear my terminal now. So I've covered few basics of get so let's move forward with\nthis devops tutorial and we are going to focus on the next stage which is called continuous\nintegration. So we have seen few basic commands of get we saw how to initialize an empty directory\ninto a git repository how we can you know, add a file to the staging area and how we\ncan go ahead and commit in the local repository. After that. We saw how we can push the changes\nin the local repository to the remote repository. My repository was on GitHub. I told you how\nto connect to the remote repository and then how even you can pull the changes from the\nremote repository rights all of these things we have discussed in detail. Now, let's move\nforward guys in we are going to focus on the next stage which is called continuous integration.\nSo continuous integration is basically a development practice in which the developers are required\nto commit changes. Just the source code in a shared repository several times a day, or\nyou can say more frequently and every commit made in the repository is then built this\nallows the teams to detect the problems early. So let us understand this with the help of\nthe diagram that is there in front of your screen. So here we have multiple developers\nwhich are writing code for a particular application and all of them are committing code to a shared\nrepository which can be a git repository or subversion repository from there the Jenkins\nserver, which is nothing but a continuous integration tool will pull that code the moment\nany developer commits a change in the source code the moment any developer coming to change\nin the source code Jenkins server will pull that it will prepare a built now as I have\ntold you earlier as well build does not only mean compiling the source code. It includes\ncompiling but apart from that there are other things as well. For example code review unit\ntesting integration testing, you know packaging your application into an executable file.\nIt can be a war file. It can be a jar file. So it happens in a continuous manner the moment\nany developer coming to change in the source code Jenkins server will pull that prepare\na bill. Right. This is called as continuous integration. So Jenkins has various Tools\nin order to perform this so it has various tools for development testing and deployment\nTechnologies. It has well over 2,500 plugins. So you need to install that plug-in and you\ncan just go ahead and Trigger whatever job you wanted with the help of Jenkins. It is\noriginally written in Java. Right and let's move forward and we are going to focus on\ncontinuous delivery now, so continuous delivery is nothing but taking continuous integration\nto The Next Step. So what are we doing in a continuous manner or in an automated fashion?\nWe are taking this build application onto the test server for end user testing or unit\nor user acceptance test, right? So that is basically what is continuous delivery. So\nlet us just summarize containers delivery again moment. Any developers makes a change\nin the source code. Jenkins will pull that code prepare a built once build a successful.\nIt will take the build application and Jenkins will deploy it onto the test server for end\nuser testing or user acceptance test. So this is basically what continuous delivery is is\nhappens in a continuous fashion. So what advantage we get here? Basically if they the build failure\nthen we know which commits has caused that error and we don't need to go through the\nentire source code of the application similarly for testing even if any bug appears in testing\nis well, we know which comment has caused that are Ernie can just go ahead and you know\nhave a look at that particular comment instead of checking out the entire source code of\nthe application. So they basically this system allows the team to detect problems early,\nright as you can see from the diagram as web. You know, if you want to learn more about\nJenkins, I'll leave a link in the chat box. You can go ahead and refer that and people\nare watching it on YouTube can find that link in the description box below now, we're going\nto talk about continuous deployment. So continuous deployment is basically taking the application\nthe build application that you have tested and deploying that onto the process servers\nfor release in an automated fashion. So once the application is tested it will automatically\nbe deployed on to the broad service for release. Now, this is something not a good practice\nas I've told you earlier as well because there might be certain checks that you need to do\nnow to release your software in the market. Are you might want to Market your product\nbefore that? So there are a lot of things that you want to do before deploying your\napplication. So it is not advisable or a good practice to you know, actually automatically\ndeploying your application onto the processor which for release so this is basically continuous\nintegration delivery and deployment any questions. You have guys you can ask me. All right, so\nDorothy wants me to repeat it. Once more sure jovial do that. Let's start with continuous\nintegration. So continuous integration is basically committing the changes in the source\ncode more frequently and every commit will then be built using a Jenkins server, right\nor any continuous integration server. So this Jenkins what it will do it will trigger a\nbuild the moment any developer commits a change in the source code and build includes of compiling\ncode review unit, testing integration testing packaging and everything. So I hope you are\nclear with what is continuous integration. It is basically continuously building your\napplication, you know, the moment any developer come in to change in the source code. Jenkins\nwill pull that code and repairable. Let's move forward and now I'm going to explain\nyou continuous delivery now incontinence delivery the package that we Created here the war of\nthe jar file of the executable file. Jenkins will take that package and it will deploy\nit onto the test server for end user testing. So this kind of testing is called the end\nuser testing or user acceptance test where you need to deploy your application onto a\nserver which can be a replica of your production server and you perform end user testing or\nyou call it user acceptance test. For example in my application if I want to check all the\nfunctions right functional testing if I want to perform functional testing of my application,\nI will first go ahead and check whether my search engine is working then I'll check whether\npeople are able to log in or not. So all those functions of a website when I check or an\napplication and I check is basically after deploying it on to apps over right? So that's\nsort of testing is basically what is your functional testing or what? I'm trying to\nrefer here next up. We are going to continuously deploy our application onto the process servers\nfor release. So once the application is tested it will be then deployed onto the broad service\nfor release and I've told you earlier is well, it is not a good practice to deploy your application\ncontinuously or in an automated fashion. So guys you have discussed a lot about Jenkins.\nHow about I show you How Jenkins UI looks like and how you can download plugins on all\nthose things. So I've already installed Jenkins in my Center is virtual machine. So let me\njust quickly open. My Center is virtual machine. So guys, this is my Center is virtual machine\nagain and over here. I have configured my Jenkins on localhost port 8080 \/ Jenkins and\nhere we go. Just need to provide the username and password that you have given when you\nare installing Jenkins. So this is how Jenkins looks like guys over here. There are multiple\noptions. You can just go and play around with it. Let me just take you through a few basic\noptions that are there. So when you click on new item, you'll be directed to a page\nwhich will ask you to give a name to your project. So give whatever name that you want\nto give then choose a kind of project that you want. Right and then you can go ahead\nand provide the required specifications required configurations for your project. Now when\nI was talking about plugins, let me tell you how you can actually install plug-ins. So\nyou need to go to manage and kins and here's a tab that you'll find manage plugins. In\nthis tab, you can find all the updates that are there for the plugins that you have already\ninstalled in the available section. You'll find all the available plugins that Jenkins\nsupport so you can just go ahead and search for the plug-in that you want to install just\ncheck it and then you can go ahead and install it similarly. The plug-ins that are installed\nwill be found in the install Tab and then you can go ahead and check out the advanced\ntab as well. So this is something different. Let's not just focus on this for now. Let\nme go back to the dashboard and this is basically one project that I've executed which is called\nAda Rekha Pipeline and this blue color symbolizes and it was successful the blue Colour ball\nmeans it was successful. That's how it works guys. So I was just giving you a tour to the\nJenkins dashboard will actually execute the Practical as well. So we'll come back to it\nlater. But for now, let me open my slides in will proceed with the next stage in the\ndevops life cycle. So now let's talk about configuration management. So what exactly\nis configuration management, so now let me talk about few issues with the deployment\nof a particular application or provisioning of the server's so basically what happens,\nyou know, I've been My application but when I deployed onto the test servers or onto the\nprocess servers, there are some dependency issues because of his my application is not\nworking fine. For example in my developers laptop. There might be some software stack\nwhich was upgraded but in my prod and in the test environment, they're still using the\noutdated version of that software side because of which the application is not working fine.\nThis is just one example apart from that what happens when your application is life and\nit goes down because of some reason and that reason can be you have upgraded the software\nstack. Now, how will you go back to the previous table version of that software stack. So there\nare a lot of issues with you know, the admin side of the company the upside of the company\nwhich were removed the help of configuration management tools. So, you know before Edmonds\nused to write these long scripts in order to provision the infrastructure whether it's\nthe test environment of the prod environment of the dev environment, so they utilize those\nlong scripts, right which is prone to error plus. It used to take a lot of time and apart\nfrom that the Edmund who has written that script. No one else can actually recognize\nwhat's the problem with it once if you have to debug it, so there are a lot of problems\nat work. Are with the admin side or the Absurd the company which were removed by the help\nof configuration management tools and when very important concept that you guys should\nunderstand is called infrastructure as code which means that writing code for your infrastructure.\nThat's what it means suppose if I want to install lamp stack on all of these three environments\nwhether it's devtest abroad I will write the code for installing lamp stack in one central\nlocation and I can go ahead and deploy it onto devtest and prom so I have the record\nof the system State president my one central location, even if I upgrade to the next version,\nI still have the recorded the previous stable version of the software stack, right? So I\ndon't have to manually go ahead and you know write scripts and deployed onto the nodes\nthis is that easy guys. So let me just focus on few challenges at configuration management\nhelps us to overcome. First of all, it can help us to figure out which components to\nchange when requirements change. It also helps us in redoing an implementation because the\nrequirements have changed since the last implementation and very important Point guys that it helps\nus to revert to a Previous version of the component if you have replaced with a new\nbut the flawed version now, let me tell you the importance of configuration management\nthrough a use case now the best example I know is of New York Stock Exchange a software\nglitch prevented the NYC from Trading stocks for almost 90 minutes this led to millions\nof dollars of loss a new software installation caused the problem that software was installed\non 8 of its twenty trading Terminals and the system was tested out the night before however\nin the morning it failed to operate on the a term ends. So there was a need to switch\nback to the old software. Now you might think that this was a failure of nyc's configuration\nmanagement process, but in reality, it was a success as a result of proper configuration\nmanagement NYC recovered from that situation in 90 minutes, which was pretty fast have\nthe problem continued longer the consequences would have been more severe guys. So I hope\nyou have understood its importance. Now, let's focus on various tools available for configuration\nmanagement. So we have multiple tools like Papa Jeff and silence. Stack I'm going to\nfocus on pop it for now. So pop it is a configuration management tool that is used for deploying\nconfiguring and managing servers. So, let's see, what are the various functions of puppet.\nSo first of all, you can Define distinct configurations for each and every host and continuously check\nand confirm whether required configuration is in place and is not altered on the host.\nSo what I mean by that you can actually Define distinct configuration for example in my one\nparticular node. I need this office. I can another node. I need this office stack so\nI can you know, defined distinct configurations for different nodes and continuously check\nand confirm whether the required configuration is in place and is not alter and if it is\naltered pop, it will revert back to the required configurations. This is one function of puppet.\nIt can also help in Dynamic scaling up and scaling down of machines. So what will happen\nif in your company there's a big billion day sale, right and you're expecting a lot of\ntraffic. So at that time in order to provision more servers probably today our task is to\nprovision 10 servers and tomorrow you might have two revisions. Jim's right. So how will\nyou do that? You cannot go ahead and do that manually by writing scripts. You need tools\nlike puppet that can help you in Dynamic scaling up and scaling down of machines. It provides\ncontrol over all of your configured machines. So a centralized change gets propagated to\nall automatically so it follows a master-slave architecture in which the slaves will pull\nthe central server for changes made in the configuration. So we have multiple nodes there\nwhich are connected to the master. So they will poll they will check continuously. Is\nthere any change in the configuration happened the master the moment any change happen it\nwill pull that configuration and deploy it onto that particular node. I hope you're getting\nmy point. So this is called pull configuration and push configuration. The master will actually\npush the configurations on to the nose which happens in ansible and salts that but does\nnot happen in pop it in Chef. So these two tools follow full configuration and an smellin\nsalts that follows push configuration in which these configurations are pushed onto the nodes\nand here in chef and puppet. The nodes will pull that configurations. They keep on checking\nthe master at regular intervals and if there's any change in the configuration It'll pull\nit now. Let me explain you the architecture that is there in front of your screen. So\nthat is basically a typical puppet architecture in which what happens you can see that there's\na master\/slave architecture here is our puppet master and here is our puppet slave now the\nfunctions which are performed in this architecture first, the puppet agent sends the fact to\nthe puppet master. So this puppet slave will first send the fact to the Puppet Master facts\nwhat our Fox basically they are key value data appears. It represents some aspects of\nslave states such as its IP address up time operating system or whether it's a virtual\nmachine, right? So that's what basically facts are and the puppet master uses a fact to compile\na catalog that defines how the slaves should be configured. Now. What is the catalog it\nis a document that describes a desired state for each resource that Puppet Master manages.\nHonestly, then what happens the puppet slave reports back to the master indicating that\nconfiguration is complete and which is also visible in the puppet dashboard. So that's\nhow it works guys. So let's move Forward and talk about containerization. So what exactly\nis containerization so I believe all of you have heard about virtual machines? So what\nare containers containers are nothing but the lightweight alternatives to Virtual machines.\nSo let me just explain that to you. So we have Docker containers that will contain the\nbinaries and libraries required for a particular application. And that's when we call it. You\nknow, we have containerized a particular application. Right? So let us focus on the diagram that\nis there in front of your screen. So here we have host operating system on top of which\nwe have Docker engine. We have a No guest operating system here guys. It uses the host\noperating system and we're learning to Containers container one will have application one and\nit's binaries in libraries the container to will have application to and it's binaries\nand libraries. So all I need in order to run my application is this particular container\nor this particular container? Because all the dependencies are already present in that\nparticular container. So what is basically a container it contains my application the\ndependencies of my application. The binary is Ivory is required for that application.\nIs that in my container nowadays? If you must have noticed that even you want to install\nsome software you will actually get ready to use Docker container, right? That is the\nreason because it's pretty lightweight when you compare it with virtual machines, right?\nSo let me discuss a use case how you can actually use Docker in the industry. So suppose you\nhave some complex requirements for your application. It can be a microservice. It can be a monolithic\napplication anything. So let's just take microservice. So suppose you have complex requirements for\nyour microservice your you have written the dockerfile for that with the help of this\nDocker 5. I can create a Docker image. So Docker image is nothing but you know a template\nyou can think of it as a template for your Docker container, right? And with the help\nof Docker image, you can create as many Docker containers as you want. Let me repeat it once\nmore so we have written the complex requirements for a micro service application in an easy\nto write Docker file from there. We have created a Docker image and with the help of Docker\nimage we can build as many containers as we want. Now that Docker image I can upload that\nonto Docker Hub, which is nothing. Butter git repository of Docker images we can have\npublic repositories can have private repositories e and from Docker Hub any team beat staging\na production can pull that particular image and prepare as many containers as they want.\nSo what advantage we get here, whatever was there in my developers laptop, right? The\nMicrosoft is application. The guy who has written that and the requirement for that\nmicrobes obvious application. So that guy's basically a developer and because he's only\ndeveloping the application. So whatever is there in my developers laptop I have replicated\nin my staging as well as in a production. So there's a consistent Computing environment\nthroughout my software delivery life cycle. I hope you are getting my point. So guys,\nlet me just quickly brief you again about what exactly a Docker containers so just visualize\ncontainer as actually a box in which our application is present with all its dependencies except\nthe box is infinitely replicable. Whatever happens in the Box stays in the Box unless\nyou explicitly take something out or put something in and when it breaks you will just throw\nit away and get a new What so containers usually make your application easy to run on different\ncomputer. Ideally the same image should be used to run containers in every environment\nstage from development to production. So that's what basically Docker containers are. So guys.\nThis is my sent to us virtual machine here again, and I've already installed docker.\nSo the first thing is I need to start Docker for that. I'll type system CTL start docker.\nGive the password. And it has started successfully. So now what I'm going to do, there are few\nimages which are already there in Docker up which are public images. You can pull it at\nanytime you want. Right? So you can go ahead and run that image as many times as you want.\nYou can create as many containers as you want. So basically when I execute the command of\npulling an image from dog a rabbit will try to First find it locally whether its present\nor not and if it is present then it's well and good. Otherwise, we'll go ahead and pull\nit from the docker Hub. So right so before I move forward, let me just show you how dr.\nOf looks like If you have not created an account and Dock and have you need to go and do that\nbecause for executing a use case you have to do is it's free of cost. So this is our\ndoctor of looks like guys and this is my repository that you can notice here. Right? I can go\nahead and search for images here as well. So for example, if I want to search for Hadoop\nimages, which I believe one of you asked so you can find that we have Hadoop images present\nhere as well. Right? So these are nothing but few images that are there on Docker Hub.\nSo I believe now I can go back to my terminal and execute your basic Docker commands. So\nthe first thing that I'm going to execute is called Docker images which will give the\nlist of all the images that I have in my local system. So I have quite a lot of images you\ncan see right this is the size and and all those things when it was created the image.\nThis is called the image ID, right? So I have all of these things displayed on my console.\nLet me just clear my terminal now what I'm going to do, I'm going to pull an image rights.\nAll I have to type here is the awkward pull for example if I want to pull an Ubuntu image.\nJust type in here Docker pull open to and here we go. So it is using default tag latest.\nSo tag is something that I'll tell you later party at will provide the default tag latest\nall the time. So it is pulling from the docker Hub right now because it couldn't find it\nlocally. So download is completed is currently extracting it. Now if I want to run a container,\nall I have to type here is to occur and - IIT Ubuntu or you can type the image ideas. Well,\nso I am in the Ubuntu container. So I've told you how you can see the various Docker images\nof told you how you can pull an image from Docker Hub and how you can actually go ahead\nand run a container and you're going to focus on continuous monitoring now, so continuous\nmonitoring tools resolve any system errors, you know, what kind of system errors low memory\nunreachable server, etc, etc. Before they have any negative impact on your business\nproductivity. Now, what are the reasons to use continuous monitoring tools? Let me tell\nyou that it detects any network or server problems. It can determine the root cause\nof any issue. It maintains the security and availability of the services and also monitors\nin troubleshoot server performance issues. It also allows us to plan for infrastructure\nupgrades before outdated system cause failures and it can respond to issues of the first\nsign of problem and let me tell you guys these tools can be used to automatically fix problems\nwhen they are detected as well. It also ensures it infrastructure outages have a minimal effect\non your organization's bottom line and can monitor your entire infrastructure and business\nprocesses. So what is continuous monitoring it is all about the ability of an organization\nto detect report respond contain and mitigate that acts that occur on its infrastructure\nor on the software. So basically we have to monitor the events on the ongoing basis and\ndetermine what level of risk. We are experiencing. So if I have to summarize continuous monitoring\nin one definition, I will say it is the integration of an organization security tools. So we have\ndifferent security tools in an organization the integration of those tools the aggregation\nnormalization and correlation of the data that is produced by the security tools right\nnow. It happens the data that has been produced the analysis of that data based on the organization's\nrisk goals and threat knowledge and near real-time response to the risks identified is basically\nwhat is continuous monitoring and this is a very good saying guys if you can't measure\nit, you can't manage it. I hope you know what I'm talking about. Now, there are multiple\ncontinuous monitoring tools available in the market. We're going to focus on nagas now\ngive us is used for continuous monitoring of systems application services and business\nprocesses in a devops culture, right and in the event of failure nagas can alert technical\nstaff of the problem allowing them to begin the mediation process before outages affect\nbusiness processes and users or Customers so with nagas you don't have to explain why\n19 infrastructure outage affect your organization's bottom line. So let me tell you how it works.\nSo I'll focus on the diagram that is there in front of your screen. So now I give is\nruns on a server usually as a Daemon or a service it periodically runs plugins residing\non the same server, they contact holes or servers on your network so you can see it\nin the diagram as well. It periodically runs plugins residing on the same server. They\ncontact horse or servers on your network or on the Internet or Source overs, which can\nbe locally present or can be remotely present as well. One can view the status information\nusing the web interface. You can also receive email or SMS notification if something happens,\nso now gives them and behaves like a scheduler that runs out in scripts at certain moments.\nIt stores the results of those scripts and we'll run other scripts if these results change\nnow what our plugins plugins are compiled executables or scripts that can be run from\na command line to check the status of a host or service. So now uses the results from the\nplugins. Mine the current status of the host and services on your network. So what happened\nactually in this diagram now your server is running on a host and plugins interact with\nlocal or remote host right. Now. These plugins will send the information to the scheduler\nwhich displays that in the gy that's what is happening guys. All right, so we have discussed\nall the stages. So let me just give you a quick recap of what all things we have discussed\nfirst. We saw what was the methodology before devops? We saw the waterfall model. What were\nits limitations then we understood the agile model and the difference between the waterfall\nand agile methodology. And what are the limitations of agile methodology then we understood how\ndevops overcomes all of those limitations in what exactly is the worms. We saw the various\nstages and tools involved in devops starting from Version Control. Then we saw continuous\nintegration. Then we saw countenance delivery. Then we saw countenance deployment. Basically,\nwe understood the difference between integration delivery and deployment then we saw what is\nconfiguration management and containerization and finally explained continuous monitoring,\nright? So in between I was even switching back to my virtual machine where a few tools\nalready installed and I was telling you a few Basics about those tools now comes the\nmost awaited topic of today's session which is our use case. So let's see what we are\ngoing to implement in today's use case. So this is what we'll be doing. We have git repository,\nright? So developers will be committing code to this git repository. And from there. Jenkins\nwill pull that code and it will first clone that repository after cloning that repository\nit will build a Docker image using a Docker file. So we have the dockerfile will use that\nto build an image. Once that image is built. We are going to test it and then push it onto\nDocker Hub as I've told you what is the organ of is nothing but like a git repository of\nDocker images. So this is what we'll be doing. Let me just repeat it once more so developers\nwill be committing changes in the source code. So the moment any developers commit to change\nin the source code Jenkins will clone the entire git repository. It will build a Docker\nimage based on a Docker file that will create and from there. It will push the docker image\nonto the docker Hub. This will happen automatically. The click of a button. So what I'll do is\nwe'll be using will be using gate Jenkins and Docker. So let me just quickly open my\nVirtual Machine and I'll show you that so what our application is all about. So we are\nbasically what creating a Docker image of a particular application and then pushing\nit onto Docker Hub in an automated fashion. And our code is written in the GitHub repository.\nSo what is it application? So it's basically a Hello World server written with node. So\nwe have a main dot JS. Let me just go ahead and show you on my GitHub repository. Let\nme just go back. So this is how our application looks like guys we have main dot J's right\napart from that. We have packaged or Json for a dependencies. Then we have Jenkins file\nand dockerfile Jenkins file. I'll explain it to you what we are going to do with it.\nBut before that let me just explain you few basics of Docker file and how we can build\na Docker image of this particular. Very basic node.js application. First thing is writing\na Docker file now to be able to build a Docker image with our application. We will need a\nDocker file. Yeah, right you can think of it as a blueprint for Docker. It tells Docker\nwhat the contents in parameters of our image should be so Docker images are often based\non other images, but before that, let me just go ahead and create a Docker file for you.\nSo let me just first clone this particular Repository. So let me go to that particular\ndirectory first. It's Darren downloads. Let me unzip this first unzip divorce - tutorial\nand let me hit an LS command. So here is my application present. So I'll just go to this\nparticular devops - tutorial - master and let me just say my terminal let us focus on\nwhat all files we have. We have dockerfile. Let's not focus on Jenkins file at all for\nnow, right we have dockerfile. We have main dot J's package dot Json read me dot MD and\nwe have test dot J's. So I have a Docker file with the help of which I will be creating\na Docker image, right? So let me just show you what I have written in this Docker file\nbefore this. Let me tell you that Docker images are often based on other images right for\nthis example. We are basing our image on the official node Docker image. So this line that\nyou are seeing is basically to base our application on the official node Docker image. This makes\nour job easy and our dockerfile very very short guys. So the in a hectic task of installing\nnode, and it's dependencies in the image is already done in our basement. So we'll just\nneed to include our application. Then we have set a label maintainer. I mean, this is optional\nif you want to do it. Go ahead. If you don't want to do it, it's still fine. There's a\nhealth check which is basically for Docker to be able to tell if the server is actually\nup or not. And then finally we are telling Docker which Port ask server will run on right?\nSo this is how we have written the dockerfile. Let me just go ahead and close this and now\nI'm going to create an image using this Docker file. So for that all I have to type here\nis sudo docker Bell slash home slash Edureka downloads devops - tutorial basically the\npath to my dockerfile and here we go need to provide the sudo password. So had I started\nnow and is creating an image for me the docker image and it is done it successfully built\nand this is my image ID, right so I can just go ahead and run this as well. So all I have\nto type here is Docker Run - it and my image ID and here we go. So it is listening at Port\n8000. Let me just stop it for now. So I've told you how you can create an image using\nDocker file right now. What I'm going to do, I'm going to use Jenkins in order to clone\na git repository then build an image and then perform testing and finally pushing it onto\nDocker Hub my own tokra profile. All right, but before that what we need to do is we need\nto tell Jenkins what our stages are and what to do in each one of them for this purpose.\nWe will write Jenkins pipeline specification in on Jenkins file. So let me show you how\nthe Jenkins file looks like just click on it. So this is what I have written in my Jenkins\nfile, right? That's pretty self-explanatory first. I've defined my application. I mean\njust clone the repository that I have then build that image. This is the target I'm using\na draca one, which is username. And Erica is the repository name rights built that image\nthen test it. So we are just going to print test passed and then finally push it onto\nDocker Hub, right? So this is the URL of Docker Hub and my credentials are actually saved\nin Jenkins in Docker Hub credentials. So, let me just show you how you can save those\ncredentials. So go to the credentials tab, so here you need to click on system and click\non global credentials. Now over here, you can go ahead and click on update and you need\nto provide your username your password and your doctor have credential ID that whatever\nyou gonna pass there, right? So, let me just type the password again. All right. Now we\nneed to tell Jenkins two things where to find our code and what credentials to use to publish\nthe docker image, right? So I've already configured my project. Let me just go ahead and show\nyou what I have written there. So the first thing is the name of my project right which\nI was showing you when you create a new item over there. There's an option called where\nyou need to give the name of your project and I've chosen pipeline project. So if I\nhave to show you the pipeline project you can go to new item. And this is what I've\nchosen that the kind of project and then I have clicked on Bill triggers. So basically\nthis will pull my CM the source code management repository after every minute Whenever there\nis a change in the source code will pull that and it will repeat the entire process after\nevery minute then Advanced project options are selected the pipeline script from SCM\nhere either you can write pipeline script directly or you can click on Pipeline script\nfrom source code management that kind of source code management is get then I've provided\nthe link to my repository and that's all I have done now when I scroll down there's nothing\nelse I can just click on apply and Save So I've already build this project one. So let\nme just go ahead and do it again. All right side. I started first. It will clone the repository\nthat I have. You can find all the logs. Once you click on this blue color ball and you\ncan find the logs here as well. So once you click here, you'll find it over here as well.\nAnd similarly the logs are present here also, so now I we have successfully build our image.\nWe have tested it now. We are pushing it onto Docker hub. So we are successfully pushed\nour image onto Docker Hub as well. Now if I go back to my profile and I go to my repository\nhere. So you can find the image is already present here have actually pushed it multiple\ntimes. So this is how you will execute the Practical. It was very easy guys. So let me\njust give you a quick recap of all the things we have done first. I told you how you can\nwrite a Docker file in order to create a Docker image of a particular application. So we were\nbasing our image on the official node image of present of the docker Hub, right which\nalready contains all the dependencies and it makes a Docker file looks very small after\nthat. I build an image using the dockerfile then I explain to you how you can use Jenkins\nin order to automate the task of cloning a repository then building a Docker image testing\nthe docker image and then finally uploading the add-on to the docker Hub. We did that\nautomatically with the help of Jenkins a told you where you need to provide the credentials\nwhat our tags how you can write Jenkins file the next part of the use cases different teams\nbeat staging and production can actually pull the image that we have uploaded onto Docker\nHub and can run as many containers as you want. Hey everyone, this is Reyshma from Edureka\nand today's tutorial. We're going to learn about git and GitHub. So without any further\nAdo, let us begin this tutorial by looking at the topics that we'll be learning today.\nSo at first we will see what is Version Control and why do we actually need Version Control\nafter that? We'll take a look at the different version control tools and then we'll see all\nabout GitHub and get lots of taking account a case study of the Dominion Enterprises about\nhow they're using GitHub after that. We'll take a look at the features of git and finally\nwe're going to use all the git commands to perform all the get operations. So this is\nexactly what we'll be learning today. So we're good to go. So let us begin with the first\ntopic. What is Version Control? Well, you can think of Version Control as the management.\nSystem that manages the changes that you make in your project till the end the changes that\nyou make might be some kind of adding some new files or you're modifying the older files\nby changing the source code or something. So what the version control system does is\nthat every time you make a change in your project? It creates a snapshot of your entire\nproject and saves it and these snapshots are actually known as different versions. Now\nif you're having trouble with the word snapshot just consider that snapshot is actually the\nentire state of your project at a particular time. It means that it will contain what kind\nof files your project is storing at that time and what kind of changes you have made. So\nthis is what a particular version contains now, if you see the example here, let's say\nthat I have been developing my own website. So let's say that in the beginning. I just\nhad only one web page which is called the index dot HTML and Few days. I have added\nanother webpage to it, which is called about dot HTML and I have made some modifications\nin the about our HTML by adding some kind of pictures and some kind of text. So, let's\nsee what actually the Version Control System stores. So you'll see that it has detected\nthat something has been modified and something has been created. For example, it is storing\nthat about dot HTML is created and some kind of photo is created or added into it and let's\nsay that after a few days. I have changed the entire page layout of the about dot HTML\npage. So again, my version control system will detect some kind of change and will say\nthat some about duration T. Ml has been modified and you can consider all of these three snapshots\nas different versions. So when I only have my index dot HTML webpage and I do not have\nanything else. This is my version 1 and after that when I added another web page, this is\ngoing to be a version 2 and after have The page layout of my web page. This is my version\n3. So this is how a Version Control System stores different versions. So I hope that\nyou've all understood what is a version control system and what are versions so let us move\non to the next topic and now we'll see why do we actually need Version Control? Because\nyou might be thinking that why should I need a Version Control? I know what the changes\nthat I have made and maybe I'm making this changes just because I'm correcting my project\nor something, but there are a number of things because of why we need Version Control n so\nlet us take a look at them one by one. So the first thing that version control system\navails us is collaboration. Now imagine that there are three developers working on a particular\nproject and everyone is working in isolation or even if they're working in the same shared\nfolder. So there might be conflicts sometimes when each one of them are trying to modify\nthe same file. Now, let's say they are working in isolation. Everyone is minding their own\nbusiness. Now the developer one has made some changes XYZ in a particular application and\nin the same application the developer to has made some kind of other changes ABC and they\nare continuing doing that same thing. They're making the same modifications to the same\nfile, but they're doing it differently. So at the end when you try to collaborate or\nwhen you try to merge all of their work together, you'll come up with a lot of conflicts and\nyou might not know who have done what kind of changes and this will at the end end up\nin chaos. But with Version Control System, it provides you with a shared workspace and\nit continuously tells you who has made what kind of change are what has been changed.\nSo you'll always get notified if someone has made changed in your project. So with Version\nControl System a collaboration is available tween all the developers and you can visualize\neveryone's work properly and as a result your project will always evolve as a whole from\nthe start and it will save a lot of time for you because there won't be much conflicts\nbecause obviously if the developer a will see that he has already made some changes\nhe won't go for that right because he can carry out his other work. You can make some\nother changes without interfering his work. Okay, so we'll move on to the next reason\nfor what I we need Version Control System. And this is one of the most important things\nbecause of why we need Version Control System. I'll tell you why now. The next reason is\nbecause of storing versions because saving a version of your project after you have made\nchanges is very essential and without a Version Control System. It can actually get confusing\nbecause there might be some kind of questions that will arise in your mind when you are\ntrying to save a version the first question might be how much would you save would you\njust save the entire project or would you just save the changes that you made now? If\nyou only save the changes it'll be very hard for you to view the whole project at a time.\nAnd if you try to save the entire project at every time there will be a huge amount\nof unnecessary and redundant data lying around because you'll be saving the same thing that\nhas been remaining unchanged again. And again, I will cover up a lot of your space and after\nthat they're not the problem comes that. How do I actually named this versions now? Even\nif you are a very organized person and you might actually come up with a very comprehensive\nnaming scheme, but as soon as your project starts varying and it comes to variance there\nis a pretty good chance that you'll actually lose track of naming them. And finally the\nmost important question. Is that how do you know what exactly is different between these\nversions now you ask me that? Okay. What's the difference between version 1 and version\n2 what exactly was changed you need to remember or document them as well. Now when you have\na version control system, you don't have to worry about any of that. You don't have to\nworry about how much you need to save. How do you name them? Are you have to you don't\nhave to remember that what exactly is different different between the versions because the\nVersion Control System always acknowledges that there is only one project. So when you're\nworking on your project, there is only one version on your disk. And everything else\nall the changes that they've made in the past are all neatly packed inside the Version Control\nSystem. Let us go ahead and see the next reason now version control system provides me with\na backup. Now the diagram that you see here is actually the layout of a particul distributed\nVersion Control System here. You've got your central server where all the project files\nare located and apart from that every one of the developers has a local copy of all\nthe files that is present in the central server inside their local machine and this is known\nas the local copies. So what the developers do is that every time they start coding at\nthe start of the day, they actually fetch all the project files from the central server\nand store it in the local machine and after they are done working the actually transfer\nall the files back into the central server. So at every time you'll always Is have a local\ncopy in your local machine at times of Crisis. Like maybe let's say that your central server\ngets crashed and you have lost all your project files. You don't have to worry about that\nbecause all the developers are maintaining a local copy the same exact copy of all the\nfiles that is related to your project that is present in the central server. Is there\nin your local machine and even if let's say that maybe this developer has not updated\nhis local copy with all the files if he loses and the central servers gets crashed and the\ndeveloper has not maintained its local copy is always going to be someone who has already\nupdated it because obviously there is going to be huge number of collaborators working\non the project. So even a particular developer can communicate with other developers and\nget fetch all the project files from other developers local copy as well. So it is very\nreliable when you have a version control system because you're always going to have a backup\nof all. You're fired. So the next thing and which Version Control helps us is to analyze\nmy project because when you have finished your project you want to know that how your\nproject has actually evolved so that you can make an analysis of it and you can know that\nwhat could you have done better or what could have been improved in your project? So you\nneed some kind of data to make an analysis and you want to know that what is exactly\nchanged and when was it change and how much time did it take and Version Control System\nactually provides you with all the information because every time you change something version\ncontrol system provides you with the proper description of what was changed. And when\nwas it changed you can also see the entire timeline and you can make your analysis report\nin a very easy way because you have got all the data present here. So this is how a version\ncontrol system helps you to analyze your project as well. So let us move ahead and let us take\na look. Add the Version Control tools because in order to incorporate version control system\nin your project, you have to use a Version Control tool. So let us take a look at what\nis available. What kind of tools can I use to incorporate version control system. So\nhere we've got the four most popular version control system tools and they are get and\nthis is what we'll be learning in today's tutorial will be learning how to use git and\napart from get you have got other options as well. You've got the Apache subversion\nand this is also popularly known as SBN SVN and CVS, which is the concurrent version systems.\nThey both are a centralized Version Control tool. It means that they do not provide all\nthe developers with a local copy. It means that all the contributors are all the collaborators\nare actually working directly with the central repository only they don't maintain local\ncopy and Kind of actually becoming obsolete because everyone prefers a distributed Version\nControl System where everyone has an okay copy and Mercurial on the other hand is very\nsimilar to get it is also a distributed Version Control tool but we'll be learning all about\nget here. That's what I get is highlighted in yellow. So let's move ahead. So this is\nthe interest over time graph and this graph has been collected from Google Trends and\nthis actually shows you that how many people have been using what at what time so the blue\nline here actually represents get the green is SVN. The yellow is Mercurial and the red\nis CVS. So you can see that from the start get has always been the most popular version\ncontrol tool as compared to as bian Mercurial and CVS and it has always kind of been a bad\nday for CVS, but get has always been popular. So why not use get right? So there's nothing\nto say much about That a yes and a lot of my fellow attendees agree with me. We should\nall use get and we're going to learn how to use get in this tutorial. So let us move ahead\nand let us all learn about git and GitHub right now. So the diagram that you see on\nmy left is actually the diagram which represents that what exactly is GitHub and what exactly\nis get now I've been talking about a distributed version control system and the right hand\nside diagram actually shows you the typical layout of a distributed Version Control System\nhere. We've got a central server or a central repository now, I'll be using the word repository\na lot from now on just so that you don't get confused. I'll just give you a brief overview.\nI'll also tell you in detail. What is the repository and I'll explain you everything\nlater in this tutorial, but for now just consider repository as a data space where you store\nall the project files any kind of files that is related. Your project in there, so don't\nget confused when I say rip off the tree instead of server or anything else. So in a Distributive\nVersion Control System, you've got a central repository and you've got local repositories\nas well and every of the developers at first make the changes in their local repository\nand after that they push those changes or transfer those changes from into the central\nrepository and also the update their local repositories with all the new files that are\npushed into the central repository by an operation called pull. So this is how the fetch data\nfrom Central repository. And now if you see the diagram again on the left, you'll know\nthat GitHub is going to be my central repository and get is the tool that is going to allow\nme to create my local repositories. Now, let me exactly tell you what is GitHub. Now people\nactually get confused between git and GitHub they I think that it's kind of the same thing\nmaybe because of the name they sound very alike. But it is actually very different.\nWell git is a Version Control tool that will allow you to perform all these kind of operations\nto fetch data from the central server and to just push all your local files into the\ncentral server. So this is what get will allow you to do it is just a Version Control Management\ntool. Whereas in GitHub. It is a code hosting platform for Version Control collaboration.\nSo GitHub is just a company that allows you to host your central repository in a remote\nserver. If you want me to explain in easy words, you can consider GitHub as a social\nnetwork, which is very much similar to Facebook. Like only the differences that this is a social\nnetwork for the developers. We're in Facebook, you're sharing all your photos and videos\nor any kind of statuses. What the developers doing get have is that they share their code\nfor everyone to see their projects either code about how they have worked on. So that\nis GitHub. There are certain advantages of a distributed Version Control System. Well,\nthe first thing that I've already discussed was that it provides you with the backup.\nSo if at any time your central server crashes, everyone will have a backup of all their files\nand the next reason is that it provides you with speed because Central servers typically\nlocated on a remote server and you have to always travel over a network to get access\nto all the files. So if at sometimes you don't have internet and you want to work on your\nproject, so that will be kind of impossible because you don't have access to all your\nfiles, but with a distributed Version Control System, you don't need internet access always\nyou just need internet when you want to push or pull from the central server apart from\nthat you can work on your own your files are all inside your local machine so fetching\nit. In your workspace is not a problem. So that are all the advantages that you get with\na distributed version control system and a centralized version control system cannot\nactually provide you that so now let us take a look at a GitHub case study of the Dominion\nEnterprises. So Dominion Enterprises is a leading marketing services and Publishing\ncompany that works across several Industries and they have got more than 100 offices worldwide.\nSo they have distributed a technical team support to develop a range of a website and\nthey include the most popular websites like for and.com volts.com homes.com. All the Dominion\nEnterprises websites actually get more than tens of million unique visitors every month\nand each of the website that they work on has a separate development team and all of\nthem has got a unique needs and You were close of their own and all of them were working\nindependently and each team has their own goals their own projects and budgets, but\nthey actually wanted to share the resources and they wanted everyone to see what each\nof the teams are actually working on. So basically they want to transparency. Well the needed\na platform that was flexible enough to support a variety of workflows. And that would provide\nall the Dominion Enterprises development around the world with a secure place to share code\nand work together and for that they adopted GitHub as the platform. And the reason for\nchoosing GitHub is that all the developers across the Dominion Enterprises, we're already\nusing github.com. So when the time came to adopt a new version control platform, so obviously\nGitHub Enterprise definitely seemed like a very intuitive choice and because everyone\nall the developers were also familiar with GitHub. So the learning curve Was also very\nsmall and so they could start contributing code right away into GitHub and with GitHub\nall the developer teams. All the development teams were provided access to when they can\nalways share their code on what they're working on. So at the end everyone has got a very\nsecure place to share code and work together. And as Joe Fuller, the CIO of dominion Enterprises\nsays that GitHub Enterprise has allowed us to store our company source code in a central\ncorporately control system and Dominion Enterprises actually manages more than 45 websites, and\nit was very important for dominion and the price to choose a platform that made working\ntogether possible. And this wasn't just a matter of sharing Dominion Enterprises open\nsource project on GitHub. They also had to combat the implications of storing private\ncode publicly to make their work more transparent across the company as well and they were also\nusing Jenkins to facilitate continuous integration environment and in order to continuously deliver\ntheir software. They have adopted GitHub as a Version Control platform. So GitHub actually\nfacilitated a lot of things for Dominion Enterprises and for that there were able to incorporate\na continuous integration environment with Jenkins and they were actually sharing their\ncode and making software delivery even more faster. So this is how GitHub helped not only\njust a minute Enterprises, but I'm sure there's might be common to a lot of other companies\nas well. So let us move forward. So now this is the topic that we were waiting for and\nnow we'll learn what is get so git is a distributed Version Control tool and it supports distributed\nnon linear workflow. So get is the tool that actually facilitates all the distributed Version\nControl System benefits because it will provide you to create a local Repository. In your\nlocal machine and it will help you to access your remote repository to fetch files from\nthere or push files and do that. So get is the tool that you required to perform all\nthese operations and I'll be telling you all about how to perform these operations using\nget later in this tutorial for now. Just think of get as a to that you actually need to do\nall kind of Version Control System task. So we'll move on and we'll see the different\nfeatures of git now. So these are the different features of get is distributed get is compatible\nget a provides you with the non linear workflow at avails you branching. It's very lightweight\nit provides you with speed. It's open source. It's reliable secure and economical. So let\nus take a look at all these features one by one. So the first feature that we're going\nto look into is its distributed now, I've been like telling you it's a it's a distributor.\nVersion Control tool that means that the feature that get provides you is that it gives you\nthe power of having a local repository and lets you have a local copy of the entire development\nhistory, which is located in the central repository and it will fetch all the files from the central\nrepository to get your local repository always updated and this time calling it distributed\nbecause every was let's say that there might be a number of collaborators or developers\nso they might be living in different parts of the world. Someone might be working from\nthe United States and one might be in India. So the word the project is actually distributed.\nEveryone has a local copy. So it is distributed worldwide you can say so this is what distributed\nactually means. So the next feature is that it is compatible. Now, let's say that you\nmight not be using get on the first place. But you have a different version control system\nalready installed like SVN, like Apache subversion or CVS and you want to switch to get because\nobviously you're not happy with the centralized version control system and you want a more\ndistributed version control system. So you want to migrate from SVN to get but you are\nworried that you might have to transfer all the files all the huge amount of files that\nyou have in your SVN repository into a git repository. Well, if you are afraid of doing\nthat, let me tell you you don't have to be anymore because get is compatible with as\nVM repositories as well. So you just have to download and install get in your system\nand and you can directly access the SVN repository over a network which is the central repository.\nSo the local repository that you'll have is going to be a good trip. The tree and if you\ndon't want to change your central repository, then you can do that as well. We can use get\nSVN and you can directly access all the files all the files in your project that is residing\nin an SVN repository. So do you don't have to change that and it is compatible with existing\nsystems and protocols but there are protocols like SSH and winner in protocol. So obviously\nget users SSH to connect to the central repository as well. So it is very compatible with all\nthe existing things so you don't have to so when you are migrating into get when you are\nstarting to use get you don't have to actually change a lot of things so is as I have everyone\nunderstood these two features by so far Okay, the next feature of get is that it supports\nnonlinear development of software. Now when you're working with get get actually records\nthe current state of your project by creating a tree graph from the index a tree that you\nknow is nonlinear now when you're working with get get actual records the current state\nof the project by creating a tree graph from the index. And as you know that a tree is\na non linear data structure and it is usually actually in the form of a directed acyclic\ngraph which is popularly known as the DH e. So, this is how I actually get facilitates\na nonlinear development of software and it also includes techniques where you can navigate\nand visualize all of your work that you are currently doing and how does it actually facilitate\nand when I'm talking about non-linearity, how does get actually facilitates a nonlinear\ndevelopment is actually by Crunching now branching actually allows you to make a nonlinear software\ndevelopment. And this is the gift feature that actually makes get stand apart from nearly\nevery other Version Control Management do because get is the only one which has a branching\nmodel. So get allows and get actually encourages you to have a multiple local branches and\nall of the branches are actually independent of each other the and the creation and merging\nand deletion of all these branches actually takes only a few seconds and there is a thing\ncalled the master Branch. It means the main branch which starts from the start of your\nproject to the end of your project and it will always contain the production quality\ncode. It will always contain the entire project and after that it is very lightweight now\nyou might be thinking that since we're using local repositories on our local machine and\nwe're fetching all the files that are in the central repository. And if you think that\nway you can know that there are like hon, maybe there are It's of people's pushing their\ncode into the central repository and and updating my local repository with all those files.\nSo the data might be very huge but actually get uses lossless compression technique and\nit compresses the data on the client side. So even though it might look like that you've\ngot a lot of files when it actually comes to storage or storing the data in your local\nrepository. It is all compressed and it doesn't take up a lot of space only when you're fetching\nyour data from the local repository into your workspace. It converts it and then you can\nwork on it. And whenever you push it again, you can press it again and store it in a very\nminimal space in your disk and after that it provides you with a lot of speed now, since\nyou have a local repository and you don't have to always travel over a network to fetch\nfiles, so it does not take any time to get files in your into your workspace from your\nlocal repository because and if you see that it is actually Three times faster than fetching\ndata from a remote repository because he's obviously have to travel over a network to\nget that data or the files that you want and Mozilla has actually performed some kind of\nperformance tests and it is found out that get is actually one order of magnitude faster\nthan other version control tools, which is actually equal to 10 times faster than other\nversion control tools. And the reason for that is because get is actually written in\nC and C is not like other high-level languages. It is very close to machine language. So it\nproduces all the runtime overheads and it makes all the processing very fast. So get\nis very small and it get is very fast. And the next feature is that it is open source.\nWell, you know that get was actually created by Linus Torvalds and he's the famous man\nwho created the Linux kernel and he actually used get in the development of the Next Colonel\nnow, they were using a Version Control System called bitkeeper first, but it was not open\nsource day. So the owner of bitkeeper has actually made it a paid version and this actually\ngot Linus Torvalds mad. So what he did is that he created his own version control system\ntool and he came up with get and he made it open source for everyone so that you can so\nthe source code is available and you can modify it on your own and you can get it for free.\nSo there is one more good thing about get and after that it is very reliable. Like I've\nbeen telling you since the star that egg have a backup of all the files in your local repository.\nSo if your central server crashes, you don't have to worry your files are all saving your\nlocal repository and even if it's not in your local repository, it might be in some other\ndevelopers local repository and you can tell him when and whenever you need some that data\nand you lose the data and after your central server is all If it was crashed, he can directly\npush all the data into the central repository and from there everyone and Skinner always\nhave a backup. So the next thing is that get is actually very secure now git uses the sha-1\ndo name and identify objects. So whenever you actually make change it actually creates\na commit object and after you have made changes and you have committed to those changes, it\nis actually very hard to go back and change it without other people knowing it because\nwhenever you make a commit the sha-1 actually converts it what is sha-1. Well it is a kind\nof cryptographic algorithm. It is a message digest algorithm that actually converts your\ncommit object into a four digit hexadecimal code Now message AI uses techniques and algorithms\nlike md4 md5 and it is actually very secure. It is considered to be very secure because\neven National Security Agency of the United States of America uses ssj. I so if they're\nusing it so you might know that it is very secure as well. And if you want to know what's\nmd5 and message digest I'm not going to take you through the whole algorithm whole cryptographic\nalgorithm about how they make that Cipher and all you can Google it and you can learn\nwhat is sji, but the main concept of it is that after you have made changes. You cannot\ndeny that you have not met changes because it will store it and everyone can see it it\nwill create commit hash for you. So everyone will see it and this commit hash can is also\nuseful when you want to revert back to previous versions you want to know that which commits\nexactly caused what problem and if you want to remove that commit or if you want to remove\nthat version you can do that because sha I will give you the hash log of every government\nso we move on and see the Feature, which is economical now get is actually released under\nthe general public license and it means that it is for free. You don't have to pay any\nmoney to download get in your system. You can have kids without burning a hole in your\npocket. And since all the heavy lifting is done on the kind side because everything you\ndo you do it on your own entire workspace and you push it into the local repository\nfirst, and after that it's pushing the central server. So it means that people are only pushing\ninto the central server after when they're sure about their work and and they're not\nexperimenting on the central repository. So your central repository can be very simple\nenough. You don't have to worry about having a very complex and very powerful hardware\nand a lot of money can be saved on that as well. So get us free get a small so good provides\nyou with all the cool features that you would actually want. So this All the get features.\nSo we'll go ahead to the next topic our next the first we'll see what is a repository now\nas GitHub says that it is a directory or storage space where all your projects get live. It\ncan be local to a folder on your computer like your local repository or it can be a\nstorage space and GitHub or another online host. It means your central repository and\nyou can keep your gold files text files image files. You name it? You can keep it inside\na repository everything that is related to your project and like I have been chanting\nsince the start of this tutorial that we have got two kinds of repositories. We've got the\ncentral repository and we've got the local repository and now let us take a look at what\nthis repositories actually are. It's on my left hand side. You can see all about the\ncentral repository and in the right hand side. This is all about my local repository and\nthe diagram in the middle actually shows you the entire layout so the local repository\nwill be inside my local machine and my central repository for now is going to be on GitHub.\nSo my central repository is typically located on a remote server and like I just told you\nit is typically located on GitHub and my local repository is going to be my local machine\nat we reside in as in a DOT git folder and it will be inside your Project's root. The\ndot git folder is going to be inside your Project's root and it will contain all the\ntemplates and all the objects and every other configuration files when you create your local\nrepository and since you're pushing all the code, your central repository will also have\nthe same dot git repository folder inside it and the sole purpose of having a central\nrepository is so that you're all the Actors are all the developers can actually share\nand exchange the data because someone might be working on a different problem and someone\nmight be needing help in that so what you can do is that he can push all the code all\nthe problems that he has sauce or something that he has worked on it to the central repository\nand everyone else can see it and everyone else can pull his code and use it for themselves\nas well. So this is just meant for sharing data. Whereas in local repository. It is only\nyou can access it and it is only meant for your own so you can work in your local repository.\nYou can work in isolation and no one will interfere even after you have done after years\nsure that your code is working and you want to show it to everyone just transfer it or\npush it into the central Repository. Okay, so now we'll be seeing the get operations\nand come on. So this is how we'll be using it. There are various operations and commands\nthat will help us to do all the things that we were just talking about right now. We're\ntalking about pushing changes. So these are all get operations. So we'll be performing\nall these operations will be creating repositories with this command will be making changes in\nthe files that are in a repositories with the commands will be also doing parallel nonlinear\ndevelopment that I was just talking about and we also be sinking a repositories so that\nour Central repository and local repository are connected. So I'll show you how to do\nthat one by one. So the first thing that we need to do is create repositories, so we need\na central repository and we need a local repository now will host our Central repository on GitHub.\nSo for that you need an account in GitHub. And create a repository there and for your\nlocal repository you have to install get in your system. And if you are working on a completely\nnew project and if you want to start something fresh and very new you can just use git init\nto create your repository or if you want to join an ongoing project, and if you're new\nto the project and you just join so what you can do is that you can clone the central repository\nusing this command get blown. So let us do that. So let's first create a GitHub account\nand create repositories on GitHub. I said first you need to go to github.com. And if\nyou don't have an account, you can sign up for GitHub and here you just have to pick\na username that has not been already taken you have just provide your email address get\na password and then just click this green button here and your account will be created.\nIt's very easy don't have to do much and after that you just have to verify your email and\neverything and after you're done with all sort of thing. You can just go a sign in our\nalready have an account. So I'm just going to sign in here. Softer you're signed in you'll\nfind this page here. So you'll get two buttons where you can read the guide of how to use\nGitHub or you can just start a project right away. Now, I'll be telling you all about GitHub\nso you don't have to click this button right now. So you can just go ahead and start a\nproject. So now get tells that for every project you need to have you need to maintain a unique\nrepository it is because it's very healthy and keeps things very clean because if you\nare storing just the files related to one project in a repository, you won't get confused\nlater. So when you're creating a new repository, you have to provide with a repository name\nnow, I'm just going to name it get - GitHub. And you can provide it with the description\nof this repository. And this is optional. If you don't want to you can leave it blank\nand you can choose whether you want it public or private. Now if you want to it to be private,\nyou have to pay some kind of amount. So like this will cost you $7 a month. And so what\nwhat is the benefit of having a private account? Is that only you can say it if you don't want\nto share your code with anyone and you don't want anyone to see it. You can do that in\nGitHub as well. But for now, I'll just leave it public. I just want it for free and let\neveryone see my work what you have done. So we'll just leave it up lik for now and after\nthat you can initialize this repository with the read me. So the readme file will contain\nthe description of your files. This is the first file that is going to be inside a repository\nwhen you create the repository, so and it's a good habit to actually initialize your repository\nof the readme, so I'll just click this option. This is the option to add git ignore. Now.\nThere might be some kind of files that you don't want when you're making operations,\nlike push or pull you don't want those files to get pushed or pulled like it might be some\nkind of log files or anything so you can add those files and get ignore here. So right\nnow I don't have gone any files that this is just the starting of our project. So I\nwill just ignore this get ignore for now. And then you can actually add some license\nas well. So you can just go through what this license actually are. But if you want to just\nleave it as none. And after that just click on this green button here, so just create\na repository. And so there it is so you can see this is the initial comment you have initialized\nyour repository with the readme and this is your readme file. Now if you want to make\nchanges and do the read me file, just click on it and click on the edit pencil image or\nicon kind of that is in here and you can make changes on the readme files if you want to\nwrite something. Let's say just write it as scription. So this is our tutorial purpose\nand that's it. Just keeping it simple. And after that you've made changes. The next thing\nthat you have to do is you have to commit a changes so you can just go down and click\non this commit changes green button here. And it's done. So you have updated read me\ndot MD and this is your commit hash so you can see that in here. So if you go back to\nyour repository, you can say that something has been updated and will show you when was\nyour last commit little even show you the time? So and for now you're on the branch\nmaster your and this will actually show you all the logs. So since only I'm contributing\nhere. So this is only one contributor and I've just made two commits. The first one\nwas when I initialized it and right now when I modified it and right now I have not created\nany branches. So there is only one branch. So now my central repository has been created.\nSo the next thing that I need to do is create a local repository in my local machine. Now\nI have already installed get in my system. I have using a Windows system. So I have installed\nget for Windows. So if you want some help with the installation, I have already written\na Blog on that. I'll leave the link of the blog in the description below. You can refer\nto that blog and install get in your system. Now, I've already done that. So let's say\nthat I want my project to be in the C drive. So let's say I'm just waiting in folder here\nfrom my project. So just name it. Ed Eureka project and let's say that this is where I\nwant my local repository to been. So the first thing that I'll do is right click and I'll\nclick this option here git bash here. And this will actually open up a very colorful\nterminal for you to use and this is called the git bash emulator. So this is where you'll\nbe typing all your commands and you'll be doing all your work in the get back here.\nSo in order to create your local repository, the first thing that you'll do is type in\nthis command get in it and press enter. So now you can see that it is initialized empty\ngit repository on this path. So, let's see and you can see that a DOT get of a folder\nhas been created here and if you see here and see you can see that it contains all the\nconfigurations and the object details and everything. So your repository is initializing.\nThis is going to be your local repository. So after we have created a repositories, it\nis very important to link them because how would you know which repository to push into\nand how will you just pull all the changes or all the files from a remote repository?\nIf you don't know if they're not connected properly. So in order to connect them with\nthe first thing that we need to do is that we need to add a region and we're going to\ncall our remote repository as origin and we'll be using the command git remote add origin\nto add so that we can pull files from our GitHub or Central repository. And in order\nto fetch files. We can use git pull and if you want to transfer all your files or push\nfiles into GitHub will be using git push. So let me just show you how to do that. So\nwe are back in the local repository. And as you can see now that I have not got any kind\nof files. And if you go to my central repository, you can see that I've got a readme file. So\nthe first thing that I need to do is to add this remote repository as my origin. So for\nthat I'll clear my screen first. So for that you need to use this command. Git remote add\norigin. And the link of yours and the repository and let me just show you where you can find\nthis link. So when you go back into your repository, you'll find this green button here, which\nis the Clone or download just click here. And this is the HTTP URL that you want. So\njust copy it on your clipboard. Go back to your git bash and paste it and enter so your\noriginal has been added successfully because it's not showing any kind of Errors. So now\nwhat will do is that will perform a git pull. It means will fetch all the files from the\ncentral repository into my local Repository. So just type in the command get full. origin\nmaster And you can see that they have done some kind of fetching from the master Branch\ninto the master branch and let us see that whether all the files have been fished or\nnot. Let us go back to our local repository and there is the readme file that was in my\ncentral repository and now it is in my local repository. So this is how you actually update\nyour local repository from the central repository you perform a git pull and it will fetch all\nthe files from this entire repository in your local machine. So let us move forward and\nmove ahead to the next operation. Now, I've told you in order to sync repositories, you\nalso need to use a git push, but since we have not done anything in our local repository\nnow, so I'll perform the good get push later on after a show you all the operations and\nwe'll be doing a lot of things. So at the end I'll be performing the git push and push\nall the changes into my central Repository. And actually that is how you should do that\nthe it's a good habit and it's a good practice if you're working with GitHub and get is that\nwhen you start working. The first thing that you need to do is make a get bull to fetch\nall the files from your central repository so that you could get updated with all the\nchanges that has been recently made by everyone else and after you're done working after you're\nsure that your code is running then only make the get Bush so that everyone can see it you\nshould not make very frequent changes into the central repository because that might\ninterrupt the work of your other collaborators or other contributors as well. So let us move\nahead and see how we can make changes. So now get actually has a concept it has an intermediate\nlayer that resides between your workspace and your local repository. Now when you want\nto commit changes or make changes in your local repository, you have to add those files\nin the index first. So this is the layer that is between your workspace and local repository.\nNow, if your files are not in the index, you cannot make commit organ app cannot make changes\ninto your local repository. So for that you have to use the command git add and you might\nget confused that which all files are in the index and which all are not. So if you want\nto see that you can use the command git status and after you have added the changes in the\nindex you can use the command git commit to make the changes in the local repository.\nNow, let me tell you what is exactly a git commit everyone will be talking about get\ncoming. Committing changes when you're making changes. So let us just know what is a git\ncommit. So let's say that you have not made any kind of changes or this is your initial\nproject. So what a comet is is that it is kind of object which is actually a version\nof your project. So let's say that you have made some changes and you have committed those\nchanges what your version control system will do is that it will create another commit object\nand this is going to be your different version with the changes. So your commit snapshots\nactually going to contain snapshots of the project which is actually changed. So this\nis what come it is. So I'll just show you I'll just go ahead and show you how to commit\nchanges in your local repository. So we're back into our local repository. And so let's\njust create some files here. So now if you're developing a project you might be just only\ncontributing your source code files into the central repository. So now I'm not just going\nto tell you all about coding. So we're just going to create some text files write something\nin that which is actually pretty much the same if you're working on a gold and you're\nstoring your source code in your repositories. So I just go ahead and create a simple text\nfile. Just name it Eddie one. Just write something so I'll just try first file. Save this file\nclose it. So now remember that even if I have created inside this repository, this is actually\nshowing my work space and it is not in my local repository now because I have not committed\nit. So what I'm going to do is that I'm going to see what all files are in my index. But\nbefore that I'll clear my screen because I don't like junk on my screen. Okay. So the\nfirst thing that we're going to see is that what all files are added in my index and for\nthat I just told you we're going to use the command git status. So you can see that it\nis calling anyone dot txt which we just have written. It is calling it an untracked file\nnow untracked files are those which are not added in the index yet. So this is newly created.\nI have not added it explicitly into the index. So if I want to commit changes in Eddie one\ndot txt, I will have to add it in the index. So for that I'll just use the command git\nadd and the name of your file which is a D1 Dot txt. And it has been added. So now let\nus check the status again. So for that will choose get status. And you can see that changes\nready to be committed is the Eddie Wonder txt? Because it's in the index and now you\ncan commit changes on your local repository. So in order to commit the command that you\nshould be using is git commit. - em because whenever you are committing you always have\nto give a commit message so that everyone can see who made all this comments and what\nexactly is just so this commit message is just for your purpose that you can see what\nexactly was changed. But even if you don't dry it it the version control system is also\ngoing to do that. And if you have configured your get it is always going to show that who's\nthe user who has committed this change. So I was just talking about writing a commit\nmessage. So I'm just going to write something like adding first commit and press enter so\nyou can see one file change something has been inserted. So this is the changes are\nfinally committed in my local repository. And if you want to see how actually get stores\nall this commit with actually I'll show you after I show you how to commit multiple files\ntogether. So let's just go back into our local Rebel folder and we'll just create some more\nfiles more text files. I'm just going to name it. I do do with create another one. Just\nname it I do three. Let's just write something over here. We just say second file. Sorry.\nso let's go back to our get bash terminal and Now let us see the get status. So now\nyou can see that it is showing that I do too and I do three are not in my index and if\nyou remember anyone was already in the index, actually, let me just go back and make some\nmodifications in Eddie one as well. So I'm going to ride. modified one So, let's see\nget status again. And you can see that it is showing that anyone is modified and there\nare untracked files and you do and edit three. Because I haven't added them in my index yet.\nSo now Sebastian and Jamie you have been asking me how to like a doll multiple files together.\nSo now I'm going to add all these files at once so for that I'm just going to use get\nat - capital a Just press enter and now see the get status. And you see that all the files\nhave been added to the index and ones. And it's similarly with commit as well. So now\nthat you have added all the files in the index. I can also commit them all at once and how\nto do that. Let me just show you you just have to write git commit and - small a so\nif you want to commit all you have to use - small are in case of git commit whereas\nin case of get add if you want to add all the files you have to use - capital A. So\njust remember that difference and add message. hiding so you can see three files has been\nchanged and now let me show you how this actually gets stores all this comets. So you can perform\nan operation called the git log. And you can see so This Is 40 digit hexadecimal code that\nI was taking a talking about and this is the sha-1 hash and you can see the date and you\nhave got the commit message that we have just provided where I just wrote adding three files\ntogether. It shows it it shows the date and the exact time and the author and this is\nme because I've already configured it with my name. So this is how you can see come in\nand this is actually how Version Control System like get actually stores all your commit.\nSo let us go back and see the next operation which is how to do parallel development or\nnon-linear development. And the first operation is branching now, we've been talking about\nbranching a lot and let me just tell you what exactly is branching and what exactly you\ncan do with branching. Well, you can think of branches like a pointer to a To become\nit. Let's say that you've made changes in your main branch. Now remember that your main\nbranch that I told you about. It's called The Master branch and the master Branch will\ncontain all the code. So let's say that you're working on the master branch and you've just\nmade a change and you've decided to add some new feature on to it. So you want to work\non the new feature individually or you don't want to interfere with the master Branch.\nSo if you want to separate that you can actually create a branch from this commit and let me\nshow you how to actually create branches. Now Alice tell you that there are two kinds\nof branches their local branches and remote tracking branches. Your remote branches are\nthe branches that is going to connect your branches from your local repository to your\ncentral repository and local branches are something that you only create in your workspace.\nThat is only going to work with your with the files in your local repository. So I'll\nshow you how to create branches and then everything will Clear to you. So let us go back to our\ngit Bash. Clear the screen. And right now we are in the master branch and this indicates\nwhich brands you were onto right now. So we're in the master Branch right now and we're going\nto create a different branch. So for that you just have to type the command git branch\nand write a branch name. So let us just call it first branch. and enter so now you have\ncreated a branch and and this first Branch will now contain all the files that were in\nthe master because it originated from the master Branch. So now the shows that you are\nstill in the master branch and if you want to switch to the new branch that you just\ncreated you have to use this command git checkout, but it's called checking out it going to move\nfrom one branch to another it's called checking out and get so we're going to use git checkout\nand the name of the branch. Switch to first brush and now you can see that we are in the\nfirst branch and now we can start doing all the work in our first Branch. So let us create\nsome more files in the first Branch. So let's go back and this will actually show me my\nworkspace off my first Branch right now. So we'll just create another text document and\nwe're going to name it edu for and you can just write something first. garage to save\nit just will go back and now we've made some changes. So let us just commit this changes\nall at once. So let me just use git add. After that, what do you have to do if you remember\nis that you have to perform a git commit? And I guess one pile changed. So now remember\nthat I have only made this edu for change in my first branch and this is not in my master\nBranch it because now we are in the first Branch if it lists out all the files in the\nfirst Branch, you can see that you've got the Eddie one. I did 283 and the readme which\nwas in the master Branch because it will be there because it originated from the master\nbranch and apart from that. It has a new file called edu for DOT txt. And now if you just\nmove back into the master Branch, let's say We're going back into the Master Garage. And\nif you just see the five Master Branch, you'll find that there is no edu for because I've\nonly made the changes in my first Branch. So what we have done now is that we have created\nbranches and we have also understood the purpose of creating branches because you're moving\non to the next topic. The next thing we'll see is merging so now if you're creating branches\nand you are developing a new feature and you want to add that new feature, so you have\nto do an operation called emerging emerging means combining the work of different branches\nall together and it's very important that after you have branched off from a master\nBranch always combine it back in at the end after you're done working with the branch\nalways remember to merge it back in so now we have created branches. Let us see and we\nhave made changes in our Branch like we have added edu for and if you want to combine that\nback in our Master Branch because like I told you your master Branch will always contain\nyour production quality. Code so let us know actually merge start merging those files because\nI've already created branches. It's time that we merge them. So we are back in my terminal.\nAnd what do we need to do is merge those changes and if you remember that we've got a different\nfile in my first branch, which is the ending for and it's not there in the master Branch\nyet. So what I want to do is merge that Branch into my master Branch so for that I'll use\na command called git merge and the name of my branch and there is a very important thing\nto remember when you're merging is that you want to merge the work of your first Branch\ninto master. So you want Master to be the destination. So whenever you're merging you\nhave to remember that you were always checked out in the destination Branch some already\nchecked out in the master Branch, so I don't have to change it back. So I'll just use the\ncommand git merge and the name of the branch which word you want to merge it into and you\nhave to provide the name of the branch whose work you want merged into the current branch\nthat you were checked out. So for now, I've just got one branch, which is called the first\nbranch. and and so you can see that one file chain. Something has been added. We are in\nthe master bounce right now. So now let us list out all the files in the master branch\nand there you see now you have edu for DOT txt, which was not there before. I'm merged\nit. So this is what merging does now you have to remember that your first branch is still\nseparate. Now, if you want to go back into your first branch and modify some changes\nagain in the first branch and keep it there you can do that. It will not actually affect\nthe master Branch until you merge it. So let me just show you an example. So just go back\nto my first branch. So now let us make changes and add you for. I'll just ride modified in\nfirst branch. We'll go back and we'll just commit all these changes and I'll just use\ngit. So now remember that the git commit all is also performed for another purpose now.\nIt doesn't only actually commit all the uncommitted file at once if your files are in the index\nand you have just modified it also does the job of adding it to the index Again by modifying\nit and then committing it but it won't work. If you have never added that file in the index\nnow Eddie for was already in the index now after modifying it I have not explicitly added\nin the index. And if I'm using git commit all it will explicitly add it in the index\nbit will because it was already a track file and then it will commit the changes also in\nmy local Repository. So you see I didn't use the command git add. I just did it with Git\ncommit because it was already attract file. So one file has been changed. So now if you\njust just cat it and you can see that it's different. It shows the modification that\nwe have done, which is modified it first Branch now, let's just go back to my master branch.\nNow remember that I have not emerged it yet and my master Branch also contains a copy\nof edu for and let's see what this copy actually contains. See you see that the modification\nhas not affected in the master Branch because I have only done the modification in the first\nBranch. So the copy that is in the master branch has not it's not the modified copy\nbecause I have not emerged it yet. So it's very important to remember that if you actually\nwant all the changes that you have made in the first Branch all the things that you have\ndeveloped in the Anu branch that you have created make sure that you merge it in don't\nforget to merge or else it will not show any kind of modifications. So I hope that if understood\nwhy emerging is important how to actually merge different branches together. So we'll\njust move on to the next topic and which is rebasing now when you say rebasing rebasing\nis also another kind of merging. So the first thing that you need to understand about vbase\nis that it actually solves the same problem as of git merge and both of these commands\nare designed to integrate changes from one branch into another. It's just that they just\ndo the same task in a different way. Now what rebasing means if you see the workflow diagram\nhere is that you've got your master branch and you've got a new Branch now when you're\nrebasing it what it does if you see in this workflow diagram here is that if God a new\nbranch and your master branch and when your rebasing it instead of creating a Comet which\nwill have two parent commits. What rebasing does is that it actually places the entire\ncommit history of your branch onto the tip of the master. Now you would ask me. Why should\nwe do that? Like what is the use of that? Well, the major benefit of using a re basis\nthat you get a much cleaner project history. So I hope you've understood the concept of\nrebase. So let me just show you how to actually do rebasing. Okay. So what we're going to\ndo is that we're going to do some more work in our branch and after that will be base\nour branch on to muster. So we'll just go back to our branch. You skip check out. first\nbranch and now we're going to create some more files here. same it at your five and\nlet's say I do six. So we're going to write some random stuff. I'd say we're saying welcome\nto Ed, Eureka. one all right the same thing again that Sarah come two so we have created\nthis and now we're going back to our get bash and we're going to add all these new files\nbecause now we need to add because it we cannot do it with just get commit all because these\nare untracked files. This is the files that I've just created right now. So I'm using\nAnd now we're going to commit. And it has been committed. So now if you just see all\nthe files, you can see any one two, three, four five six and read me and if you go back\nto the master. And if you just list out all the files and master it only has up to four\nthe five and six are still in my first brush and I have not emerged it yet. And I'm not\ngoing to use git merge right now. I'm going to use rebase this time instead of using git\nmerge and this you'll see that this will actually do the same thing. So for that you just have\nto use the command. So let us go back to our first branch. Okay did a typing error? Irst\nBR a MCH. Okay switch the first branch and now we're going to use the command git rebase\nmaster. Now it is showing that my current Branch first branch is up to date just because\nbecause whatever is in the master branch is already there in my first branch and they\nwere no new files to be added. So that is the thing. So, but if you want to do it in\nthe reverse way, I'll show you what will happen. So let's just go and check out let's do rebasing\nkit rebase first branch. So now what happened is that all the work of first branch has been\nattached to the master branch and it has been done linearly. There was no new set of comments.\nSo now if you see all the files are the master Branch, you'll find that you've got a new\nfive and Ed U6 as well, which was in the first Branch. So basically rebasing has merged all\nthe work of my first Branch into the master, but the only thing that happened is that it\nhappened in a linear way all the commits that we did in first Branch actually got rid dashed\nto the head in the master. So this was all about nonlinear development. I have told you\nabout branching merging rebasing we've made changes with pull changes committed changes,\nbut I remember that I haven't shown you how to push changes. So since we're done working\nin our local repository now, we have made are all final changes and now we want it to\ncontribute in our Central Repository. Tree. So for that we're going to use git push and\nI'm going to show you how to do a get Bush right now. Before I go ahead to explain you\na get Bush. You have to know that when you are actually setting up your repository. If\nyou remember your GitHub repository as a public repository, it means that you're giving a\nread access to everyone else in the GitHub community. So everyone else can clone or download\nyour repository files. So when you're pushing changes in a repository, you have to know\nthat you need to have certain access rights because it is the central repository. This\nis where you're storing your actual code. So you don't want other people to interfere\nin it by pushing wrong codes or something. So we're going to connect a mice and repository\nvia ssh in order to push changes into my central repository now at the beginning when I was\ntrying to make this connection with SSS rows facing some certain kind of problems. Let\nme go back to the repository of me show you when you click this button. You see that this\nis your HTTP URL in order that we use in order to connect with yours and repository now if\nyou want to use SSH, so this is your SSH connection URL. So so in order to connect with ssh, what\ndo you need to do is that you have to generate a public SSH key and then just add that key\nsimply into your GitHub account. And after that you can start pushing changes. So first\nwe'll do that will generate SSH public key. So for that, we'll use this command SSH - heejun.\nSo under file, there is already an SSH key, so they want to override it. Yes. So my SSH\nkey has been generated and it has been saved in here. So if I want to see it and just use\ncat and copy it. So this is my public SSH key if I want to add this SSH key, I'll go\nback into my GitHub account. And here I will go back and settings and we'll go and click\non this option SSH and gpg keys and I've already had two SSH Keys added and I want to add my\nnew one. So I'm going to click this button new SSH key and just make sure that you provide\na name to it. I'm just going to keep it in order because I've named the other ones sssh\nwon an SSS to just say I'm going to say it's sh3. So just paste your search key in here.\nJust copy this key. Paste it and click on this button, which is ADD SSH key. Okay, so\nnow well the first thing you need to do is clear the screen. And now what you need to\ndo is you need to use this command as the search - d And your SSI at URL that we use\nwhich is get at the rate github.com. And enter so my SSH authentication has been successfully\ndone. So I'll go back to my GitHub account. And if I refresh this you can see that the\nkey is green. It means that it has been properly authenticated and now I'm ready to push changes\non to the central repository. So we'll just start doing it. So let me just tell you one\nmore thing that if you are developing something in your local repository and you have done\nit in a particular branch in your repository and let's say that you don't want to push\nthis changes into the master branch of your central report or your GitHub repository.\nSo let's say that whatever work that you have done. It will stay in a separate branch in\nyour GitHub repository so that it does not interfere with the master branch and everyone\ncan identify that it is actually your branch and you have created it and this Branch only\ncontains your work. So for that let me just go to the GitHub repository and show you something.\nLet's go to the repositories. And this is the repository that I have just created today.\nSo when you go in the repository, you can see that I have only got one branch here,\nwhich is the master branch. And if I want to create branches I can create it here, but\nI would advise you to create all branches from your command line or from you get bash\nonly in your central repository as well. So let us go back in our branch. So now what\nI want is that I want all the work of the first branch in my local repository to make\na new branch in the central repository and that branch in my central repository will\ncontain all the files that is in the first branch of my local repository through so for\nthat I'll just perform. get Push the name of my remote which is origin and first branch.\nAnd you can see that it has pushed all the changes. So let us verify. Let us go back\nto our repository and let's refresh it. So this is the master branch and you can see\nthat it has created another branch, which is called the first Branch because I have\npushed all the files from my first Branch into an and I have created a new Branch or\nfirst Branch as similar to my first branch in my local repository here in GitHub. So\nnow if we go to Branch you can see that there is not only a single Master we have also got\nanother branch, which is called the first Branch now if you want to check out this brand\njust click on it. And you can see it has all the files with all the combat logs here in\nthis Branch. So this is how you push changes and if you want to push all the change in\nto master you can do the same thing. Let us go back to our Branch master. And we're going\nto perform a git push here. But only what we're going to do this time is we're going\nto push all the files into the master branch and my central repository. So for that I'll\njust use this get bush. Okay, so the push operation is done. And if you go back here\nand if you go back to master, you can see that all the files that were in the master\nbranch in. My local repo has been added into the master branch of my central Ripple also.\nSo this is how you make changes and from your central repository to look repository. So\nthis is exactly what you do with get so if I have to summarize what I just showed you\nentirely in this when I'm when I was telling about get ad and committing and pushing and\npulling so this is exactly what is happening. So this is your local repository. This is\nyour working directory. So the staging area is our index the intermediate layer between\nyour workspace and your local repository. So you have to add your files into the staging\narea or the index with Git add and a commit those changes with Git commit and your local\nrepository and if you want to push all this Listen to the remote repository or the central\nrepository where everyone can see it you use a get Bush and similarly. If you want to pull\nall those files of fetch all those files from your GitHub repository, you can use git pull\nand you want to use branches. If you want to move from one branch to another you can\nuse git checkout. And if you want to combine the work of different branches together, you\ncan use git merge. So this is entirely what you do when you're performing all these kind\nof operations. So I hope it is clear to everyone so I'll just show you how can you check out\nwhat has been changed and modifications so So just clear the screen and okay. So let\nus go back to our terminal and just for experimentation proper just to show you that how we can actually\nget revert back to our previous changes. So now you might not want to change everything\nthat you made an Eddie wanted to do a duet for or some other files that we just created.\nSo let's just go and create a new file modify it two times and revert back to the previous\nversion just for demonstration purpose. So I'm just going to create a new text file.\nLet's call it revert. And now let us just type something. Hello. Let's just keep it\nthat simple. Just save it and go back. We'll add this file. then commit this let's say\njust call it revert once just remember that this is the first comment that I made with\nrevert one enter. So it has been changed. So now let's go back and modify this. So after\nI've committed this file, it means that it has stored a version with the text Hello exclamation\nin my revert text file. So I'm just going to go back and change something in here. So\nI'm just let us just add there. Hello there. Save it. Let's go back to our bash. Now. Let\nus commit this file again because I've made some changes and I want a different version\nof the revert file. So we'll just go ahead and commit again. So I'll use git commit all.\nSaints River do and enter and it's done. So now if I want to revert back to okay, so now\nyou just see the file. You can see I've modified it. So now it has got hello there. Let's say\nthat I want to go back to my previous version. I would just want to go back to when I had\njust hello. So for that, I'll just check my git log. I can check the hair that this is\nthe commit log or the commit hash. When I first committed revert it means that this\nis the version one of my revert. Now, what you need to do is that you need to copy this\ncommit hash. Now, you can just copy the first eight hexadecimal digits and that will be\nit. So just copy it whole I just clear the screen first. So you just need to go use this\ncommand get check out and hexadecimal code or the hexadecimal digits that you just copied\nand the name of your file, which is revert Dot txt. So you just have to use this command\nkit. Check out and the commit hash that you just copied the first 8 digits and you have\nto name the file, which is revert Dot txt. So now when you just see this file, you have\ngone back to the previous commit. And now when you just display this file, you can see\nthat now I've only got just hello. It means that I have rolled back to the previous version\nbecause I have used the commit hash when I initially committed with the first change.\nSo this is how you revert back to a previous version. So this is what we have learned today\nin today's tutorial. We have understood. What is Version Control and why do we need version\ncontrols? And we've also learned about the different version control tools. And in that\nwe have primarily focused on get and we have learned all about git and GitHub about how\nto create repositories and perform some kind of operations and commands in order to push\npull and move files from one repository to another we've also studied about the features\nof git and we've also seen a case study about how Dominion Enterprises which is one of the\nbiggest public In company who makes very popular websites that we have got right now. We have\nseen how they have used GitHub as well. Hello everyone. This is order from 80 Rekha in today's\nsession will focus on what is Jenkins. So without any further Ado let us move forward\nand have a look at the agenda for today first. We'll see why we need continuous integration.\nWhat are the problems that industries were facing before continuous integration was introduced\nafter that will understand what exactly is continuous integration and will see various\ntypes of continuous integration tools among those countries integration tools will focus\non Jenkins and we'll also look at Jenkins distributed architecture finally in our hands\non part will prepare a build pipeline using Jenkins and I'll also tell you how to add\nJenkins slaves now, I'll move forward and we'll see why we need continuous integration.\nSo this is the process before continuous integration over here, as you can see that there's a group\nof developers who are making changes to the source code that is present in the source\ncode repository. This repository can be a git repository subversion repository Etc.\nAnd then the entire source code of the application is written it will be built by tools like\nand Maven Etc. And after that that built application will be deployed onto the test server for\ntesting if there's any bug in the code developers are notified with the help of the feedback\nloop as you can see it on the screen and if there are no bugs then the application is\ndeployed onto the production server release. I know you must be thinking that what is the\nproblem with this process is process looks fine. As you first write the code then you\nbuild it. Then you test it and finally you deploy but let us look at the flaws that were\nthere in this process one by one. So this is the first problem guys as you can see that\nthere is a developer who's waiting for a long time in order to get the test results as first\nthe entire source code of the application will be built and then only it will be deployed\nonto the test server for testing. It takes a lot of time so developers have to For a\nlong time in order to get the test results. The second problem is since the entire source\ncode of the application is first build and then it is tested. So if there's any bug in\nthe code developers have to go through the entire source code of the application as you\ncan see that there is a frustrated developer because he has written a code for an application\nwhich was built successfully but in testing there were certain bugs in that so he has\nto check the entire source code of the application in order to remove that bug which takes a\nlot of time so basically locating and fixing of bugs was very time-consuming. So I hope\nyou are clear with the two problems that we have just discussed now, we'll move forward\nand we'll see two more problems that were there before continuous integration. So the\nthird problem was software delivery process was slow developers were actually wasting\na lot of time in locating and fixing of birds instead of building new applications as we\njust saw that locating and fixing of bugs was a very time-consuming task due to which\ndevelopers are not able to focus on building new applications. You can relate that to the\ndiagram which is present in front of your screen as Always a lot of time in watching\nTV doing social media similarly developers were also basic a lot of time in fixing bugs.\nAll right. So let us have a look at the fourth problem that is continuous feedback continues\nfeedback related to things like build failures test status Etc was not present due to which\nthe developers were unaware of how their application is doing the process that you showed before\ncontinuous integration. There was a feedback loop present. So what I will do I will go\nback to that particular diagram and I'll try to explain you from there. So the feedback\nloop is here when the entire source code of the application is built and tested then only\nthe developers are notified about the bugs in the code. All right, when we talk about\nCantonese feedback suppose this developer that I'm highlighting makes any commit to\nthe source code that is present in the source code repository. And at that time the code\nshould be pulled and it should be built and the moment it is built the developer should\nbe notified about the build status and then once it is built successfully it is then deployed\nonto the test server for testing at that time. Whatever the test data says the developer\nshould be notified about it. Similarly, if this developer makes any commit to the source\ncode at that time. The coach should be pulled. It should be built and the build status should\nbe notified the developers after that. It should be deployed onto the test server for\ntesting and the test results should also be given to the developers. So I hope you all\nare clear. What is the difference between continents feedback and feedback? So incontinence\nfeedback you're getting the feedback on the run. So we'll move forward and we'll see how\nexactly continuous integration addresses these problems. Let us see how exactly continuous\nintegration is resolving the issues that we have discussed. So what happens here, there\nare multiple developers. So if any one of them makes any commit to the source code that\nis present in the source code repository, the code will be pulled it will be built tested\nand deployed. So what advantage we get here. So first of all, any comment that is made\nto the source code is built and tested due to which if there is any bug in the code developers\nactually know where the bug is present or bitch come it has caused that error so they\ndon't need to go through the entire source code of the application. They just need to\ncheck that particular. Because introduce the button. All right. So in that way locating\nand fixing of bugs becomes very easy apart from that the first problem that we saw the\ndevelopers have to wait for a long time in order to get the test result here every commit\nmade to the source code is tested. So they don't need to wait for a long time in order\nto get the test results. So when we talk about the third problem that was software delivery\nprocess was slow is completely removed with this process developers are not actually focusing\non locating and fixing of bugs because that won't take a lot of time as we just discussed\ninstead of that. They're focusing on building new applications. Now a fourth problem was\nthat there is feedback was not present. But over here as you can see on the Run developers\nare getting the feedback about the build status test results Etc developers are continuously\nnotified about how their application is doing. So I will move forward now, I'll compare the\ntwo scenarios that is before continuous integration and after continuous integration now over\nhere what you can see is before continuous integration as we just saw first the source\ncode of the application will be built the entire source code then only it will be tested.\nBut when we talk about after continuous integration every commit whatever change you made in the\nsource code whatever change the my new changes. Well you committed to the source code that\ntime only the code will be pulled. It will be built and then lll be tested developers\nhave to wait for a long time in order to get the test results as we just saw because the\n- source code will be first build and then it will be deployed onto the test server.\nBut when we talk about continuous integration the test result of every come it will be given\nto the developers and when we talk about feedback, there was no feedback that was present earlier,\nbut in continuous integration feedback is present for every committee met to the source\ncode. You will be provided with the relevant result. All right, so now let us move forward\nand we'll see what exactly is continuous integration now in continuous integration process developers\nare required to make frequent commits to the source code. They have to frequently make\nchanges in the source code and because of that any change made in the source code, it\nwill report by The Continuous integration server, and then that code will be built or\nyou can say it will be compiled. All right now. Pentagon The Continuous integration tool\nthat you are using or depending on the needs of your organization. It will also be deployed\nonto the test server for testing and once testing is done. It will also be deployed\nonto the production server for release and developers are continuously getting the feedback\nabout their application on the run. So I hope I'm clear with this particular process. So\nwe'll see the importance of continuous integration with the help of a case study of Nokia. So\nNokia adopted a process called nightly build nightly build can be considered as a predecessor\nto continuous integration. Let me tell you why. All right. So over here as you can see\nthat there are there are developers who are committing changes to the source code that\nis present in a shared repository. All right, and then what happens in the night? There\nis a build server. This build server will pull the shared repository for changes and\nthen it'll pull that code and prepare a bill. All right. So in that way whatever commits\nare made throughout the day are compiled in the night. So obviously this process is better\nthan writing the entire source code of the application and then Bai Ling it but again\nsince if there is any bug in the code developers have to check all the comments that have been\nmade throughout the day so it is not the ideal way of doing things because you are again\nwasting a lot of time in locating and fixing of bucks. All right, so I want answers from\nyou all guys. What can be the solution to this problem. How can Nokia address is particular\nproblem since we have seen what exactly continuous integration is and why we need now without\nwasting any time. I'll move forward and I'll show you how Nokia solved this problem. So\nNokia adopted continuous integration as a solution in which what happens developers\ncommit changes to the source code in a shared repository. All right, and then what happens\nis a continuous integration server this continuous integration server pose the repository for\nchanges if it finds that there is any change made in the source code and it will pull the\ncode and compile it. So what is happening the moment you commit a change in the source\ncode continuous integration server will pull that and prepare a build. So if there is any\nbug in the code developers know which government is causing that error. All right, so they\ncan do Go through that particular commit in order to fix the bug. So in this way locating\nand fixing of box was very easy, but we saw that in nightly builds if there is any bug\nthey have to check all the comments that have been made throughout the day. So with the\nhelp of continuous integration, they know which commits is causing that error. So locating\nin fixing of bugs didn't take a lot of time. Okay before I move forward, let me give you\na quick recap of what we have discussed till now first. We saw why we need continuous integration.\nWhat were the problems that industries were facing before continuous integration was introduced\nafter that. We saw how continuous integration addresses those problems and we understood\nwhat exactly continuous integration is. And then in order to understand the importance\nof continuous integration, we saw case study of Nokia in which they shifted from nightly\nbuild to continuous integration. So we'll move forward and we'll see various continuous\nintegration tools available in the market. These are the four most widely used continuous\nintegration tools. First is Jenkins on which we will focus in today's session then buildbot\nTravis and bamboo. Right and let us move forward and see what exactly jenkins's so Jenkins\nis a continuous integration tool. It is an open source tool and it is written in Java\nhow it achieves continuous integration. It does that with the help of plugins. Jenkins\nhave well over a thousand plugins. And that is the major reason why we are focusing on\nJenkins. Let me tell you guys it is the most widely accepted tool for continuous integration\nbecause of its flexibility and the amount of plugins that it supports. So as you can\nsee from the diagram itself that it is supporting various development deployment testing Technologies,\nfor example gate Maven selenium puppet ansible lawgivers. All right. So if you want to integrate\na particular tool you need to make sure that plug-in for that tool is installed in your\nJenkins the for better understanding of Jenkins. Let me show you the Jenkins dashboard. I've\ninstalled Jenkins in my Ubuntu box. So if you want to learn how to install Jenkins,\nyou can refer the Jenkins installation video. So this is a Jenkins dashboard guys, as you\ncan see that there are currently no jobs because of that this section is empty otherwise We'll\ngive you the status of all your build jobs over here. Now when you click on new item,\nyou can actually start a new project all over from scratch. All right. Now, let us go back\nto our slides. Let us move forward and see what are the various categories of plugins\nas I told you earlier is when the Jenkins achieves continuous integration with the help\nof plugins. All right, and Jenkins opposed well over a thousand plugins and that is the\nmajor reason why Jenkins is so popular nowadays. So the plug-in categorization is there on\nyour screen but there are certain plugins for testing like j-unit selenium Etc when\nwe talk about reports, we have multiple plugins, for example HTML publisher for notification.\nAlso, we have many plugins and I've written one of them that is Jenkins build notification\nplug-in and we talked about deployment we have plugins like deploy plug-in when we talk\nabout compiled we have plugins like Maven and Etc. Alright, so let us move forward and\nsee how to actually install a plug-in on the same about to box where my Jenkins is installed.\nSo over here in order to install Jenkins, what you need to do is you need to click on\nmanage. Ken's option and overhead, as you can see that there's an option called manage\nplugins. Just click over there. As you can see that it has certain updates for the existing\nplugins, which I have already installed. Right then there's an option called installed where\nyou'll get the list of plugins that are there in your system. All right, and at the same\ntime, there's an option called available. It will give you all the plugins that are\navailable with Jenkins. Alright, so now what I will do I will go ahead and install a plug-in\nthat is called HTML publisher. So it's very easy. What you need to do is just type the\nname of the plug-in. Headed HTML publisher plugin, just click over there and install\nwithout restart. So it is now installing that plug-in we need to wait for some time. So\nit has now successfully installed now, let us go back to our Jenkins dashboard. So we\nhave understood what exactly Jenkins is and we have seen various 10 kids plugins as well.\nSo now is the time to understand Jenkins with an example will see a general workflow how\nJenkins can be used. All right. So let us go back to our slides. So now as I have told\nyou earlier as well, we'll see Jenkins example, so let us move forward. So what are what is\nhappening developers are committing changes to the source code and that source code is\npresent in a shared repository. It can be a git repository subversion repository or\nany other repository. All right. Now, let us move forward and see what happens now now\nwe're here what is happening. There's a Jenkins server. It is actually polling the source\ncode repository at regular intervals to see if any developer has made any commit to the\nsource code. If there is a change in the source code it will pull the code and we'll prepare\na build and at the same time developers will be notified about the build results now, let\nus execute this practically. All right, so I will again go back to my Jenkins dashboard,\nwhich is there in my Ubuntu bar. What had what I'm going to do is I'm going to create\na new item read basically a new project now over here. I'll give a suitable named my project\nyou can use any name that you want. I'll just write compile. And now I click on freestyle\nproject. The reason for doing that is free-style project is the most configurable and the flexible\noption. It is easier to set up as well. And at the same time many of the options that\nwe configure here are present in other build jobs as well move forward with freestyle project\nand I'll click on ok now over here what I'll do, I'll go to the source code management\nTab and it will ask you for what type of source code management you want. I'll click on get\nand over here. You need to type your repository URL in my case. It is http. github.com your\nusername slash the name of your Repository. And finally dot get all right now in the bill\nauction, you have multiple options. All right. So what I will do I click on invoke top-level\nMaven targets. So now over here, let me tell you guys it may even has a built life cycle\nand that build life cycle is made up of multiple build phases. Typically the sequence for build\nphase will be festive validate the code then you compile it. Then you test it. Then you\nperform unit test by using suitable unit testing framework. Then you package your code in a\ndistributable format like a jars, then you verify it and you can actually install any\npackage that you want with the help of install build phase and then you can deploy it in\nthe production environment for release. So I hope you have understood the maven build\nlife cycle. So in the goals tab, so what I need to do is I need to compile the code that\nis present in the GitHub account. So for that in the gold stabbed I need to write compile.\nSo this will trigger the compile build phase of Maven now, that's it guys. That's it. Just\nclick on apply. And save now on the left hand side. There's an option called bill now to\ntrigger the built just click over there and you will be able to see the the Builder starting\nin order to see the console output. You can click on that build and you see the console\noutput. So it has validated the GitHub account and it is now starting to compile that code\nwhich is there in the GitHub account. So we are successfully compiled the code that was\npresent in the GitHub account. Now, let us go back to the Jenkins dashboard. Now in this\nJenkins dashboard, you can see that my project is displayed over here. And as you can see\nthe blue color of the ball indicates that as that it has been successfully executed.\nAll right. Now, let us go back to the slides now, let us move forward and see what happens.\nOnce you have compile your code. Now the code that you have compiled you need to test it.\nAll right. So what Jenkins will do it will deploy the code onto the test server for testing\nand at the same time developers will be notified about the test results as well. So let us\nagain execute this practically, I'll go back to my Ubuntu box again. So in the GitHub repository,\nthe test cases are already defined. Alright, so we are going to analyze those test cases\nwith the help of Maven. So let me tell you how to do it will again go and click on new\nitem on over here will give any suitable name to a project. I'll just type test. I'll again\nuse freestyle project for the reason that I've told you earlier click on OK and in the\nsource code management tab. Now before applying unit testing on the code that I've compiled.\nI need to First review it with the help of PMD plug-in. I'll do that. So for that I will\nagain click on new item and a over here. I need to type the name of the project. So I'll\njust type it as code underscore review. Freestyle project click. Ok. Now the source code management\ntab. I will again choose gate and give my repository URL https. github.com username\n\/ name of the Repository . Kit All right now scroll doubt now in the build tab. I'm going\nto click over there. And again, I will click on invoke top-level Maven targets now in order\nto review the code. I am going to use the Matrix profile of Maven. So how to do that.\nLet me tell you you need to type here - p Matrix PMD: PMD, all right, and this will\nactually produce a PMD report that contains all the warnings and errors now in the post\nBill action tab, I click on publish PMD analysis result. That's all click on apply and Save\nthe finally click on Bill now. And let us see the console output. So it has now pulled\nthe code from the GitHub account and Performing the code review. So they successfully review\nthe code now. Let us go back to the project over here. You can see an option called PMD\nwarnings just click over there and it will display all the warnings that are there present\nin your code. So this is the PMD Alice's report over here. As you can see that there are total\n11 warnings and you can find the details here as well like package you have then you have\nthen you have categories then the types of warnings which are there like for example,\nempty cache blocks empty finally block. Now, you have one more tab called warnings over\nthere. You can find where the warning is present the filename package. All right, then you\ncan find all the details in the details tab. It will actually tell you where the warning\nis present in your code. All right. Now, let us go back to the Jenkins dashboard and now\nwe'll perform unit tests on the code that we have compiled for that again. I'll click\non new item and I'll give a name to this project. I will just type test. And I click on freestyle\nproject. Okay. Now in the source code management tab, I'll click on get now over here. I'll\ntype the repository URL http. github.com \/ username \/ name of the Repository . Kit and in the\nbuild option I click on again invoke top-level Maven targets now over here as I've told you\nearlier as well that Maven build life cycle has multiple build phases like first it would\nvalidate the code compile then tested package that will verify then it will install if certain\npackages are required. And then finally it will deploy it. Alright. So one of the phase\nis actually testing that performs unit testing using the suitable unit testing framework.\nThe test cases are already defined in my GitHub account. So to analyze the test case in the\nGold section, I need to write tests. All right, and it will invoke the test phase of the maven\nbuild life cycle. All right, so just click on apply and Save finally click on Builder\nTo see the console output click here now in the source code management tab. I'll select\nget all right over here again. I need to type my repository URL. That is HTTP github.com\n\/ username. \/ repository name dot get and now in the build tab. I'll select invoke top-level\nMaven targets and over here as I have told you earlier as well that the maven build life\ncycle has multiple phases. All right, and one of that phase is unit tests, so in order\nto invoke that unit test what I need to do is in the goals tab, I need to write tests\nand it will invoke the test build phase of the maven build life cycle. All right. So\nthe moment I write tests here and I'll build it. It will actually analyze the test cases\nthat are present in the GitHub account. So let us write test and apply and Save Finally\nclick on Bill now. And in order to see the console output click here. So does pull the\ncode from the GitHub account and now it's performing unit test. So we have successfully\nperform testing on that code now, I will go back to my Jenkins dashboard or as you can\nsee that all the three build jobs that have executed a successful which is indicated with\nthe help of view colored ball. All right. Now, let us go back to our slides. So we have\nsuccessfully performed in unit tests on the test cases that were there on the GitHub account\nnow, we'll move forward and see what happens after that. Now finally, you can deploy that\nbuild application or to the production environment for release, but when you have one single\nJenkins over there are multiple disadvantages. So let us discuss that one by one so we'll\nmove forward and we'll see what are the disadvantages of using one single Jenkins over now. What\nI'll do I'll go back to my Jenkins dashboard and I'll show you how to create a build pipeline.\nAll right. So for that I'll move to my Ubuntu box. Once again now over here you can see\nthat there is an option of plus. Ok, just click over there now over here click on build\npipeline view, whatever name you want. You can give I'll just give it as a do Rekha.\npipeline And click on ok. Now over here what you can do you can give some certain description\nabout your bill pipeline. All right, and there are multiple options that you can just have\na look and over here. There's an option called select initial job. So I want compiled to\nbe my first job and there are display options over here number of display builds that you\nwant. I'll just keep it as 5 now the row headers that you want column headers, so you can just\nhave a look at all these options and you can play around with them just for the introductory\nexample, let us keep it this way now finally click on apply and ok. Currently you can see\nthat there is only one job that is compiled. So what I'll do, I'll add more jobs this pipeline\nfor that. I'll go back to my Jenkins dashboard and over here. I'll add code review as well.\nSo for that I will go to configure. And in this bill triggers tab, what I'll do I click\non build after other projects are built. So whatever project that you want to execute\nbefore code review just type that so I want compile. Yeah, click on compile and over here.\nYou can see that there are multiple options like trigger only if build stable trigger,\neven if the build is unstable trigger, even if the build page so I'll just click on a\ntrigger even if the bill fails. All right, finally click on apply and safe. Similarly\nif I want to add my test job as well to the pipeline. I can click on configure and again\nthe bill triggers tab. I'll click on build after other projects are built. So overhead\ntype the project that you want to execute before this particular project in our case.\nIt is code review. So let us click over there trigger, even if the build fails apply and\nSave Now let us go back to the dashboard and see how our pipeline looks like. So this is\nour pipeline. Okay, so when we click on run Let us see what happens first. It will compile\nthe code from the GitHub account. That is it will pull the code and it will compile\nit. So now this compile is done. All right, now it will review the code. So the code review\nhas started in order to see the log. You can click on Console. It will give you the console\noutput. Now once code review is done. It will start testing. It will perform unit tests\nor it's a code has been successfully reviewed with the as you can see the color has become\ngreen. Now, the testing has started it will perform unit tests on the test case is that\nthere in the GitHub account? So we have successfully executed three build jobs that is compile\nthe code then review it and then perform testing. All right, and this is the build pipeline\nguys. So let us go back to the Jenkins dashboard. And we'll go back to our slides now. So now\nwe have successfully performed unit tests on the test cases that are present in the\nGitHub account. All right. Now, let us move forward and see what else you can do with\nJenkins. Now the application that we have tested that can also be deployed onto the\nproduction server for release as well. Alright, so now let us move forward and see what are\nthe disadvantages of this one single Jenkins over. So there are two major disadvantages\nof using one single Jenkins over first is you might require different environments for\nyour builds and test jobs. All right. So at that time one single Jenkins over cannot serve\na purpose and the second major disadvantages suppose. You have a heavier projects to build\non regular basis. So at that time one single Jenkins server cannot simply handle the load.\nLet us understand this with an example suppose. If you need to run web test using Internet\nExplorer. So at that time you need a Windows machine, but your other build jobs might require\na Linux box. So you can't use one single Jenkins over. All right, so let us move forward. See\nwhat is actually the solution to this problem the solution to this problem is Jenkins distributed\narchitecture. So the Jenkins distributed architecture consists of a Jenkins master and multiple\nJenkins slave. So this Jenkins Master is actually used for scheduling build jobs. It also dispatches\nbuilds to the slaves for actual execution. All right, it also monitors a slave that is\npossibly taking them online and offline as required and it also records and presents\nthe build results and you can directly executable job or Master instance as well. Now when we\ntalk about Jenkins slaves, these slaves are nothing but the Java executable that are present\non remote machines. All right, so these slaves basically here's the request of the Jenkins\nmaster or you can say they perform the jobs As Told by the Jenkins Master they operate\non variety of operating system. So you can configure Jenkins in order to execute a particular\ntype of builds up on a particular Jenkins slave or on a particular type of Jenkins slave\nor you can actually let Jenkins pick the next available. Budget get slave. All right. Now\nI go back again to my Ubuntu box and I'll show you practically how to add Jenkins slaves\nnow over here as you can see that there is an option called Mana Jenkins just click over\nthere and when you scroll down you'll see man option called managed nodes under the\nleft hand side. There is an option called new node. Just click over there click on permanent\nagent give a name to your slave. I'll just give it as slave underscore one. Click on\nOK over here. You need to write the remote root directory. So I'll keep it as slash home\nslash Edureka. And labels are not mandatory still if you want you can use that and launch\nmethod. I want it to be launched slave agents via SSH. All right over here. You need to\ngive the IP address of your horse. So let me show you the IP address of my Host this\nmy Jenkins slave, which I'll be using like Jenkins slave. So, this is the machine that\nI'll be using as Jenkins slave in order to check the IP address. I'll type ifconfig.\nThis is the IP address of that machine just copy it. Now I'll go back to my Jenkins master.\nAnd in the host tab, I'll just paste that IP address and over here. You can add the\ncredentials to do that. Just click on ADD and over here. You can give the user name.\nI'll give it as root password. That's all just click on ADD. And over here select it.\nFinally save it. Now it is currently adding the slave in order to see the logs. You can\nclick on that slave again. Now, it has successfully added that particular slave. Now what I'll\ndo, I'll show you the logs for that and click on slave. And on the left hand side, you will\nnotice an option called log just click over there and we'll give you the output. So as\nyou can see agent has successfully connected and it is online right now. Now what I'll\ndo, I'll go to my Jenkins slave and I'll show you in slash home slash enter a car that it\nis added. Let me first clear my terminal now what I'll do, I'll show you the contents of\nSlash home slash at Eureka. As you can see that we have successfully added slave dot\njar. That means we have successfully added Jenkins slave to our Jenkins Master. Hello\neveryone. This is ordered from 80 Rekha and today's session will focus on what is docker.\nSo without any further Ado let us move forward and have a look at the agenda for today first.\nWe'll see why we need Docker will focus on various problems that industries were facing\nbefore Docker was introduced after that will understand what exactly Docker is and for\nbetter understanding of Docker will also look at a Docker example after that will understand\nhow Industries are using Docker with the case study of Indiana University. Our fifth topic\nwill focus on various Docker components, like images containers Etc and our Hands-On part\nwill focus on installing WordPress and phpmyadmin using Docker compose. So we'll move forward\nand we'll see why we need Docker. So this is the most common problem that industries\nwere facing as you can see that there is a developer who has built an application that\nworks fine in his own environment. But when it reach production there were certain issues\nwith that application. Why does that happen that happens because of difference in the\nComputing environment between deaf and product I'll move forward and we'll see the second\nproblem before we proceed with the second problem. It is very important for us to understand.\nWhat a microservices consider a very large application that application is broken down\ninto smaller Services. Each of those Services can be termed as micro services or we can\nput it in another way as well microservices can be considered a small processes that communicates\nwith each other over a network to fulfill one particular goal. Let us understand this\nwith an example as you can see that there is an online shopping service application.\nIt can be broken down into smaller micro services like account service product catalog card\nserver and Order server Microsoft was architecture is gaining a lot of popularity nowadays even\ngiants like Facebook and Amazon are adopting micro service architecture. There are three\nmajor reasons for adopting microservice architecture, or you can say there are three major advantages\nof using Microsoft's architecture first. There are certain applications which are easier\nto build and maintain when they are broken down into smaller pieces or smaller Services.\nSecond reason is suppose if I want to update a particular software or I want a new technology\nstack in one of my module on one of my service so I can easily do that because the dependency\nconcerns will be very less when compared to the application as a whole. Apart from that\nthe third reason is if any of my module of or any of my service goes down, then my whole\napplication remains largely unaffected. So I hope we are clear with what our micro services\nand what are their advantages so we'll move forward and see what are the problems in adopting\nthis micro service architecture. So this is one way of implementing microservice architecture\nover here, as you can see that there's a host machine and on top of that host machine there\nare multiple virtual machines each of these virtual machines contains the dependencies\nfor one micro service. So you must be thinking what is the disadvantage here? The major disadvantage\nhere is in Virtual machines. There is a lot of wastage of resources resources such as\nRAM processor disk space are not utilized completely by the micro service which is running\nin these virtual machines. So it is not an ideal way to implement microservice architecture\nand I have just given you an example of five microservices. What if there are more than\n5 micro Services what if your application is so huge that it requires? Microsoft versus\nso at that time using virtual machines doesn't make sense because of the wastage of resources.\nSo let us first discuss the implementation of microservice problem that we just saw.\nSo what is happening here. There's a host machine and on top of that host machine. There's\na virtual machine and on top of that virtual machine, there are multiple Docker containers\nand each of these Docker containers contains the dependencies 41 Microsoft Office. So you\nmust be thinking what is the difference here earlier? We were using virtual machines. Now,\nwe are using our Docker containers on top of virtual machines. Let me tell you guys\nDocker containers are actually lightweight Alternatives of virtual machines. What does\nthat mean in Docker containers? You don't need to pre-allocate any Ram or any disk space.\nSo it will take the RAM and disk space according to the requirements of applications. All right.\nNow, let us see how Dockers all the problem of not having a consistent Computing environment\nthroughout the software delivery life cycle. Let me tell you first of all Docker containers\nare actually developed by the developers. So now let us see how Dockers all the first\nThat we saw where an application works fine and development environment but not in production.\nSo Docker containers can be used throughout the SCLC life cycle in order to provide consistent\nComputing environment. So the same environment will be present in Dev test and product. So\nthere won't be any difference in the Computing environment. So let us move forward and understand\nwhat exactly Docker is. So the docker containers does not use the guest operating system. It\nuses the host operating system. Let us refer to the diagram that is shown. There is the\nhost operating system and on top of that host operating system. There's a Docker engine\nand with the help of this Docker engine Docker containers are formed and these containers\nhave applications running in them and the requirements for those applications such as\nall the binaries and libraries are also packaged in the same container. All right, and there\ncan be multiple containers running as you can see that there are two containers here\n1 & 2. So on top of the host machine is a docker engine and on top of the docker engine\nthere are multiple containers and Each of those containers will have an application\nrunning on them and whatever the binaries and library is required for that application\nis also packaged in the same container. So I hope you are clear. So now let us move forward\nand understand Docker in more detail. So this is a general workflow of Docker or you can\nsay one way of using Docker over here. What is happening a developer writes a code that\ndefines an application requirements or the dependencies in an easy to write Docker file\nand this Docker file produces Docker images. So whatever dependencies are required for\na particular application is present inside this image and what our Docker containers\nDocker containers are nothing but the runtime instance of Docker image. This particular\nimage is uploaded onto the docker Hub. Now, what is Docker Hub? Docker Hub is nothing\nbut a git repository for Docker images it contains public as well as private repositories.\nSo from public repositories, you can pull your image as well and you can upload your\nown images as well on to the docker Hub. All right from Docker Hub various teams such as\nQA or production. We'll pull the image and prepare their own containers as you can see\nfrom the diagram. So what is the major advantage we get through this workflow? So whatever\nthe dependencies that are required for your application is actually present throughout\nthe software delivery life cycle. If you can recall the first problem that we saw that\nan application works fine in development environment, but when it reaches production, it is not\nworking properly. So that particular problem is easily resolved with the help of this particular\nworkflow because you have a same environment throughout the software delivery lifecycle\nbe Dev test or product will see if a better understanding of Docker a Docker example.\nSo this is another way of using Docker in the previous example, we saw that Docker images\nwere used and those images were uploaded onto the docker Hub. I'm from Doc and have various\nteams were pulling those images and building their own containers. But Docker images are\nhuge in size and requires a lot of network bandwidth. So in order to say that Network\nbandwidth, we use this kind of a work flow over here. We use Jenkins server. Or any continuous\nintegration server to build an environment that contains all the dependencies for a particular\napplication or a Microsoft Office and that build environment is deployed onto various\nteams, like testing staging and production. So let us move forward and see what exactly\nis happening in this particular image over here developer has written complex requirements\nfor a micro service in an easy to write dockerfile. And the code is then pushed onto the get repository\nfrom GitHub repository continuous integration servers. Like Jenkins will pull that code\nand build an environment that contains all they have dependencies for that particular\nmicro service and that environment is deployed on to testing staging and production. So in\nthis way, whatever requirements are there for your micro service is present throughout\nthe software delivery life cycle. So if you can recall the first problem we're application\nworks fine in Dev, but does not work in prod. So with this workflow we can completely remove\nthat problem because the requirements for the Microsoft Office is present throughout\nThe software delivery life cycle and this image also explains how easy it is to implement\na Microsoft's architecture using Docker now, let us move forward and see how Industries\nare adopting Docker. So this is the case study of Indiana University before Docker. They\nwere facing many problems. So let us have a look at those problems one by one. The first\nproblem was they were using custom script in order to deploy that application onto various\nvm's. So this requires a lot of manual steps and the second problem was their environment\nwas optimized for legacy Java based applications, but they're growing environment involves new\nproducts that aren't solely java-based. So in order to provide these students the best\npossible experience, they needed to began modernizing their applications. Let us move\nforward and see what all other problems Indiana University was facing. So in the previous\nproblem of dog, Indiana University, they wanted to start modernizing their applications. So\nfor that they wanted to move from a monolithic architecture to a Microsoft Office architecture\nand the previous slides. We also saw that if you want to update a particular technology\nin one of your micro service it is easy to do that because will be very less dependency\nconstrains when compared to the whole application. So because of that reason they wanted to start\nmodernizing their application. They wanted to move to a micro service architecture. Let\nus move forward and see what are the other problems that they were facing Indiana University\nalso needed security for their sensitive student data such as SSN and student health care data.\nSo there are four major problems that they were facing before Docker now, let us see\nhow they have implemented Docker to solve all these problems the solution to all these\nproblems was docker Data Center and Docker data center has various components, which\nare there in front of your screen first is universal control plane, then comes ldap swarm.\nCS engine and finally Docker trusted registry now, let us move forward and see how they\nhave implemented Docker data center in their infrastructure. This is a workflow of how\nIndiana University has adopted Docker data center. This is dr. Trusted registry. It is\nnothing but the storage of all your Docker images and each of those images contain the\ndependencies 41 Microsoft Office as we saw that the Indiana University wanted to move\nfrom a monolithic architecture to a Microsoft is architecture. So because of that reason\nthese Docker images contain the dependencies for one particular micro service, but not\nthe whole application. All right, after that comes universal control plane. It is used\nto deploy Services onto various hosts with the help of Docker images that are stored\nin the docker trusted registry. So it obscene can manage their entire infrastructure from\none single place with the help of universal control plane web user interface. They can\nactually use it to provision Docker installed software on various hosts, and then deploy\napplications without doing a lot Of manual steps as we saw in the previous slides that\nIndiana University was earlier using custom scripts to deploy our application onto VMS\nthat requires a lot of manual steps that problem is completely removed here when we talk about\nsecurity the role based access controls within the docker data center allowed Indiana University\nto Define level of access to various themes. For example, they can provide read-only access\nto Docker containers for production team. And at the same time they can actually provide\nread and write access to the dev team. So I hope we all are clear with how Indiana University\nhas adopted Docker data center will move forward and see what are the various Docker components.\nFirst is Docker registry Docker registry is nothing but the storage of all your Docker\nimages your images can be stored either in public repositories or in private repositories.\nThese repositories can be present locally or it can be present on the cloud dog. A provides\na cloud hosted service called Docker Hub Docker Hub as public as well as private repositories\nfrom public repositories. You can actually pull an image and prepare your own containers\nat the same time. You can write an image and upload that onto the docker Hub. You can upload\nthat into your private repository or you can upload that on a public repository as well.\nThat is totally up to you. So for better understanding of Docker Hub, let me just show you how it\nlooks like. So this is how a Docker Hub looks like. So first you need to actually sign in\nwith your own login credentials. After that. You will see a page like this, which says\nwelcome to Docker Hub over here, as you can see that there is an option of create repository\nwhere you can create your own public or private repositories and upload images and at the\nsame time. There's an option called explore repositories this contains all the repositories.\nThese which are available publicly. So let us go ahead and explore some of the publicly\navailable repositories. So we have a repositories for nginx reddish Ubuntu then we have Docker\nregistry Alpine Mongo my SQL swarm. So what I'll do I'll show you a centralized repository.\nSo this is the centralized repository which contains the center West image. Now, what\nI will do later in the session, I'll actually pull a centralized image from Docker Hub.\nNow, let us move forward and see what our Docker images and containers. So Docker images\nare nothing but the read-only templates that are used to create containers these Docker\nimages contains all the dependencies for a particular application or a Microsoft Office.\nYou can create your own image and upload that onto the docker Hub. And at the same time\nyou can also pull the images which are available in the public repositories and the in Docker\nHub. Let us move forward and see what our Docker containers Docker containers are nothing\nbut the runtime instances of Docker images it contains everything that is required to\nrun an application or a Microsoft Office and at the same time. It is also possible that\nmore than one image is required to create a one container. Alright, so for better understanding\nof Docker images and Docker containers, what I'll do on my Ubuntu box, I will pull a sin\n2x image and I'll run a sin to waste container in that. So let us move forward and first\ninstall Docker in my Ubuntu box. So guys, this is my Ubuntu box over here first. I'll\nupdate the packages. So for that I will type sudo apt-get update. asking for password it\nis done now. Before installing Docker. I need to install the recommended packages for that.\nI'll type sudo. Apt get install. Line-X - image - extra - you name space - are and now a line\nirks - image - extra - virtual and here we go. Press why? So we are done with the prerequisite.\nSo let us go ahead and install Docker for that. I'll type sudo. apt-get install Docker\n- engine so we have successfully installed Docker if you want to install Docker and send\ntwo ways. You can refer the center is Docker installation video. Now we need to start this\ndocker servicer for that. I'll type sudo service docker start. So it says the job is already\nrunning. Now. What I will do I will pull us into his image from Docker Hub and I will\nrun the center waste container. So for that I will type sudo. Docker pull and the name\nof the image. That is st. OS the first it will check the local registry for Centos image.\nIf it doesn't find there then it will go to the docker hub for st. OS image and it will\npull the image from there. So we have successfully pulled us into his image from Docker Hub.\nNow, I'll run the center as container. So for that I'll type sudo Docker Run - it sent\nOS that is the name of the image. And here we go. So we are now in the Centre ice container.\nLet me exit from this. Clear my terminal. So let us now recall what we did first. We\ninstalled awkard on open to after that. We pulled sent to his image from Docker Hub.\nAnd then we build a center as container using that Center West image now. I'll move forward\nand I'll tell you what exactly Docker compose is. So let us understand what exactly Docker\ncompose is suppose you have multiple applications on various containers and all those containers\nare actually linked together. So you don't want to actually execute each of those containers\none by one but you want to run those containers at once with a single command. So that's where\nDocker compose comes into the picture with Docker compose. You can actually run multiple\napplications present on various containers with one single command that is docker - compose\nup as you can see that there is an example in front of you imagine you're able to Define\nthree containers one running a web app another running a post Kris. And another running a\nred is in a uml file that is called Docker compose file. And from there. You can actually\nexecute all these three containers with one single command. That is Takin - compose up\nlet us understand this with an example suppose. You want to publish a Blog for that you'll\nuse CMS and WordPress is one of the most widely used CMS so you need one. Default WordPress\nand you need one more container for my SQL as bakit and that my SQL container should\nbe linked to the WordPress container apart from that. You need one more container for\nphpmyadmin that should be linked to my SQL database as it is used to access mySQL database.\nSo what if you are able to Define all these three containers in one yamen file and with\none command that is docker - composer, all three containers are up and running. So let\nme show you practically how it is done on the same open to box where I've installed\nDocker and I've pulled a center s image. This is my Ubuntu box first. I need to install\nDocker compose here, but before that I need python pip so for that I will type sudo. Opt\nget installed. Titan - VIP and here we go. So it is done now. I will clear my terminal\nand now I'll install Docker compose for that. I'll type sudo VIP install Docker - compose\nand here we go. So Docker compose is successfully installed. Now I'll make a directory and I'll\nname it as WordPress mkdir WordPress. Now I'll enter this WordPress directory. Now over\nhere, I'll edit Docker - compose dot HTML file using G edit. You can use any other editor\nthat you want. I'll use G edit. So I'll type sudo G edit Docker - compose dot HTML and\nhere we go. So what here what I'll do, I'll first open a document. And I'll copy this\nyeah Mel code. And I will paste it here. So let me tell you what I've done first. I have\ndefined a container as and I'm named it as WordPress. It is built from an image WordPress\nthat is present on the docker Hub. But this WordPress image does not have a database.\nSo for that I have defined one more container and I've named it as WordPress underscore\nDB. It is actually built from the image that is called Maria DB which is present in the\nword press and I need to link this WordPress underscore DB with the WordPress container.\nSo for that I have written links WordPress underscore DB: my SQL. All right, and in the\npost section this port 80 of the docker container will actually be linked to Port eight zero\neight zero of by host machine. So are we clear till here now? What I've done I've defined\na password here as a deer a cow. You can give whatever password that you want and have defined\none more container called phpmyadmin. This container is built from the image corbino's\n\/ talker - phpmyadmin that is present on the docker Hub again. I need to link this particular\ncontainer with WordPress underscore DB container for that. I have written links WordPress underscore\nDB: my SQL and the port section the port 80 of my Docker container will actually be linked\nto Port 80 181 of the host machine and finally I've given a username that is root and I've\ngiven a password as Ed Eureka. So let us now save it and we'll quit Let me first clear\nmy terminal. And now I run a command sudo Docker - compose. Up - D and here we go. So\nthis command will actually pull all the three images and we'll build the three containers.\nSo it is done now. Let me clear my terminal. Now what I'll do, I'll open my browser and\nover here. I'll type the IP address of my machine or I can type the hostname as well.\nFirst name of my machine is localhost. So I'll type localhost and put a zero eight zero\nthat I've given for WordPress. So it will direct you to a WordPress installation page\nover here. You need to fill this particular form, which is asking you for site title.\nI'll give it as editor acre username. Also, I will give as edureka password. I'll type\narea Rekha confirm the use of weak password then type your email address and it is asking\nsearch engine or visibility which I want. So I want click here and finally, I'll click\non install WordPress. So this is my WordPress dashboard and WordPress is now successfully\ninstalled. Now what I'll do, I'll open one more top on over here. I'll type localhost\nor the IP address of a machine and I'll go to Port 80 1814 phpmyadmin. And over here,\nI need to give the user name. If you can recall. I've given route and password has given as\na do Rekha and here we go. So PHP, my admin is successfully installed. This phpmyadmin\nis actually used to access a my SQL database and this my SQL database is used as back-end\nfor WordPress. If you've landed on this video, then it's definitely because you want to install\na Kubernetescluster at your machine. Now, we all know how tough the installation process\nis hence this video on our YouTube channel. My name is Walden and I'll be your host for\ntoday. So without wasting any time let me show you what are the various steps that we\nhave to follow. Now. There are various steps that we have to run both at the Masters and\nand the slave end and then a few commands only at the master sent to bring up the cluster\nand then one command which has to be run at all the slave ends so that they can join the\ncluster. Okay. So let me get started by showing you those commands on those installation steps,\nwhich have to be run commonly on both the Masters and and the slave and first of all,\nwe have to update your repository. Okay, since I am using Ubuntu To update my app to get\nrepository. Okay, and after that we would have to turn up this vapp space be the Masters\nend or the slaves and communities will not work if the swap space is on. Okay, we have\nto disable that so there are a couple of commands for that and then the next part is you have\nto update the hostname the hosts file and we have to set a static IP address for all\nthe nodes in your cluster. Okay, we have to do that because at any point of time if your\nmaster or if your node in the cluster of fails, then when they restart they should have the\nsame IP address if you have a dynamic IP address and then if they restart because of a failure\ncondition, then it will be a problem because they are not be able to join the cluster because\nyou'll have a different IP address. So that's all you have to do these things. All right,\nthere are a couple of commands for that and after that we have to install the openssh\nserver and docker that is because Humanity's requires the openssh functionality and it\nof course needs Docker because everything in kubernetes is containers, right? So we\nare going to make use of Docker containers. So that's why we have to install these two\ncomponents and finally we have to install Q barium. You're black and you have cereal\nnow. These are the core components of your Kubernetes. All right. So these are the various\ncomponents that have to be installed on both your master and your slave and so let me first\nof all open up my VMS and then show you how to get started now before I get started. Let\nme tell you one thing. You have a cluster you have a master and then you have slaves\nin that cluster, right? Your master should always have better configurations than your\nslave. So for that reason, if you're using virtual machines on your host, then you have\nto ensure that your master has at least 2 GB of RAM and to core CPUs. Okay, and your\nslave has 2GB of RAM and at least one core CPU. So these are the basic necessities for\nyour master and slave machines on that note. I think I can get started. So first of all,\nI'll bring up my virtual machine and go through these installation processes. So I hope everyone\ncan see my screen here. This is my first VM and what I'm going to do is I'm going to make\nthis my master. Okay, so all the commands to install the various components are present\nwith me in my notepad Okay, so I'm going to use this for reference and then quickly execute\nthese commands and show you how communities is installed. So first of all, we have to\nupdate our Advocate repository. Okay, but before that, let's log in as s you okay, so\nI'm going to do a sudo OSU so that I can execute all the following commands as pseudo user.\nOkay. So so to OSU there goes my root password and now you can see the difference here right\nhere. I was executing it as a normal user, but from here am a root user. So I'm going\nto execute all these commands as s you so first of all Let's do an update. I'm going\nto copy this and paste it here apt-get update update my Ubuntu repositories. All right,\nso it's going to take quite some time. So just hold on till it's completed. Okay. So\nthis is done. The next thing I have to do is turn off my swap space. Okay. Now the command\nto disable my strap space is swap off space flag a let me go back here and do the same.\nOkay swap off but flag. And now we have to go to this FS tab. So this is a file called\nFS tap OK and we will have a line with the entry of swap space because at any point of\ntime if you have enabled swap space, then you will have a line over there. Now we have\nto disable that line. Okay, we can disable that line by commenting out that line. So\nlet me show you how that's done. I'm just using the Nano Editor to open this fstab file.\nOkay, so you can see this land right where it says swap file. This is the one which after\ncomment out. So just let me come down here and comment it out like this. Okay with the\nhash now, let me save this and exit. Now the next thing after do is update my host name\nand my hosts file and then set a static IP address. So let me get started by first updating\nthe hostname. So for that I have to go to this file host name, which is in this \/hc\npath. So I'm again using Nano for that. You can see here. It's a director - virtualbox,\nright? So let me replace this and say okay Master as in Cuba not he's master. So let\nme save this and exit now if you want your host name to reflect over here because right\nnow it says root at the rate at Oracle virtualbox the host name is does not look updated as\nyet and if you want it to be updated to k Master, then you have to first of all restart\nthis VM or your system. If you're doing it on a system, then you have to restart your\nsystem. And if you do it on a VM, you have to restart your VM. Okay, so let me restart\nmy VM in some time. But before that there are a few more commands, which I want to run\nand that is set a static IP address. Okay, so I'm going to copy this if conflict I'm\ngoing to run this config command Okay. So right now my IP address is one ninety two\ndot one sixty eight dot 56.1 not one and the next time when I turn on this machine, I do\nnot want a different IP address. So to set this as a static IP address. I have a couple\nof commands. Let me execute that command first. So you can see this interface is file. Right?\nSo under SC \/ Network, we have a file called interfaces. So this is where you define all\nyour network interfaces. Now, let me enter this file and add the rules to make it static\nIP address as you can see here. The last three lines are the ones which ensure that this\nmachine will have a static IP address. These three lines are already there on my machine.\nNow if you want to set a static IP address of your and then make sure that you have these\nthings defined correctly. Okay. My IP address is not one not one. So I would just read in\nit like this. So let me just exit. So the next thing that I have to do is go to the\nhosts file and update my IP address over there. Okay, so I'm going to copy this and go to\nmy Etsy \/ hosts files now over here. You can see that there is no entry. So after mention\nthat this is Mike a master. So let me specify my IP address first. This is my IP address\nand now we have to update the name of the host. So this host of - Kay Master so I'm\njust going to enter that and save this. Okay. Now the thing that we have to do now is restart\nthis machine. So let me just reset this machine and get back to you in the meanwhile. Okay.\nSo now that we are back on let me check if my host name and hosts have all been updated.\nYes. There you go. You can see here, right it recorded k Master. So this means that my\nhost name has been successfully updated we can also verify my IP address is the same\nlet me do an if config and as you can see my appearance has not changed. All right,\nso this is good. Now. This is what we wanted. Now. Let's continue with our installation\nprocess. Let me clear the screen and go back to the notepad and execute those commands\nwhich first of all install my openssh server. So this is going to be the command to do that\nand we have to execute this as pseudo user. Right so sudo apt-get install openssh server.\nThat's the command. Okay, let me say yes and enter. Okay. So my SSH server would have been\ninstalled by now that makes clear the screen and install Docker. But before I run this\ncommand which installs Dhaka and it will update my repository. Okay, so let me log in as pseudo\nfirst fault. Okay, so do is use the command and okay I have logged in as root user. Now.\nThe next thing is update my repository so after do an update update. Now again, this\nis going to take some more time. So just hold on till then. Okay, this is also done. Now\nwe can straight away run the command to install Docker. Now. This is the command to install\nDocker. Okay from the aggregate repository. I'm installing Docker and this specifying\n- why because - why is my flag? So whenever there's a problem that comes in while installation\nsaying do you want to install it? Yes or no, then when you specify - why then it means\nthat by default it will accept why as a parameter. Okay, so that is the only constant behind\n- why so again inserting Dockers going to take a few more minutes. Just hang on till\nthen. Okay, great. So Docker is also installed. Okay. So let me go back to the notepad. So\nto establish the Kubernetes environment the three main components that Kubernetes is made\nup of RQ barium cubelet and Cube cereal, but just before I install these three components\nthere are a few things I have to do they are like installing curl and then downloading\ncertain packages from this URL and then running an update. Okay. So let me execute these commands\none after the other first and then install Kubernetes. So let's first of all start with\nthis command where I'm installing curl. Okay. Now the next command is basically downloading\nthese packages using curl and curl is basically this tool using which you can download these\npackages using your command line. Okay. So this is basically a web URL right so I can\naccess whatever packages are there on this web URL and download them using curl. So that's\nwhy I've installed car in the first place. So when executing this command I get this\nwhich is perfect now when I go back then there is this which we have to execute. Okay, let\nme hit enter and I'm done and finally I have to update my app get repository and common\nfor that. Is this one apt-get update? Okay, great. So all the presentation steps are also\ndone. Now. I can say to me set up my Kubernetes environment by executing this command. So\nin the same command I say install cubelet you barium and Cube CDL and to just avoid\nthe yes prompt am specifying the - wife lat. Okay, which would by default take yes as a\nparameter. And of course I'm taking it from the aggregate repository, right? So, let me\njust copy this and paste it here. Give it a few more minutes guys because in Sony kubernetes\nis going to take some time. Okay bingo. So my humanities has also been installed successfully.\nOkay. Let me conclude the setting up of this cube root is environment by updating the communities\nconfiguration. Okay. So there's this file. You're right Q beta m dot f so, this is the\ncube ADM is the one that's going to let me administer my Kubernetes. So after go to this\nfile and add this one line, okay, so let me first of all open up this file using my Nano\neditor. So let me again log in as soda OSU and this is the command. So as you can see\nwe have these set of environment variables. So right after the last environment variable\nhave to add this one line and that line is this one All right. Now, let me just save\nthis and exit brilliant. So with that the components which have to be installed at both\nthe master and the slave come to an end. Now. What I will do next is run certain commands\nonly at the master to bring up the cluster and then run this one command at all my slaves\nto join the cluster. Alright. So before I start doing anything more over here, let me\nalso tell you that I have already done the same steps on my node. So if you are doing\nit at your end, then whatever steps you've done so far run the same set of commands on\nanother VM because that will be acting as your node v m but in my case, I have already\ndone that just to save some time, you know, so let me show you that this is Mike a master\nof and right here. I have my K node, which is nothing but my communities node and I've\nbasically run the same set of commands in both the places, but there is one thing which\nI have to ensure before I bring up the cluster and that is and short the network IP addresses\nand the host name and the hosts. So this is my communities node, so All I'm going to do\nwhat chat and say \/hc posts. Okay. Now over here. I have the IP address of my Cube ladies\nnode. That is this very machine and a specify the name of the host. However, the name of\nmy Kubernetes Master host is not present and neither is the IP address. So that is one\nmanual entry we have to do if you remember let me go to my master on check. What is the\nIP address? Yes. So the IP address over here is one ninety two dot one sixty eight dot\n56.1 not one. So this is the IP address. I have to add in my node end. So after modify\nthis file for that, all right, but before that you have to also ensure that this is\na static IP address. So let me ensure that the IP address of my cluster node does not\nchange. So the first thing we have to do before anything is check. What is the current IP\naddress and for my node the IP addresses one? Ninety two dot one sixty eight dot 56.1 not\nto okay now, let me run this command. Network interfaces. Okay. So as you can see here,\nthis is already set to be a static IP address. We have to ensure that these same lines are\nthere in your machine if you wanted to be a static IP address since it's already there\nfor me. I'm not going to make any change but rather I'm going to go and check. What's my\nhost name? I mean the whole same should anyways give the same thing because right now it's\nkeynote. So that's what it's gonna reflect. But anyways, let me just show it to you. Okay,\nso my host name is keynote brilliant. So this means that that is one thing which I have\nto change and that is nothing but adding the particular entry for my master. So let me\nfirst clear the screen and then using my Nano editor. In fact, I'll have to run it as pseudo.\nSo as a pseudo user I'm going to open my Nano editor and edit my hosts file. Okay, so here\nlet me just add the IP address of my master. So what exactly is the IP address of the master?\nYes, this is my k Master. So I'm just going to copy this IP address come back here and\npaste the IP address and I'm gonna say the name of that particular host is came master.\nAnd now let me save this perfect. Now, what I have to do now is go back to my master and\nensure that the hosts file here has raised about my slave. I'll clear the screen and\nfirst I'll open up my hosts file. So on my masters and the only entry is there for the\nmaster. So I have to write another line where that specify the IP address or my slave and\nthen add the name of that particular host. That is K node. And again, let me use the\nNano editor for this purpose. So I'm going to say sudo Nano \/hc posts. Okay, so I'm going\nto come here say one ninety two dot one sixty eight dot 56.1 not to and then say Okay node.\nAll right. Now all the entries are perfect. I'm going to save this and Exit so the hosts\nfile on both my master and my slave has been updated the static IP address for both my\nmaster and the slave has been updated and also the kubernetes environment has been established.\nOkay. Now before we go further and bring up the cluster, let me do a restart because I've\nupdated my hosts file. Okay. So let me restart both of my master and my slave VMS and if\nyou're doing it at your and then you have to do the very same, okay, so let's say restart\nand similarly. Let me go to my load here and do a restart. Okay, so I've just logged in\nand now that my systems are restarted. I can go ahead and execute the commands at only\nthe Masters and to bring up the cluster. Okay. So first of all, let me go through the steps\nwhich are needed to be run on the Masters end. So add the master of first of all, we\nhave to run a couple of commands to initiate the Kubernetes cluster and then we have to\ninstall a pod Network. We have to install a pod Network because all my containers inside\na single port will have to communicate over a network Port is nothing but a network of\ncontainers. So there are various container networks, which I can use so I can use the\nCalico poor Network. I can use a flannel poor Network or I can use anyone you can see the\nentire list in the communities documentation. And in this session, I am going to use the\ncalcio network. Okay, so that's pretty simple and straightforward and that's what I'm going\nto show you next. So once you've set up the Pod Network, you can straight away bring up\nthe communities dashboard and remember that you have to set up the communities dashboard\nand bring this up before your notes join the cluster because in this version of Cuba Nettie's\nif you first get your notes to join the cluster and after that if you try bringing the kubernetes\ndashboard up then your communities dashboard gets hosted on the And you don't want that\nto happen, right? If you want the dashboard to come up at your Masters and you have to\nbring up the dashboard before your nodes join the cluster. So these would be the three commands\nthat we will have to run initiating the cluster of inserting the poor Network and then setting\nup the Kubernetes dashboard. So let me go to my master and execute commands for each\nof these processes. So I suppose this is my master. And yes, this is my k Master. So so\nfirst of all to bring up the cluster we have to execute this command. Let me copy this\nand over here. We have to replace the IP addresses. So the IP address of my master, right? So\nthis machine after specified that IP address over here because this is where the other\nIP addresses can come and join This is the master right? So I'm just seeing a pi server\nadvertise the address 56.1 not one so that all the other nodes can come and join the\ncluster on this IP address and along with this. I have to also specify the port Network\nsince I've chosen the Calico poor Network. There is a network range which my Calico poor\nNetwork uses so a cni basically stands for container network interface. If I'm using\nthe Calico poor Network then after use this network range, but in case of few want to\nuse a flannel poor Network, then you can use this network range. Okay, so let me just copy\nthis one and paste it. All right. So the command is pseudo Cube ADM in it for Network followed\nby the IP address from where the other nodes will have to join. So let's go ahead and enter\nSo since you're doing for the first time give it a few minutes because kubernetes take some\ntime to install. Just hold on until that happens. All right. Okay, great. Now it says that your\nkubernetes master has initialized successfully that's good news. And it also says that to\nstart using your cluster. We need to run the following commands as a regular user. Okay,\nso we'll note that log out as a pseudo user and as a regular user executes these three\ncommands and also if I have to deploy a poor Network then after run a command, okay. So\nthis is that command which I have to run to bring up my poor Network. So I'll be basically\ncloning the yamen file which is present over here. So before I get to all these things\nlet me show you that we have a cube joint command which is generated. Right? So this\nis generated in my masters and and I have to execute this command at my node to join\nthe cluster, but that would be the last step because like I said earlier these three commands\nwill have to be first executed then after bring up my poor Network then after bring\nup my dashboard and then I have to get my notes to join the class are using this command.\nSo for my reference, I'm just going to copy this command and store it somewhere else.\nOkay. So right under this Let me just do this command for later reference. And in the meanwhile,\nlet me go ahead and execute all these commands one after the other. These are as per Cube\nentities instructions, right? Yes. I would like to rewrite it. And then okay. Now that\nI've done with this let me first of all bring up my pod Network. Okay. Now the command to\nbring up my pod network is this Perfect. So my calcio pod has been created now I can verify\nif my poor has been created by running the cube CDL get pods command. Okay. So this is\nmy Cube serial get pods. I can say - oh wide all namespaces. Okay by specifying the - oh\nwide and all namespaces. I'll basically get all the pods ever deployed. Even the default\npose with get deployed when the Kubernetes cluster initiates. So basically the kubernetes\ncluster is initiated and deployed along with a few default ones especially for your poor\nNetwork. There is one part which is hosted for your cluster. There's one pod For Your\nRocker board itself, and then there's one pot which is deployed for your dashboard and\nwhatnot. So this is the entire list, right? So if you're calcio for your SED, there's\none pod for your Cube controller. There's a pot and we have various spots like this\nright for your master and you're a pi server and many things. So these are the default\ndeployments that you get So anyways, as you can see the default deployments are all healthy\nbecause it says the status is all running and everything is basically you're running\nin the cube system namespace. All right, and it's all running on my k Master That's Mike\nunit is master. So the next thing that I have to do is bring up the dashboard before I can\nget my notes to join. Okay, so I'll go to the notepad and copy the command to bring\nup my dashboard. So copy and paste so great. This is my communities dashboard, which as\nyou know, basically this part has come up now. If I execute this same Cube serial, get\npods command, then you can see that I've got one more pot which is deployed for my dashboard\nbasically. So last time this was not there because I had not deployed my dashboard at\nthat time, right? So I don't need to plug my iPod Network and whatnot and the other\nthings right? So I've deployed it and the continuous creating so in probably a few more\nseconds, this would also be running anyways in the meanwhile, what we can do is we can\nwork on the other things which are needed to bring up the dashboard the first fall.\nAbel your proxy and get it to be hope for web server. There's a skip serial proxy command\nOkay. So with this your service would be starting to be served on this particular port number.\nOkay, localhost port number eight thousand one of my master. Okay, not from the nodes.\nSo if I could just go to my Firefox and go to local Lowe's 8001 then my dad would be\nup and running over there. So basically my dashboard is being served on this particular\nport number. But if I want to actually get my dashboard which shows my deployments and\non my services then that's a different URL. Okay. So yeah as you can see here. Localized\n8,000 \/ API slash V 1 right this entire URL is which is going to lead me to my dashboard.\nBut at this point of time I cannot log into my dashboard because it's prompting me for\na token and I do not have a token because I have not done any cluster old binding and\nI have not mentioned that I am the admin of this particular dashboard. So to enable all\nthose things there are a few more commands that we have to execute starting with creating\na service account for your dashboard. So this is the command to create your service account.\nSo go back to the terminal and probably a new terminal window execute this command Okay.\nSo with this you're creating a service account for your dashboard, and after that you have\nto do the cluster roll binding for your newly created service account. Okay. So the dashboard\nhas been created and default namespace as per this. Okay, and here I'm saying that my\ndashboard is going to be for admin and I'm doing the cross the road binding. Okay, and\nnow that this is created I can straight away get the token because if you remember it's\nasking me for a token to login, right? So even though I am the admin now have a not\nbe able to log in without D token, so to generate the token I have to again run this command\nCube City will get secret key. Okay, so I'm going to copy this and paste it here. So this\nis the token or this is the key that basically needs to be used. So let me copy this entire\ntoken and paste it over here. So let me just save this and yeah, now you can see that my\ncommunity's cluster has been set up and I can see the same thing from the dashboard\nover here. So basically by default the communities service is deployed. Right? So this is what\nyou can see but I've just brought the dashboard now and the cluster is not ready under my\nnodes join in. So let's go to the final part of this demonstration. We're in I'll ask my\nslaves to join the cluster. So you remember I copied the joint cluster which was generated\nat my Master's end in my notepad. So I'm going to copy that and execute that at the slaves\nand to join the cluster. Okay. So let me first of all go to my notepad and yeah, this is\nthe joint command which I had copyright. So I'm going to copy this and now I'm going to\ngo to my node. Yep. So, let me just paste this and let's see what happens. Let me just\nrun this command as pseudo. It's a perfect. I've got the message that I have successfully\nestablished connection with the API server on this particular IP address and port number,\nright? So this means that my node has joined the cluster we can verify that from the dashboard\nitself. So if I go back to my dashboard, which is hosted on my master master Zen, so I have\nan option here as nodes. If I click on this then I will get the details about my nodes\nover here. So earlier I only have the keymaster but now I have both the key master and the\nK node give it a few more seconds until my note comes up. I can also verify the same\nfrom my terminal. So if I go to my terminal here and if I run the command Cube CTL get\nnodes then if we give me the details about the nodes which are there in my cluster soak\na master is one that is already there in the cluster but cannot however will take some\nmore time to join my cluster. Alright, so that's it guys. So that is about my deployment\nand that's how you deploy a community's cluster. So from here on you can do whatever deployment\nyou want. Whatever you want to deploy you can deploy it. Easily very effectively either\nfrom the dashboard or from the CLI and there are various other video tutorials of ours,\nwhich you can refer to to see how a deployment is made on Kubernetes. So I would request\nyou to go to the other videos and see how deployment is made and I would like to conclude\nthis video on that note. If you're a devops guy, then you would have definitely heard\nof communities but I don't think the devops world knows enough of what exactly kubernetes\nis and where it's used. And that's why we had Erica of come up with this video on what\nis communities. My name is Walden and I'll be representing a t\u00e1rrega in this video.\nAnd as you can see from the screen, these will be the topics that we'll be covering\nin today's session as first start off by talking about what is the need for communities? And\nafter that I will talk about what exactly it is and what it's not and I will do this\nbecause there are a lot of myths surrounding communities and there's a lot of confusion\npeople have misunderstood communities to be a containerization platform. Well, it's not\nokay. So I will explain what exactly it is over here. And then after that I will talk\nabout how exactly communities works. I will talk about the architecture and all the related\nthings. And after that I will give you a use case. I will tell you how communities was\nused at Pokemon go and how it helped Pokemon go become one of the best games of the year\n2017 And finally at the end of the video, you will get a demonstration of how to do\ndeployment with Kubernetes. Okay. So I think the agenda is pretty clear you I think we\ncan get started with our first topic then now first topic is all about. Why do we need\nKubernetes? Okay now to understand why do we need Cuba Nettie's let's understand what\nare the benefits and drawbacks of containers. Now, first of all containers are good. They\nare amazingly good right any container for that matter of fact a Linux container or a\nDocker container or even a rocket Continuum, right? They all do one thing they package\nyour application and isolated from everything else, right? They isolate the application\nfrom the host mainly and this makes the container of fast reliable efficient light weight and\nscalable now hold the thought yes containers are scalable, but then there's a problem that\ncomes with that and this is what is the resultant of the need for Kubernetes even though continues\nare scalable. They are not very easily scalable. Okay, so let's look at it this way. You have\none container you might want to probably scale it up to to contain over three containers.\nWill it's possible right? It's going to take a little bit of manual effort. But yeah, you\ncan scale it up. You know what I have a problem. But then look at a real world scenario where\nyou might want to scale up to like 5200 containers then in that case what happens I mean after\nscaling up, would you do you have to manage those containers? Right? We have to make sure\nthat they are all working. They are all active and they're all talking to each other because\nif they're not talking to each other then there's no point of scaling up itself because\nin that case the server's would not be able to handle the roads if they're not able to\ntalk to each other correct. So it's really important that they are manageable when they\nare scaled up and now let's talk about this point. Is it really tough to scale up containers?\nWell the answer for that might be know. It might not be tough. It's pretty easy to scale\nup containers, but the problem is what happens after that. Okay, once you scale up containers,\nyou will have a lot of problems. Like I told you the containers first for should have to\ncommunicate with each other because Not so many in number and they work together to basically\nhost the service right the application and if they are not working together and talking\ntogether then the application is not hosted and scaling up is a waste so that's the number\none reason and the next is that the containers have to be deployed appropriately and they\nhave to also be managed they have to be deployed appropriately because you cannot have the\ncontainers deployed in this random places. You have to deploy them in the right places.\nYou cannot have one container in one particular cloud and the other one somewhere else. So\nthat would have a lot of complications. Well, of course it's possible. But yeah, it would\nlead to a lot of complications internally you want to avoid all that so you have to\nhave one place where everything is deployed appropriately and you have to make sure that\nthe IP addresses are set everywhere and the port numbers are open for the containers to\ntalk to each other and all these things. Right. So these are the two other points the next\nPoint our the next problem with scaling up is that auto scaling is never a functionality\nover here? Okay, and this is one of the things which is the biggest benefit with Cuba Nets.\nThe problem technically is there is no Auto scaling functionality. Okay, there's no concept\nof that at all. And you may ask at this point of time. Why do we even need auto-scaling?\nOkay, so let me explain the need for auto scaling with an example. So let's say that\nyou are an e-commerce portal. Okay, something like an Amazon or a flip card and let's say\nthat you have decent amount of traffic on the weekdays, but on the weekends, you have\na spike in traffic. Probably you have like 4X or 5x the usual traffic and in that case\nwhat happens is maybe your servers are good enough to handle the requests coming in on\nweekdays, right? But the requests that come on the weekends right from the increased traffic\nthat cannot be serviced by our servers right? Maybe it's too much for your servers to handle\nthe load and maybe in the short term. It's fine maybe once or twice you can survive but\nthey will definitely come a time when your server will start crashing because it cannot\nhandle that many requests per second or permanent. And if you want to really avoid this problem\nwhat you do you have to scale up and now would you Lead keep scaling up every weekend and\nscaling down after the weekend, right? I mean technically is it possible? Will you be buying\nyour servers and then setting it up and every Friday would you be again by new Star Wars\nsetting up your infrastructure? And then the moment your weekday starts. Would you just\ndestroy all your servers? Whatever you build. Would that would you be doing? No, right?\nObviously, that's a pretty tedious task. So that's where something like Cuban Aires comes\nin and what communities does is it keeps analyzing your traffic and the load that's being used\nby the container and as and when the traffic is are reaching the threshold auto-scaling\nhappens where if the server's have a lot of traffic and if it needs no more such servers\nfor handling requests, then it starts killing of the containers on its own. There is no\nmanual intervention needed at all. So that's one benefit with Kubernetes and one traditional\nproblem that we have with scaling up of containers. Okay, and then yeah, the one last problem\nthat we have is the distribution of traffic that is still challenging without something\nthat can manage your containers. I mean you have so many containers, but how will the\ntraffic be distributed? Load balancing. How does that happen? You just have containers\nright? You have 50 containers. How does the load balancing happen? So all these are questions.\nWe should really consider because containerization is all good and cool. It was much better than\nVMS. Yes containerization. It was basically a concept which was sold on the basis of for\nscaling up. Right? We said that vm's cannot be scaled up easily. So we told use containers\nand with containers you can easily scale up. So that was the whole reason we basically\nsold containers with the tagline of scaling up. But in today's world, our demand is ever\nmore that even the regular containers cannot be enough so scaling up a so much or and so\ndetailed that we need something else to manage your containers, correct. Do we agree that\nwe need something right? And that is exactly what Cuban Aries is. So Kubernetes is a container\nmanagement tool. All right. So this is open source and this basically automate your container\ndeployment your continue scaling and descaling and your continual load balancing the benefit\nwith this is that it works brilliantly with all the cloud vendors with all A big cloud\nvendors or your hybrid Cloud vendors and it also works on from Isis. So that is one big\nselling point of kubernetes. Right? And if I should give more information about communities\nthen let me tell you that this was a Google developed product. Okay. It's basically a\nbrainchild of Google and that pretty much is the end of the story for every other competitor\nout there because the community that Google brings in along with it is going to be huge\nor basically the Head Start that communities would get because of being a Google brain\nchild is humongous. And that is one of the reasons why kubernetes is one of the best\ncontainer management tools in the market period and given that communities is a Google product.\nThey have written the whole product on go language. And of course now Google has contributed\nthis whole communities project to the CN CF which is nothing but the cloud native Computing\nFoundation or simply Cloud native Foundation, right? You can just call them either that\nand they have donated their open source project to them. And if I have to just summarize what\nHumanities is you can just think of it like this it can group like a number. Containers\ninto one logical unit for managing and deploying an application or a particular service. So\nthat's a very simple definition of what communities is. It can be easily used for deploying your\napplication. Of course. It's going to be Docker containers which you will be deploying. But\nsince you will be using a lot of Docker containers as part of your production, you will also\nhave to use Kubernetes which will be managing your multiple Docker containers, right? So\nthis is the role it plays in terms of deployment and scaling upskilling down is primarily the\ngame of communities from your existing architecture. It can scale up to any number you want. It\ncan scale down anytime and the best part is the scaling can also be set to be automatic.\nLike I just explained some time back right you can make communities communities would\nanalyze the traffic and then figure out if the scaling up needs to be done or the Skilling\nnoun can be done and all those things. And of course the most important part load balancing,\nright? I mean what good is your container or group of containers if load balancing cannot\nbe enabled right? So communities does that also and these Some of the points on based\non which kubernetes is built. So I'm pretty sure you have got a good understanding of\nwhat communities is by now Write a brief idea at least so moving forward. Let's look at\nthe features of Kubernetes Okay. So we've seen what exactly kubernetes is how would\nusers Docker containers or other connector or containers in general? But now let's see\nsome of the selling points of humanities or why it's a must for you. Let's start off with\nautomatic bin packing when we say automatic bin packing. It's basically that communities\npackages your application and it automatically places containers based on their requirements\nand the resources that are available. So that's the number one advantage the second thing\nservice Discovery and load balancing. There is no need to worry. I mean if you know, if\nyou're if you're going to use Kubernetes then you don't have to worry about networking and\ncommunication because communities will just automatically assign containers their own\nIP addresses and probably a single DNS name for a set of containers which are performing\na logical operation. And of course, there will be loads. Dancing across them so you\ndon't have to worry about all these things. So that's why we say that there is service\nDiscovery and load balancing with kubernetes and the third feature of kubernetes. Is that\nstorage orchestration with communities, you can automatically Mount your storage system\nof your choice. You can choose that to be either a local storage or maybe on a public\nCloud providers such as a gcp or AWS or even a network storage system such as NFS or other\nthings, right? So that was the feature number three now, please remember for self-healing\nnow, this is one of my favorite parts of Humanity's actually not just communities even with respect\nto dr. Swamp. I really like this part of self-healing what self feeling is all about is that whenever\nkubernetes realizes that one of your containers has failed then it will restart that container\non its own right and we create a new container in place of this crashed one and in case you're\nnode itself fails, then what you bilities would do in that case has whatever containers\nwere running in that failed node. Those containers would be started in another node, right? Of\ncourse, you would have to have more In that cluster if there's another node in the cluster\ndefinitely room would be made for this field container to start a service. So that happens\nso the next feature is batch execution. So when we say batch execution, it's that along\nwith Services Humanities can also manage your batch and CIA work loads, which is more of\na devops roll. Right? So as part of your CIA workloads communities can replace your containers\nwhich fail and it can restart and restore the original state that is what is possible\nwith kubernetes and secret and configuration management. That is another big feature with\nkubernetes. And that is the concept of where you can deploy and update your secrets and\napplication configuration without having to rebuild your entire image and without having\nto expose your secrets in your stack configuration or anything, right? So if you want to deploy\nan update your secrets only that can be done. So it's not available with all the other tools,\nright? So communities is one that does that you don't have to restart everything and rebuild\nyour entire container. That's one benefit and then we have Horizonte scaling which of\ncourse you will My that of already you can scale your applications up and down easily\nwith a simple command. The simple command can be run on the CLI or you can easily do\nit on your GUI, which is your dashboard. Your community is dashboard or Auto scaling is\npossible Right based on the CPU usage. Your containers would automatically be scaled up\nor scaled down. So that's one more feature and the fun feature that we have is automatic\nrollbacks and roll outs now Kubernetes what it does is whenever there's an update your\napplication, which you want to release communities progressively rolls out these changes and\nupdates to the application or its complications by this ensuring that one instance after the\nother is send these updates and it makes sure that not all instances are updated at the\nsame time thus ensuring that yes, there is high availability. And even if something goes\nwrong, then the Cuban ladies will roll back that change for you. So all these things are\nenabled and these are the features with Humanities. So if you're really considering a solution\nfor your containers from managing your containers, then communities should be your solution.\nTo that should be your answer. So that is about the various features of Kubernetes now\nmoving forward here. Let's talk about a few of the myths surrounding communities and we\nare doing this because a lot of people have confusion with respect to what exactly it\nis. So people have this misunderstanding that communities is like docker which is a continuation\nplatform, right? That's what people think and that is not true. So this kind of a confusion\nis what I intend to solve in the upcoming slides. I will not talk about what exactly\nkubernetes is and what communities is not let me start with what it's not now. The first\nthing is that communities is not to be compared with Docker because it's not the right set\nof parameters which are comparing them against Docker is a containerization platform and\na Kubernetes is a container management platform, which means that once you have containerized\nyour application with the help of Docker containers or Linux containers, and when you are scaling\nup these containers to a big number like 50 or a hundred that's where communities would\ncome in when you have like multiple containers which need to be managed. That's where communities\ncan comment and effectively do it. You can specify the configurations and communities\nwould make sure that at all times these conditions are satisfied. So that's what community is\nyou can tell in your configurations that at all time. I want these many containers running.\nI want these many pods running and so many other needs right you can specify much more\nthan that and whatever you do at all times your cluster master or your communities Master\nwould ensure that this condition is satisfied. So that is what exactly Community is, but\nthat does not mean that talker does not solve this purpose. So Docker also have their own\nplug-in. I wouldn't call it a plug-in. It's actually another tool of there's so there's\nsomething called as Docker swamp and Dockers warm does a similar thing it does contain\na management like Mass container management so similar to what communities does when you\nhave like 50 to 100 containers Docker swarm would help you in managing those containers,\nbut if you look at who is prevailing in the market today, I would say it's communities\nbecause communities came in first and the moment they came in they were backed by Google\nThey had this huge Community with they just swept along with them. So they have like hardly\nleft any in any market for Docker and for dr. Stromm, but that does not mean that they\nare better than Docker because they are at the end of the day using Docker. So communities\nis only as good as what Docker is if there are no Docker containers, then there's no\nneed for communities in the first place. So Cuban adiz and Docker they go hand in hand.\nOkay. So that is the point you have to note and I think that would also explain the point\nthat kubernetes is not for continue Rising applications. Right? And the last thing is\nthat Kubernetes is not for applications with a simple architecture. Okay, if your architecture\nreview your applications architecture is pretty complex, then you can probably use Cuban IDs\nto uncomplicate that architecture. Okay, but if you're having a very simple one in the\nfirst place then using kubernetes would not serve you any good and it could probably make\nit a little more complicated than what it already is, right. So this is what kubernetes\nis not now speaking of what exactly kubernetes is. The first point is Kubernetes is robust.\nAnd reliable now when I see a robust and reliable, I'm referring to the fact that the cluster\nthat is created the communities cluster, right? This is very strong. It's very rigid and it's\nnot going to be broken easily. The reason being the configurations which is specified\nright at any point of time if any container fails a new container would come up right\nor that whole container would be restarted. One of the things will definitely happen.\nIf your node fails then the containers which are running in a particular node. They would\nstart running in a different node, right? So that's why it's reliable and it's strong\nbecause at any point of time your cluster would be at full force. And at any time if\nit's not happening, then you would be able to see that something's wrong and you have\nto troubleshoot your node and then everything would be fine. So Cuban, it's would do everything\npossible and it pretty much does everything possible to let us know that the problem is\nnot at its end and it's giving the exact result that we want. That's what communities are\ndoing. And the next thing is that Humanity's actually is the best solution for scaling\nup containers at least in today's. I could because the two biggest players in this market\nare radhika swamp and Humanities and Docker swarm is not really the better one here because\nthey came in a little late even though doctor was there from the beginning communities came\nafter that but doc a swarm which we are talking about came in somewhere around 2016 or 2017.\nRight? But communities came somewhere around 2015 and they had a very good Head Start.\nThey were the first ones to do this and they're backing by Google is just icing on the cake\nbecause whatever problem you have with respect to Containers, if you just go up and if you\nput your error there then you will have a lot of people on github.com and get up queries\nand then on stack overflow will be resolving those errors, right? So that's the kind of\nMarket they have so it's back be a really huge Community. That's what kubernetes is\nand to conclude this slide Humanities is a container orchestration platform and nothing\nelse. All right. So I think these two slides would have given you more information and\nmore clarity with respect to what kubernetes is. And how different it is from docker and\ndocker swamp, right? So now moving on let's go to the next topic where we will compare\nHumanities with DACA swamp and we are comparing with Docker swamp because we cannot compare\nDocker and Kubernetes head on. Okay, so that is what you have to understand if you are\nthis person over here if you are Sam who is wondering which is the right comparison then\nlet me reassure you that the difference can only be between Humanities and doctors Mom.\nOkay. So let's go ahead and see what the difference is. Actually. Let's start off with your installation\nand configuration. Okay. So that's the first parameter will use to compare these two and\nover here doc a swarm comes out on top because Dockers little easier you have around two\nor three commands which will help you have your cluster up and running that includes\nthe node joining the cluster, right? But with kubernetes it's way more complicated than\ntalking swamp, right? So you have like close to ten to eleven commands, which you have\nto execute and then there's a certain pattern you have to follow to ensure that there are\nno errors, right? Yes, and that's why I'm consuming and that's why it's complicated.\nBut once your cluster is ready that time kubernetes is the winner because the flexibility the\nrigidness and the robustness that communities gives you cannot be offered by dr. Swamp.\nYes, dr. Storm is faster, but yes not as good as communities when it comes to your actual\nworking and speaking of the GUI. Once you have set up your cluster or you can use a\nGOI with communities for deploying your applications. Right? So you don't need to always use your\nCLI. You have a dashboard which comes up and the dashboard. If you give it admin privileges,\nthen you can use it. You can deploy your application from the dashboard itself everything just\ndrag-and-drop click functionality right with just click functionality. You can do that.\nThe same is not the case with Docker swarm. You have no GUI in Dhaka swamp Okay. So doc\nIslam is not the winner over here. It's Kubernetes and he is going to the third parameter scalability.\nSo people again have a bad misconception that communities is better it is the solution for\nscaling up. And it is better and faster than dr. Stromm. Well, it could be better but yes,\nit's not faster than doctors warm. Even if you want to scale up right? There is a report\nwhere I recently read that the scaling up in Dhaka swarm is almost five times faster\nthan the scaling up with Kubernetes. So that is the difference. But yes, once you are scaling\nup is done after that your cluster strength with kubernetes is going to be much stronger\nthan your doctor swamp plus the strength. That's again because of the various configurations.\nThat should have been specified by then. That is the thing now moving on to the next parameter\nwe have is load balancing requires manual service configuration. Okay. This is in case\nof kubernetes and yes, this could be shortfall. But with dr. Storm there is inbuilt load balancing\ntechniques, which you don't need to worry about. Okay, even the load balancing which\nrequires manual effort in case of communities is not do much there are times when you have\nto manually specify what are your configuration you have to make a few changes but yes, it's\nnot as much as what you thinking and speaking of updates and rollbacks. What communities\ndoes is it does the Scheduling to maintain the services while updating. Okay. Yeah, that's\nvery similar to how it works of darkness form wherein you have like Progressive updates\nand service Health monitoring happens throughout the update, but the difference is when something\ngoes wrong Humanity's goes that extra mile of doing a roll back and putting you back\nto the previous state right before the update was launched. So that is the thing with kubernetes\nand the next parameter. We are comparing those two upon is data volumes. So data volumes\nin Cuba nattie's can be shared with other containers, but only within the same pod,\nso we have a concept called pods in communities. Okay, now board is nothing but something which\ngroups related containers right a logical grouping of containers together. So that is\na pot and whichever containers are there inside this pod. They can have a shared volume. Okay,\nlike storage volume, but in case of doctors from you don't have the concept of poor at\nall. So the shared volumes can be between any other container. There is no restriction\nwith respect to that and dr. Swann and then finally we have this All the logging and monitoring.\nSo when it comes to logging and monitoring Humanities provides inbuilt tools for this\npurpose. Okay, but with dr. Storm you have to install third-party tools if you want to\ndo logging and monitoring so that is the fall backward. Dr. Swann because logging is really\nimportant one because you will know what the problem is. You'll know which card in a failed\nwhat happened there is exactly the error, right? So logs would help you give that answer\nand monitoring is important because you have to always keep a check on your nodes, right?\nSo as the master of the cluster it's very important that there's monitoring and that's\nwhere our communities has a slight advantage over doc a swarm. Okay, but before I finish\nthis topic there is this one slide. I want to show you which is about the statistics.\nSo this stat I picked it up from this Platform 9, which is nothing but a company that writes\nabout tech. Okay and what they've said is that the number of news articles there were\nproduced right in that one particular year had 90% of those covered on Kubernetes compared\nto the 10 percent. It on Docker swamp amazing, right? That's a big difference. That means\nfor every one blog written or for everyone article written on Docker swamp. There are\nnine different articles written on humanities and similarly for web searches for web searches\nkubernetes is 90 percent compared to Dhaka swarms 10% and Publications GitHub Stars.\nThe number of commits on GitHub. All these things are clearly one vacuum energy is everywhere.\nSo communities is the one that's dominating this market and that's pretty visible from\nthis stat also, right? So I think that pretty much brings an end to this particular topic\nnow moving forward. Let me show you a use case. Let me talk about how this game this\namazing game called Pokemon go was powered with the help of communities. I'm pretty sure\nyou all know what it is, right? You guys know Pokemon go. It's the very famous game and\nit was actually the best game of the year 2017 and the main reason for that being the\nbest is because of kubernetes and let me tell you why but before I tell you why there are\nfew things, which I want to just talk about I'll give you an overview of Pokemon goers\nand let me talk about a few key Stacks. So Pokemon go is an augmented reality game developed\nby Niantic for your Android and for iOS devices. Okay, and those key stats read that they've\nhad like 500 million plus downloads overall and 20 million plus daily active users. Now\nthat is massive daily. If you're having like 20 million users plus then you have achieved\nan amazing thing. So that's how good this game is. Okay, and then this game was actually\ninitially launched only in North America Australia New Zealand, and I'm aware of this fact because\nI'm based out of India and I did not get access to this game because the moment news got out\nthat we have a game like this. I started downloading it, but I couldn't really find any link or\nI couldn't download it at all. So they launched it only in these countries, but what they\nfaced right in spite of just reading it in these three countries. They had like a major\nproblem and that problem is what I'm going to talk about in the next slide, right? So\nmy use case is based on that very fact that In spite of launching it only in these three\ncountries or in probably North America and then in Australia New Zealand, they could\nhave had a meltdown but rather with the help of Humanity's they used that same problem\nas the basis for their raw success. So that's what happened. Now let that be a suspense\nand before I get to that let me just finish this slide one amazing thing about Pokemon\ngo is that it has inspired users to walk over 5.4 billion miles an hour. Okay. Yes do the\nmath five point four billion miles in one year. That's again a very big number and it\nsays that it has surpassed engineering Expectations by 50 times. Now this last sign is not with\nrespect to the Pokemon Go the game but it is with respect to the backend and the use\nof Kubernetes to achieve whatever was needed. Okay, so I think I've spent enough time over\nhere. Let me go ahead and talk about the most interesting part and tell you how the back\nin architecture of Pokemon go was okay. So you have a Pok\u00e9mon go container, which had\ntwo primary components one is your Google big table, which is your main. Database where\neverything is going in and coming out and then you have your programs which is a run\non your java Cloud, right? So these two things are what is running your game mapreduce and\nCloud dataflow wear something it was used for scaling up. Okay, so it's not just the\ncontainer scaling up but it's with respect to the application how the program would react\nwhen there are these increased number of users and how to handle increased number of requests.\nSo that's where the mapper uses. The Paradigm comes in right the mapping and then reducing\nthat whole concept. So this was their one deployment. Okay, and when we say in defy,\nit means that they had this over capacities which could go up til five times. Okay. So\ntechnically they could only serve X number of requests but in case of failure conditions\nor heavy traffic load conditions, the max the server could handle was 5x because after\n5x the server would start crashing that was their prediction. Okay, and what actually\nhappened at Pokemon go on releasing in just those three different geographies. Is that\nthe Deployed it the usage became so much that it was not XM R of X, which is technically\nthey're a failure limit and it is not even 5 x which is the server's capability but the\ntraffic that they got was up to 50 times 50 times more than what they expected. So, you\nknow that when your traffic is so much then you're going to be brought down to your knees.\nThat's a definite and that's a given right. This is like a success story and this is too\ngood to be true kind of a story and in that kind of a scenario if the request start coming\nin are so much that if they reach 50 x then it's gone, right the application is gone for\na toss. So that's where kubernetes comes in and they overcome all the challenges. How\ndid you overcome the challenges because Cuban areas can do both vertical scaling and horizontal\nscaling at ease and that is the biggest problem right? Because any application and any other\ncompany can easily do horizontal scaling where you just spin up more containers and more\ninstances and you set up the environment but vertical scaling is something which is very\nspecific and this is even more challenging. Now it's more specific to this particular\ngame because the virtual reality would keep changing whenever a person moves around or\nwalks around somewhere in his apartments or somewhere on the road. Then the ram right\nthat would have to increase the memory the in memory and the storage memory all this\nwould increase so in real time your servers capacity also has to increase vertically.\nSo once they have deployed it, it's not just about horizontal scalability anymore. It's\nnot about satisfying more requests. It's about satisfying that same request with respect\nto having more Hardware space more RAM space and all these things right that one particular\nserver should have more performance abilities. That's what it's about and communities solve\nboth of these problems effortlessly and neon tape were also surprised that kubernetes could\ndo it and that was because of the help that they got from Google. I read an article recently\nthat they had a neon thick slab. He met with some of the top Executives in Google and then\ngcp right and then they figure out how things are supposed to go and they of course Met\nwith the Hedgehog communities and they figure out a way to actually scale it up to 50 time\nin a very short time. So that is the challenge that they represented and thanks to communities.\nThey could handle three times the traffic that they expected which is like a very one\nof story and which is very very surprising that you know, something like this would happen.\nSo that is about the use case and that pretty much brings an end to this topic of how Pokemon\ngo used communities to achieve something because in today's world Pokemon go is a really revered\ngame because of what it could write it basically beat all the stereotypes of a game and whatever\nanybody could have anything negative against the game, right? So they could say that these\nmobile games and video games make you lazy. They make you just sit in one place and all\nthese things. Right and Pokemon go was something which was different it actually made people\nwalk around and it made people exercise and that goes on to show how popular this game\nbecame if humanity is lies at the heart of something which became so popular and something\nNow became so big then you should imagine how big the humanities or how beautiful communities\nis, right? So that is about this topic now moving forward. Let me just quickly talk about\nthe architecture of communities. Okay. So the communities architecture is very simple.\nWe have the cube Master which controls a pretty much everything. We should note that it is\nnot a Docker swarm where your Cube Master will also have containers running. Okay, so\nthey won't be containers over here. So all the containers will be running all the services\nwhich will be running will be only on your nodes. It's not going to be on your master\nand you would have to first of all create your rock Master. That's the first step in\ncreating your cluster and then you would have to get your notes to join your cluster. Okay.\nSo bead your pods or beat your containers everything would be running on your nodes\nand your master would only be scheduling or replicating these containers across all these\nnodes and making sure that your configurations are satisfied, right? Whatever you specify\nin the beginning and the way you access your Cube Master is why are two ways You can either\nuse it via the UI or where the CLI. So the CLI is the default way and this is the main\nway technically because if you want to start setting up your cluster you use the CLI, you\nset up your cluster and from here, you can enable the dashboard and when you enable the\ndashboard then you can probably get the GUI and then you can start using your communities\nand start deploying by just with the help of the dashboard right my just the click functionality.\nYou can deploy an application which you want rather than having to write. I am L file or\nfeed commands one after the other from the CLI. So that is the working of Kubernetes.\nOkay. Now, let's concentrate a little more on how things work on the load end. Now as\nsaid before communities Master controls your nodes and inside nodes you have containers.\nOkay, and now these containers are not just contained inside them but they are actually\ncontained inside pods. Okay, so you have nodes inside which there are pots and inside each\nof these pods. They will be a number of containers depending upon Your configuration and your\nrequirement right now these pods which contain a number of containers are a logical binding\nor logical grouping of these containers supposing you have one application X which is running\nin Node 1. Okay. So you will have a part for this particular application and all the containers\nwhich are needed to execute this particular application will be a part of this particular\npart, right? So that's how God works and that's what the difference is with respect to what\nDoc is warm and two bananas because I'm dr. Swamp. You will not have a pot. You just have\ncontinuous running on your node and the other two terminologies which you should know is\nthat of replication controller and service. Your replication controller is the Masters\nresource to ensuring that the request number of pods are always running on the nodes, right?\nSo that's trigger confirmation or an affirmation which says that okay. This many number of\nPODS will always be running and these many number of containers will always be running\nsomething like that. Right? So you see it and the replication controller will always\nensure that's happening and your service is just an object on the master that provides\nload. I don't think of course is replicated group of PODS. Right? So that's how Humanities\nworks and I think this is good enough introduction for you. And I think now I can go to the demo\npart where and I will show you how to deploy applications on your communities by either\nyour CLI, or either via your Jama files or by or dashboard. Okay guys, so let's get started\nand for the demo purpose. I have two VMS with me. Okay. So as you can see, this is my Cube\nMaster which would be acting as my master in my cluster. And then I have another VM\nwhich is my Cube Node 1. Okay. So it's a cluster with one master and one node. All right. Now\nfor the ease of purpose for this video, I have compiled the list of commands in this\ntext document right? So here I have all the commands which are needed to start your cluster\non then the other configurations and all those things. So I'll be using these every copying\nthese commands and then I'll show you side-by-side and I will also explain when I do that as\nto what each of these commands mean now there's one prerequisite that needs to be satisfied.\nAnd that is the master of should have at least two core CPUs. Okay and 4GB of RAM and your\nnode should have at least one course if you and 4GB of ram so just make sure that this\nmuch of Hardware is given to your VMS right if you are using To what a Linux operating\nsystem well and good but if you are using a VM on top of a Windows OS then I would request\nyou to satisfy these things. Okay, these two criterias and I think we can straight away\nstart. Let me open up my terminal first fault. Okay. This is my node. I'm going back to my\nmaster. Okay. Yes. So first of all, if you have to start your cluster, you have to start\nit from your Masters end. Okay, and the command for that is Q barium in it, you specify the\nport Network flag and the API server flag. We are specifying the port Network flag because\nthe different containers inside your pod should be able to talk to each other easily. Right?\nSo that was the whole concept of self discovery, which I spoke about earlier during the features\nof communities. So for this self-discovery, we have like different poor networks using\nwhich the containers would talk to each other and if you go to the documentation the community\nis documentation. You can find a lot of options are you can use either Calico pod or you can\nuse a flannel poor Network. So when we say poor Network, it's basically a framed as the\ncni. Okay container network interface. Okay, so you can use either a Calico cni or a flannel\ncni or any of the other ones. This is the two popular ones and I will be using the calcio\ncni. Okay. So this is the network range for this particular pod, and this will Specify\nover here. Okay, and then over here we have to specify the IP address of the master. So\nlet me first of all copy this entire line. And before I paste it here, let me do an if\nconfig and find out what is the IP address of this particular machine of my master machine.\nThe IP address is one ninety two dot one sixty eight dot 56.1. Not one. Okay. So let's just\nkeep that in mind and let me paste the command over here in place of the master IP address.\nI'm going to specify the IP address of the master. Okay, but I just read out. It is one.\nNinety two dot one sixty eight dot 56.1 not one and the Pod Network. I told you that I'm\ngoing to use the Calico pod. So let's copy this network range and paste it here. So all\nmy containers inside this particular pot would be assigned an IP address in this range. Okay.\nNow, let me just go ahead and hit enter and then your cluster would begin to set up. So\nit's going X expected. So it's going to take a few minutes. So just to hold on there. Okay,\nperfect. My Cuban its master has initialized successfully and if you want to start using\nyour cluster, you have to run the following as a regular user. Right so we have three\ncommands which is suggested by kubernetes itself. And that is actually the same set\nof commands or even I have here. Okay, so I'll be running the same commands. This is\nto set up the environment. And then after that we have this token generated, right the\njoining token. So the token along with the inlet address of the IP of the master if I\nbasically execute this command in my nodes, then I will be joining this cluster where\nthis is the master, right? So this is my master machine. This is created the cluster. So now\nbefore I do this though, there are a few steps in the middle. One of those steps is executing\nall these three commands and after that comes bring up the dashboard and setting up the\nboard Network right - the calcio apart. So I have to set up the Calico pod and then after\nalso set up the dashboard because if I do not start the And this before the nodes then\nthe node cannot join and I will have very severe complications. So let me first of all\ngo ahead and run these three commands one of the other. Okay, since I have the same\ncommands in my text doc. I'll just copy it from there. Okay, say ctrl-c paste enter.\nOkay, and I'll copy this line. So remember you have to execute all these things as regular\nuser. Okay, you can probably use your pseudo. But yeah, you'll be executing it as your regular\nuser and it's asking me if I want to overwrite the existing whatever is there in this directory,\nI would say yes because I've already done this before but if you are setting up the\ncluster for the first time, you will not have this prompt. Okay. Now, let me go to the third\nline copy this and paste it here. Okay, perfect. Now I've ran these three commands as I was\ntold by communities. Now, the next thing that I have to do is before I check the node status\nand all these things. Let me just set up the network. Okay, the poor Network. So like I\nsaid, this is the Line This is the command that we have to run to set up the Calico Network.\nOkay to all of the notes to join our particular Network. So it will be copying the template\nof this Calico document file is present over here in this box. Okay. So hit enter and yes,\nmy thing is created. Calcio Cube controllers created now, I'll just go back here and see\nat this point of time. I can check if my Master's connected to the particular pod. Okay, so\nI can run the cube CDL get loads command Okay. This would say that I have one particular\nresource connected to the cluster. Okay name of the machine and this role is master and\nyet the state is ready. Okay, if you want to get an idea of all the different pods which\nare running by default then you can do the cubes. He'll get pods along with few options.\nOkay should specify these flags and they are. All namespaces and with the flag O specify\nwide. Okay. So this way I get all the pods which are started by default. Okay. So there\nare different services like at CD4 Cube controllers for the Calico node for the SED Master for\nevery single service. There's a separate container and pot started. Okay, so that's what you\ncan understand from this part. Okay, that is the safe assumption. Now that we know the\ncluster the cluster is ready and the Masters part of a cluster. Let's go ahead and execute\nthis dashboard. Okay. Remember if you want to use a dashboard then you have to run this\ncommand before your notes join this particular cluster because the moment your notes join\ninto the cluster bring up the dashboard is going to be challenging and it will start\nthrowing arrows. OK it will say that it's being hosted on the Node which we do not want\nwe want the dashboard to be on the server itself right on the master. So first, let's\nbring the dashboard up. So I'm going to copy this and paste it here. Okay, Enter great.\nCommunities dashboard is created. Now the next command that you have to get your dashboard\nup and running is Cube cereal proxy. Okay with this we get a message saying that it's\nbeing served at this particular port number and yes, you are right now there you can if\nyou access Local Host. What was the port number again? Localhost? Yeah one 27.0 or 0.1 is\nlocalhost. Okay followed by port number eight thousand one, okay. Yeah, so right now we\nare not having the dashboard because it is a technically accessed on another URL. But\nbefore we do that, there are various other things that we have to access. I mean we have\nto set okay, because right now we have only enabled the dashboard now if you want to access\nthe dashboard you have to first of all create a service account. Okay. The instructions\nare here. Okay, you have to first of all create a service account for dashboard. Then you\nhave to say that okay, you are going to be the admin user of this particular service\naccount and we have to enable that functionality here. You should say dashboard admin privileges\nand you should do the cluster binding. Okay, the cluster roll binding is what you have\nto do and after that to join to that poor to get access to that particular dashboard.\nWe have to basically give a key. Okay. It's like a password. So we have to generate that\ntoken first and then we can access the dashboard. So again for the dashboard there are these\nthree commands. Well, you can get confused down the line. But remember this is separate\nfrom the above. Okay. So what we did initially is rant these three commands which kubernetes.\nOh To execute and after that the next necessity was bring up a pod. So this was that command\nfor the Pod and then this was the command for getting the dashboard up and right after\nthat run the proxy and then on that particular port number will start being served. So my\ndad would is being served but I'm not getting the UI here and if I want to get the you--if\nyou create the service account and do these three things, right? So let's start with this\nand then continue. I hope this wasn't confusing guys. Okay, I can't do it here. So let me\nopen a new terminal. Okay here I'm going to paste it. And yes service account created.\nLet me go back here and execute this command when I'm doing the role binding I'm saying\nthat my dashboard will should have admin functionalities and that's going to be the cluster roll. Okay\ncluster admin, and then the service account is what I'm using and it's going to be in\ndefault namespace. Okay. So when I created the account I said that I want to create this\nparticular account in default namespace. So the same thing I'm specifying here. Okay - good\nadmin created good. So let's generate the That is needed to access my dashboard. Okay\nbefore I execute this command, let me show you that once so if you go to this URL, right\n\/ API slash V 1 \/ namespaces. Yep, let me show to you here. Okay. So this is the particular\nURL where you will get access to the dashboard. Okay login access to the dashboard localhost\n8001 API V1 namespaces \/ Cube system \/ Services slash HTTP Cuban eighties. Dashboard: \/ proxy.\nOkay. Remember this one that is the same thing over here and like I told you it's asking\nme for my password. So I would say token but let me go here and hit the command and generate\nthe token. So this is the token amount of copy this from here till here going to say\ncopy and this is what I have to paste over here. All right. So Simon update. Yes, perfect\nwith this is my dashboard, right? This is my Cuban eighties dashboard. And this is how\nit looks like whatever I want. I can get an overview of everything. So that is workloads.\nIf I come down there is deployments. I have option to see the pods and then I can see\nwhat are the different Services running among most of the other functionalities. Okay. So\nright now we don't have any bar graph or pie graph shown you which clusters up which board\nis up and all because I have not added any node and there is no service out as running\nright. So I mean, this is the outlay of the dashboard. Okay, you will get access to everything\nyou want from the left. You can drill down into each of these namespaces or pods on containers\nright now. If you want to deploy something through the dashboard right through the click\nfunctionality, then you can go here. Okay, but before I create any container or before\nI create any pot or any deployment for that matter of fact, I have to have nodes because\nthese will be running only on nodes. Correct, whatever. I deploy they have done only on\nnode. So let me first open up my node and get the node to join this particular cluster\nof mine. Now, if you remember the command to join the node got generated at the master\nand correct. So, let me go and fetch that again. So that was the first command that\nwe ran right this one. So, let's just copy this. And paste this one at my node end. This\nis the IP of my master and it will just join at this particular port number. Let me hit\nenter. Let's see what happens. Okay, let me run it as root user. Okay? Okay, perfect successfully\nestablished connection with the API server and it says this node has joined the cluster\nRight Bingo. So this is good news to me. Now if I go back to my master and in fact, if\nI open up the dashboard there would be an option of nodes. Right? So initially now,\nit's showing this master Masters. The only thing that is part of my nodes, let me just\nrefresh it and you would see that even node - 1 would be a part of it. Right? So there\nare two resources to instances one is the master itself and the other is the node now\nif I go to overview, you will get more details if I start my application if I start my servers\nor containers then all those would start showing up your right. So it's high time. I start\nshowing you how to deploy it to deployed using the dashboard. I told you this is the functionality.\nSo let's go ahead and click on this create. And yeah mind you from the dashboard is the\neasiest way to deploy your application, right? So even developers around the world do the\nsame thing for the first time probably they created using the Amal file. And then from\nthere on they start editing the ml file on top of the dashboard itself or the create\nor deploy the application from here itself. So we'll do the same thing. Go to create an\napp using functionality click functionality. You can do it over here. So let's give a name\nto your application. I'll just say it you recur demo. Okay, let that be the name of\nmy application and I want to basically pull an engines image. Okay. I want to launch an\nengine service. So I'm going to specify the image name in my Docker Hub. Okay. So it says\neither the URL of a Public Image or any registry or a private image hosted on Docker Hub or\nGoogle container registry. So I don't have to specify the URL per se but if you are specifying\na Docker Hub, if you are specifying this image to be pulled from Docker Hub, then you can\njust use the name of the image which has to be pulled. That's good enough. Right engine\nto the name and that's good enough and I can choose to set my number of ports to one or\ntwo in that way. I will have two containers running in the pot. Right? So this is done\nand the final part is actually without the final part. I can strip it deployed. Okay,\nbut if I deployed then my application would be created but I would just don't get the\nUI. I mean, I won't see the engine service so that I get the service. I have to enable\none more functionality here. Okay, the server's here click on the drop down and you will have\nexternal option right? So click on external this would let you access this particular\nservice from your host machine, right? So that is the definition so you can see the\nexplanation here and internal or external service can be defined to map and incoming\nport to a Target Port seen by the container so engines which would be hosted on one of\nthe container ports. That could not be accessible if I don't specify anything here, but now\nthat I've said access it externally on a particular port number then it will get mapped for me\nby default. And jinkx runs on port number 80. So the target put would be the same but\nthe port I want to expose it to that. I can map into anything I want so I'm going to say\n82. All right, so that's it. It's as simple as this this way. Your application is launched\nwith two pods, so I can just go down and click on deploy and this way my application should\nbe deployed. My deployment is successful. There are two pods running. So what I can\ndo is I can go to the service and try to access the UI, right? So it says that it's running\non this particular port number 82153. So copy this and say localhost 321530 k hit enter\nbingo. So it says welcome to Jenkins and I'm building the UI, right? So I'm able to access\nmy application which I just launched through the dashboard. It was as simple as that. So\nthis is one way of for launching or making a deployment. There are two other ways. Like\nI told you one is using your CLI itself your command line interface of your draw Linux\nmachine, which is the terminal or you can do it by uploading the yamen file. You can\ndo it by uploading the yamen file because everything here is in the form of Yama Lord\nJason. Okay, that's like the default way. So whatever deployment I made right that also\nthose configurations are stored in the form of Yaman. So if I click on view or edit yeonggil,\nall the configurations are specified the default ones have been taken. So I said the name should\nbe a director demo that is what has been. Oh you're that is the name of my deployment?\nOkay. So kind is deployment the version of my API. It's this one extension \/we 1 beta\n1 and then other metadata I have various other lists. So if you know how to write a normal\nfile then I think it would be a little more easier for you to understand and create your\ndeployment because you will file is everything about lists and maps and these are all files\nare always lists about maps and maps about lists. So it might be a little confusing.\nSo probably will have another tutorial video on how to write a normal file for Cuban its\ndeployment but I would keep that for another session. Okay. Let me get back to this session\nand show you the next deployment. Okay, the next deployment technique, so let me just\nclose this and go back to overview. Okay, so I have this one deployment very good. Okay.\nSo let's go to this. Yeah. So what I'll do is let me delete this deployment. Okay our\nlet me at least scale it down because Don't want too many resources to be used on my node\nalso because I will have to show two more deployments. Right so I have reduced my deployment\nover here. And I think it's be good enough. Great. So let's go back to the cube set up\nthis document of mine. So this is where we're at. Right we could check our deployments we\ncould do all these things. So one thing which I might have forgotten is showing the nodes\nwhich are part of the cluster of right. So this is my master. Yeah, so I kind of forgot\nto show you this Cube CDL get node. So the same view that you got on your dashboard you\nget it here. Also, I mean, these are the two nodes and this is the name and all these things.\nOkay, and I can also do the cube CDL get pods which would tell me all the pods that are\nrunning under a car. Demo is the pot which I have started. Okay. This is my God. Now\nif I specify with the other flags right with all namespaces and with wide then all the\ndefault pause which get created along with your kubernetes cluster. Those will also get\ndisplayed. Let me show you that also just in case Okay. Yeah. So this is the one which\nI created and the other ones are the default of deployments that come with few minutes\nthe moment you install set up the cluster these get started. Okay, and if you can see\nhere this particular that this particular a dareka demo, which I started is running\non my Node 1 along with this Cube proxy and this particular Calico node. So Easter services\nare running on master and node. And this one is running only on my Node 1 right you can\nsee this right the Calico node runs both on my node over here and on my master and similarly\nthe queue proxy runs on my node here and on my master. So this is the one that's running\nonly on my Note. Okay, so getting back to what I was about to explain you. The next\npart is how to deploy anything through your terminal now to deploy your same engines application\nthrough your CLI. We can follow these set of commands Okay, so there are a couple of\nsteps here. First of all to create a deployment. We have to run this command. OK Cube cereal\ncreate deployment and drinks and then the name of the image that you want to create.\nThis is going to be the name of your deployment. And this is the name of the image which you\nwant to use so control C and let me go to the terminal here on my master. I'm executing\nthis command Cube cereal create a deployment. Okay. So the deployment engines is created\nif you want we can verify that also over here so under deployments right now, we have one\nentry in the array card Mo and yes now you can see there are two engines and arica demo.\nSo this is pending. I mean, it would take a few seconds. So in the meanwhile let this\ncontinue with the other steps. Once you have created a deployments, you have to create\nthe service. Okay after say which is the node Port which can be used to access that Particular\nservice, right because deployment of just a deployment you're just deploying your container\nif you want to access it. Like I told you earlier from your local from your host machine\nall those things. Then you have to enable the node board. If you want to get your deployments\non your terminal you can run this command Cube CDL get deployments. Okay engines also\ncomes up over here, right? If you want more details about your diploma. You can use this\ncommand Cube CDL describe you get like more details about this particular development\nas to what is the name? What is the port number? It's sort of siding on all these things. Okay.\nLet's not complicate this you can probably use that for understanding later. So once\nthat is done, the next thing that you have to do is you have to create the service on\nthe nodes you have created the deployment, but yes create the service on the nodes using\nthis particular command Cube cereal. Create service and say note Port. Okay, this means\nyou want to access it at this particular Point number you're doing the port mapping 80 is\n280. Okay, container Port 80 to the internal node, Port 80. Okay. So service for engines\nis created. And if you want to check which of the diplomas are running in which nodes\nyou can run the command Cube City L. Get SVC. Okay, this would tell you okay, you have two\ndifferent services at a record Mo and engines and they are anyone these port numbers and\non these nodes, right? So communities is the one which God created automatically enter\na car. Demo is the one which I created. Okay engines is again, the one which I created\ncommunities comes up on its own just specifying to you because this is a container for the\ncluster itself. Okay. So let's just go back here and then yes and similarly if you want\nto delete a deployment then you can just use this command Cube CDL delete deployment followed\nby the name of the deployment, right? It's pretty simple. You can do it this way. Otherwise\nfrom the dashboard. You can delete it like how I showed you all your click over here\nand then you can click on delete and then if you want to scale you can scale it. So\nboth of these deployment of mine have one porridge, right? So let's do one thing. So\nlet's just go to the engines service. And here let's try accessing this particular service.\nLocal Host. Okay, perfect here. Also it says welcome to engines right. So with this you\ncan understand that the port mapping worked and by going to service you will get to know\non which port number you can access it on your host machine, right? So this is the internal\ncontainer Port map to this particular Port of mine. Okay. Now if one if not for this\nif this doesn't work, you can also use the cluster IP for the same thing trust ripe is\ngoing to basically the IP using which all your containers access each other, right?\nSo if your body will have an IP. So whatever is running in their containers that will again\nbe accessible on your cluster I be so so it's the same thing right? So let me just close\nthese pages and that's how you deploy an application through your CLI. So this comes to our last\npart of this video, which is nothing but deployment via Yaman file. So for again deployment where\nI am and file you have to write your yawm Al code, right? You have to either write your\nyawm Al code or your Json code, correct? So this the code which I have written. Just in\nJama format. And in fact, I already have it in my machine here. So how about I just do\nan LS? Yeah, there is deployment at Dotty. Alright, so let me show you that so this is\nmy yamen file. Okay. So here I specify various configurations similar to how I did it using\nthe GUI or Rider reducing the CLI it something similar gesture. I specify everything and\none particular file here. If you can see that. I have a specify the API version. Okay, so\nI'm using extensions dot a slash b 1 or beta 1. Okay. I can do this or I can just simply\nspecify version 1 I can do either of those and then the next important line is the kind\nso kind is important because you have to specify what kind of file it is. Is it a deployment\nfile or is it for a pod deployment or is it for your container deployment or is it the\noverall deployment? What is it? So I've said deployment okay, because I want to deploy\nthe containers also along with the pot. So I'm saying deployment in case you want to\ndeploy only the pod which you realistically don't need to. Okay. Why would it just deploy\nup? But in case if you want to deploy a pot then you can go ahead and write Port here\nand then just specify what are the different containers. Okay, but in my case, it's a complete\ndeployment right with the pods and the services and the containers. So I will go ahead and\nwrite other things and under the metadata. I will specify the name of my application.\nI can specify what I want. I can put my name also over here like Warden, okay, and I can\nsave this and then the important part is this back part. So here is where you set the number\nof replicas. Do you remember I told you that there's something called has replication controller\nwhich controls the number of ports that you will be running. So it is that line. So if\nI have a set to over here, it means that I will have two pods running of this particular\napplication of Verdun. Okay, what exactly am I doing here under spec AB saying that\nI want to Containers so I have intended or container line over here and then I have two\ncontainers inside. So the first container which I want to create is of the name front\nend. Okay, and I'm using an engines image and similarly. The port number that this would\nbe active on is container Port 80. All right, and then I'm saying that I want a second container\nand the container for this could I could rename this to anything? I can say back end and I\ncan choose which image I want. I can probably choose a httpd image also. Okay, and I can\nagain say the port's that this will be running on I can say the container Port that it should\nrun on is put number is 88 right? So that's how simple it is. All right. And since it's\nyour first video tutorial the important takeaways from this yawm Al file configuration is that\nunder specular have to specify the containers? And yes everything in Json format with all\nthe Intel dacians and all these things. Okay, even if you have an extra space anywhere over\nhere, then you are real file would throw an invalid error. So make sure that is not there.\nMake sure you specify the containers appropriately if it's going to be just one container. Well\nand good it's two containers. Make sure you intend it in the right way and then you can\nspecify the number of PODS. You want to give a name to your deployment and Mainly established\nread these rules. Okay. So once you're done with this just save it and close the yamen\nfile. Okay. So this is your deployment djamel. Now, you can straight away upload this table\nfile to your Kubernetes. Okay, and that way your application would be straight with deployed.\nOkay. Now the command for that is Cube cereal create - F and the name of the file. Okay.\nSo let me copy this and then the name of my file is deployment or djamel. So let me hit\nenter. Perfect. So my deployment the third deployment vardhan is also created right so\nwe can check our deployments from the earlier command. That is nothing but Cube CDL get\ndeployments. Okay. It's not get deployment audiometer. Sorry. It's get deployments. And\nas you can see here, there is an Adder a guard Mo there is engines and there is Verdun and\nthe funny thing which you should have noticed is that I said, I want to replicas right to\npods. So that's why the desire is to currently we have to up to date is one. So okay update\nis to brilliant available is 0 because let's just give it a few seconds in 23 seconds.\nI don't think the board would have started. So let's go back to our dashboard and verify\nif there's a third deployment that comes up over here. Okay, perfect. So that's how it's\ngoing to work. Okay, so probably is going to take some more time because the containers\njust restarting. So let's just give it some more time. This could well be because of the\nfact that my node has very less resource, right? So I have too many deployments that\ncould be the very reason. So what I can do is I could go ahead and delete other deployments\nso that my node can handle these many containers and pods right? So let me delete this particular\ndeployment and Rings deployment and let me also delete this Adder a car demo deployment\nof mine. Okay. Now let's refresh and just wait for this to happen. Okay. So what I can\ndo instead is I could have a very simple deployment right? So let me go back to my terminal and\nlet me delete my deployment. Okay, and let me redeployed again, so Cube CDL delete deployment.\nOkay, so what then this deployment has been deleted? Okay. So let's just clear the screen\nand let's do G edit of the yamen file again and here let's make things simpler. Let me\njust delete this container from here. Let me save this right and close this now. Let\nme create a deployment with this. Okay. So what then is created, let me go up here and\nrefresh. Let's see what happens. Okay. So this time it's all green because it's all\nhealthy. My nodes are successful or at least it's going to be successful container creating.\nPerfect. So two parts of mine are up and running and both my paws are running right and both\nare running on Node 1 pause to or of to those are the two deployments and replica set and\nthen Services, right? So it's engines which is the basement which is being used. So well\nand good. This is also working. So guys. Yeah, that's about it. Right. So when I try to upload\nit, maybe there was some other error probably in the arm will file they could developments\nfrom small mistake or it could have been because my known had too many containers running those\ncould have been the reasons. But anyways, this is how you deployed through your yamen\nfile. All right, so that kind of brings us to the end of this session where I've showed\nyou a demonstration of deploying your containers in three different ways CLI dashboard and\nyour yamen files. Hey everyone, this is Reyshma from Edureka. And today we'll be learning\nwhat is ansible. First,let us look at the topics that we'll be learning today. Well,\nit's quite a long list. It means we'll be learning a lot of things today. Let us take\na look at them one by one. So first we'll see the problems that were before configuration\nmanagement and how configuration management help to solve. It will see what ansible is\nand the different features of ansible after that. We'll see how NASA is implemented and\ncivil to solve all their problems. After that. We'll see how we can use ansible for orchestration\nprovisioning configuration management application deployment and security. And in the end, we'll\nwrite some ansible playbooks to install lamp stack on my node machine and host your website\nin my note machine. Now before I tell you about the problems, let us first understand\nwhat configuration management actually is. Well configuration management is actually\nthe management of your software on top of your Hardware. What it does is that it maintains\nthe consistency of your product based on its requirements its design and its physical and\nfunctional attributes. Now, how does it maintain the consistency it is because the configuration\nmanagement is applied over the entire life cycle of your system. And hence. It provides\nyou with a very good visibility and control when I say visibility. It means that you can\ncontinuously check and monitor the performances of all your assistants. So if at any time\nthe performance of any of his system is degrading the configuration management system will notify\nyou and hence. You can prevent errors before it actually occurs and by control, I mean\nthat you have the power to change anything. So if any of your servers failed you can reconfigure\nit again to repair it so that it is up and running again, or you can even replace the\nserver if needed and also the configuration management system holds the entire historical\ndata of your infrastructure it DOC. Men's all the snapshots of every version of your\ninfrastructure. So overall the configuration management process facilitates the orderly\nmanagement of your system information and system changes so that it can use it for beneficial\npurposes. So let us proceed to the next topic and see the problems before configuration\nmanagement and how configuration management solved it and with that you'll understand\nmore about configuration management as well. So, let's see now, why do we need configuration\nmanagement now, the necessaries behind configuration management was dependent upon a certain number\nof factors and certain number of reasons. So let us take a look at them one by one.\nSo the first problem was managing multiple servers now earlier every system was managed\nby hand and by that, I mean that you have to login to them via SSH make changes and\nthen log off again. Now imagine if a system administrator would have to make changes in\nmultiple number of servers. You'll have to do this task of logging in making changes\nand longing of again and again repeatedly, so this would take up a lot of time and there\nis no time left for the system administrators to monitor the performances of the system\ncontinuously safe at any time any of the servers would fail it took a lot of time to even detect\nthe faulty server and to even more time to repair it because the configuration scripts\nthat they wrote was very complex and it was very hard to make changes on to them. So after\nconfiguration management system came into the picture what it did is that it divided\nall the systems in my infrastructure according to their dedicated tasks their design or architecture\nand the organize my system in an efficient way. Like I've proved my web servers together\nmy database servers together application servers together and this process is known as baselining.\nNow. Let's for an example say that I wanted to install lamp stack in my system and lamp\nstack is a software bundle where L stands for Linux a for Apache and for MySQL and P\nfor PHP. So I need this different software's for different purposes. Like I need Apache\nserver to host my web pages and it PHP for my web development. I need Linux as my operating\nsystem and MySQL as my data definition language or data manipulation language since now all\nthe systems in my infrastructure is Baseline. I would know exactly where to install each\nof the software's. For example, I'll use Apache as my web server here for database. I will\ninstall the MySQL here and also begin easy for me to monitor my entire system. For example,\nif my web pages are not running I would know that there's something wrong. With my web\nservers, so I'll go check in here. I don't have to check the database servers and application\nservers for that. Similarly. If I'm not able to insert data or extract data from my database.\nI would know that something is wrong with my database servers. I don't need to check\nthese too for that matter. So what configuration management system did with baselining is that\nit organized mess system in an efficient way so that I can manage and monitor all my servers\nefficiently. Now, let us see the second problem that we had which were scaling up and scaling\ndown. See nowadays, you can come up with requirements at any time and you might have to scale up\nor scale down your systems on the Fly and this is something that you cannot always plan\nahead and scaling up. Your infrastructure doesn't always mean that you just buy new\nhardware and just place them anywhere. Haphazardly. You cannot do that. You also need to provision\nand configure this new machines properly. So with configuration management system, I've\nalready got my infrastructure baselined so I know exactly how this new machines are going\nto work according to their dedicated task and where should I actually place them and\nthe scripts that configuration management uses are reusable so you can use the same\nscripts that you use to configure your older machines to configure your new machines as\nwell. So let me explain it to you with an example. So let me explain it to you with\nan example. Let's say that if you're working in an e-commerce website and you decide to\nhold a mega sale. New Year Christmas sale or anything? So it's obvious that there is\ngoing to be a huge rise in the traffic. So you might need more web servers to handle\nthat amount of requests and you might even need a load balancers or maybe to to distribute\nthat amount of traffic onto your web servers and these changes however need to be made\nat a very short span of time. So after you've got the necessary Hardware, you also need\nto provision them accordingly and with configuration management, you can easily provision this\nnew machines using either recipes or play books or any kind of script that configuration\nmanagement uses. And also after the sale is over you don't need that many web servers\nor a load balancer so you can disable them using the same easy scripts as well and also\nscaling down is very important when you are using cloud services when you do not need\nany of those machines, it's no point in keeping them. So you have to scale down as well because\nyou have to reconfigure your entire infrastructure as well and with configuration management.\nIt is a very easy. Anything to Auto scale up and scale down your infrastructure. So\nI think you all have understood this problem and how configuration management salted so\nlet us take a look at the third problem. Third problem was the work velocity of the developers\nwere affected because the system administrators were taking time to configure the server's\nafter the developers have written a code. The next job is to deploy them on different\nservers like test servers and production servers for testing it out and releasing it but then\nagain every server was managed by hand before so the system administrators would again have\nto do the same thing log in to its server configure them properly by making changes\nand do the same thing again to all servers. So this was taking a lot of time now before\ndevops game you the picture there was already agility in the developers end for which they\nwere able to release new software's very frequently, but it was taking a lot of time for the system\nadministrators to configure the servers for testing so the developers would have Wait\nfor all the test results and this highly hamper the word velocity of the developers. But after\nthere was configuration management the system administrator had got access to a configuration\nmanagement tool which allowed them to configure all the servers at one go. All they had to\ndo is write down all the configurations and write down the list of all the software's\nthat there need to provision this servers and deploy it on all of the servers at one\ngo. So now agility even came into the system administrators and as well. So now after configuration\nmanagement the developers and the system administrators were finally able to work in the same base.\nNow, this is how configuration management solve the third problem now, let us take a\nlook at the last problem. Now the last problem was rolling back in today's scenario. Everyone\nwants a change and you need to keep making changes frequently because customers will\nstart losing interest if things stay the same so you need to keep releasing new features\nto upgrade your application even giants like Amazon and Facebook. They do it now and then\nand still they're unsure if the users are going to like it or not. Now imagine if the\nusers did not like it they would have to roll back to the previous version again, so, let's\nsee how it creates a problem. Now before there was configuration management. Let's say you've\ngot the old version which is the version one when you're upgrading it you're changing all\nthe configurations in the production server. You're deleting the old configurations completely\nand deploying the new version now if the users did not like it you would have to reconfigure\nThis Server again with the old configurations and that will take up a lot of time. So application\nis going to be Down for that amount of time that you need for reconfiguring the server\nand this might create a problem. But when you're using configuration management system,\nas you know that it documents every version of your infrastructure when you're upgrading\nit with configuration management, it will remove the configurations of the older version,\nbut it will be well documented. It will be kept there and then the newer version is deployed.\nNow if the users did not like it this time, the older of the configuration version was\nalready documented. So all you have to do is just switch back to the old version and\nthis won't take up any time and you can upgrade or roll back your application in zero downtime\nzero downtime means that your application would be down for zero time. It means that\nthe users will not notice that your application went down and you can achieve it seamlessly\nand this is how configuration management system solved all the problems that was before. So\nguys. I hope that if all understood how Management did that let us now move on to the next topic?\nNow the question is how do I incorporate configuration Management in my system? Well, you do that\nusing configuration management tools. So let's take a look at all the available configuration\nmanagement tools. So here I've got the four most popular tools that is available in the\nmarket right now. I've got ansible and Saul stack which are push-based configuration management\ntool by push-based. I mean that you can directly push all those configurations on to your node\nmachines directly while chef and puppet are both pull based configuration management tools.\nIt means that they rely on a central server for configurations the pull all the configurations\nfrom a central server. There are other configuration management tools available in the market to\nbut but these four are the most popular ones. So now let's know more about ansible now ansible\nis a configuration management tool that can be used for provisioning orchestration application\ndeployment Automation and it's a push based configuration management tool. Like I told\nyou what it does is that it automate your entire it infrastructure and gives you large\nproductivity gains and it can automate pretty much anything. It can automate your Cloud\nyour networks your servers and all your it processes. So let us move on to the next topic.\nSo now let us see the features of ansible. The first feature is that it's very simple.\nIt's simple to install and setup and it's very easy to learn because ansible Play books\nare written in a very simple data serialization language, which is known as Gamal and it's\npretty much like English. So anyone can understand that and it's very easy to learn next feature\nbecause of which ansible is preferred over other configuration management tools is because\nit's Agent kallus it means that you do not need any kind of Agents or any kind of plan\nsoftware's to manage your node machines. All you have to do is install ansible in your\ncontrol machine and just make an SSH connection with your nodes and start pushing configurations\nright away. The next feature is that it's very powerful, even though you call ansible\nsimple and it does not require any agent. It has the capabilities to model very complex\nit workflows and it comes with a very interesting feature, which is called the batteries included.\nIt means that you've got everything that you already need and in ansible it's because it\ncomes with more than 750 inbuilt modules, which you can use them for any purpose in\nyour project. And it's very efficient because all the modules that ansible comes with they\nare extensible. It means that you can customize them according to your needs and for doing\nthat you do not need to use the same programming language that it was originally written in\nyou can choose any kind of programming language that you're comfortable with and then customize\nthose modules for your own use. So this is the power and Liberty that ansible gives you\nnow, let us take a look at the case study of NASA. What were the problems that NASA\nwas facing and how ansible solved all those problems? Now NASA is an organization that\nhas been sending men to the Moon. They are carrying out missions and Mars and they're\nlaunching satellites now and then to monitor the Earth and not just the Earth. They're\neven monitoring other galaxies and other planets as well. So you can imagine the kind and the\namount of data that NASA might be dealing with but all the applications were in a traditional\nHardware based Data Center and they wanted to move into a cloud-based environment because\nthey wanted better agility and they wanted better adaptive planning for that. And also\nthey wanted to save costs because a lot of money was spent on just the maintenance of\nthe hardware and also they wanted more security because NASA is a government organization\nof the United States of America and obviously, they wanted more security because NASA is\na government organization of the United States of America and the hold a lot of confidential\ndetails as well for the government. So they just Cannot always rely on the hardware to\nstore all This Confidential files, they needed more security because if at any time the hardware\nfails, they cannot afford to lose that data and that is why they wanted to move all their\n65 applications from a hardware environment to a cloud-based environment. Now, let us\ntake a look. What was the problem now for this migration of all the data into a cloud\nenvironment. They contacted a company called in Frozen now in Frozen is a company who is\na cloud broker and integrator to implement solutions to meet needs with security. So\nin phase and was responsible for making this transition and NASA wanted to make this transition\nin a very short span of time. So all the applications were migrated as it is into the cloud environment\nand because of this all the AWS accounts and all the virtual private clouds that was previously\ndefined they all got accumulated in a single data space and this It up a huge chunk of\ndata and NASA had no way of centrally managing it and even simple tasks like giving a particular\nsystem administrator access rights to a particular account. This became a very tedious job with\nNASA wanted to automate and to and deployment of all their apps and for that they needed\na management system. So this was the situation when NASA moved into the cloud so you can\nsee that all those AWS accounts and virtual private cows. They got accumulated and made\na huge chunk of data and everyone was excessing directly to it. So there is a problem in managing\nthe credentials for all the users and the different teams, but NASA needed was divided\nup all their inventories all the resources into groups and number of hosts. And also\nthey wanted to divide up all the users in two different teams and give each team different\ncredentials and permissions. And also if you look in the more granular level each user\nin each team could also have different credentials and permissions. Let's say that you want to\ngive the team leader of a particular team access to some kind of data what you don't\nwant the other users in the team to access that data. So also NASA wanted to Define different\ncredentials for each individual member as well the wanted to divide up all the data\naccording to the projects and jobs also now, so I wanted to move from chaos into a more\norganized Manner and for that they adopted ansible tower now ansible Tower is ansible\nin and more enterprise-level ansible Tower provides you with the dashboard which provides\nall the status summary of all the hosts and job and simple Tower is a web-based interface\nfor managing your organization. It provides you with a very easy to use user interface\nfor managing quick deployments and monitoring all the configurations. So, let's see what\nanswer build our did it has the credential management system which could give different\naccess permission to each individual user and Teams and also divided up the user into\nteams and single individual users as well and it has a job assignment system and you\ncan also assign jobs using ansible tower X suppose. Let's say that you have assigned\njob one to a single user job to another single user while job to could be assigned to a particular\nteam. Similarly. The whole inventory was also managed all the servers. Let's say dedicated\nto a particular mission was grouped together all the host machines and other systems as\nwell Sansa built our help NASA to organize everything now, let us take a look at the\ndashboard that ansible Tower provides us. So this is the screenshot of the dashboard\nat a very initial level. You can see right now there is zero host. Nothing is there but\nI'm just showing you what ansible tower provides you so on the top you can check all the users\nand teams. You can manage the credentials from here. You can check your different projects\nand inventories. You can make job templates and schedule job. As well. So this is where\nyou can schedule jobs and provide every job with a particular ID so that you can track\nit. You can check your job status here whether your job was successful or failed and since\nansible Tower is a configuration management system. It will hold the historical data as\nwell. So you can check the job statuses of the past month or the month before that. You\ncan check the host status as well. You can check how many hosts are up and running you\ncan see the host count here. So this dashboard of ansible tower provides you with so much\nease of monitoring all your systems. So it's very easy to use ansible to our dashboard\nanyone in your company anyone can use it because it's very user-friendly now, let us see the\nresults that NASA achieved after it has used ansible tower now updating nasa.gov used to\ntake one hour of time and after using ansible it got down to just five minutes security\npatching updates where a multi-day process and now it requires only 45 minutes the provisioning\nof os accounts can be done in just 10 minutes earlier the application Stack Up time required\none to two hours and now it's done in only 10 minutes. It also achieved a near real-time\nRAM and this monitoring and baselining all the standard Amazon machine image has this\nused to be a one-hour manual process. And now you don't even need manual interference\nfor that. It became a background invisible process. So you can see that how ansible has\ndrastically changed the overall management system of NASA. So guys, I hope that if understood\nhow I answered will help NASA. If you have any question, you may ask me at any time on\nthe chat window. So let us proceed to the next topic. Now this was all about how others\nhave used ansible. So now let us take a look at the ansible architecture so that we can\nunderstand more about ansible and decide how we can use ansible. So this is the overall\nansible architecture. I've got the answer. Automation engine and I've got the inventory\nand a Playbook inside the automation engine. I've got the configuration management database\nhere and host and this configuration management database is a repository that acts as a data\nwarehouse for all your it installations. It holds all the data relating to the collection\nof your all it assets and these are commonly known as configuration items and it also holds\nthe data which describe the relationships between such assets. So this is a repository\nfor all your configuration management data and here I've got the ansible automation engine.\nI've got the inventory year and inventory is nothing but the list of all the IP addresses\nof all my host machines now as I told you how to use configuration management you use\nit with the configuration management tool like ansible but how do you use ansible? Well,\nyou do that using playbooks. And playbooks describe the entire workflow of your system.\nInside playbooks. I've got modules apis and plugins now modules are the core files now\nplay books contain a set of place which are a set of tasks and inside every task. There\nis a particular module. So when you run a play book, it's the modules that actually\nget executed on all your node machines. So modules are the core files and like I told\nyou before ansible already comes with inbuilt modules, which you can use and you can also\ncustomize them as well as comes with different Cloud modules database modules. And don't\nworry. I'll be showing you how to use those modules in ansible and there are different\napis as well. Well API is an answerable are not meant for direct consumption. They're\njust there to support the command line tools. For example, they have the python API and\nthese apis can also be used as a transport for cloud services, whether it's public or\nprivate you can use it then I've got plugins now plug in Our special kind of module that\nallowed to execute ansible task as job Bill step and plugins are pieces of code that augment\nthe ansible score functionality and ansible also comes with a number of Handy plugins\nthat you can use. For example, you have action plugins cash plugins callback plugins and\nalso you can create plugins of your own as well. Let me tell you how exactly different\nit is from a module. Let me give you the example of action plug-in now action plug in our front-end\nmodules and what it does is that when you start running a Playbook something needs to\nbe done on the control machine as well. So this action plugins trigger those action and\nexecute those tasks in the controller machine before calling the actual modules that are\ngetting executed in the Playbook. And also you have a special kind of plug-in called\nThe Connection plug in which allows you to connect to the docker containers in your note\nmachine and many more and finally I have this host machine that is Elected via SSH and this\nwas machines could be either windows or Linux or any kind of machines. And also let me tell\nyou that it's not always needed to use SSH for connection. You can use any kind of network\nAuthentication Protocol you can use Kerberos and also you can use the connection plugins\nas well. So this is fairly a very simple ansible architecture. So now that you've understood\nthe architecture, let us write a play book now now let me tell you how to write a play\nbook and playbooks and ansible are simple files written in HTML code and yambol is a\ndata serialization language. You can think of data serialization language as a translator\nfor breaking down all your data structure and serialize them in a particular order which\ncan be reconstructed again for later use and you can use this reconstructed data structure\nin the same environment or even in a different environment. So this is the control machine\nwhere ansible will be installed and this is where you'll be writing your playbooks. Let\nme show you the structure of how to write a play book. However, play book starts with\nthree dashes on the top. So first you have to mention the list of all your host machines\nhere. It means where do you want this Playbook to run? Then you can mention variables by\ngathering facts, then you can mention the different tasks that you want. Now remember\nthat the task get executed in the same order that you write them. For example, if you want\nto install software a first and then softer beef later on. So make sure that the first\ntask would be install software and the next task would be install software be and then\nI've got handlers at the bottom. The handlers are also tasks but the difference is in order\nto execute handlers. You need some sort of triggers in the list of tasks. For example,\nwe use notify. I'll show you an example now. Okay, let me show you an example of Playbook\nso that you can relate to this structure. So this is an example of an ansible Playbook\nto install Apache like I told It starts with three dashes on the top remember that every\nlist starts with a dash in the front or a - here. I've only mentioned just the name\nof one group. You can mention the name of several groups where you want to run your\nplaybook. Then I've got the tasks you give a name for the task which is install Apache\nand then you use a module here. I'm using the app module to download the package. So\nthis is the syntax of writing the app module. So you give the name of the package which\nis Apache to update cache is equal to yes. So it means that it will make sure that app\nget is already updated in your note machine before it installs the Apache 2 and you mentioned\nState equal to latest. It means that it will download the latest version of Apache 2. And\nthis is the trigger because I'm using handlers you're right and the Handler here is to restart\nApache and I'm using the service module here and the name of the software that I want to\nrestart is Apache. And state is able to restart it. So notify have mentioned that there is\ngoing to be a Handler whose job would be to restart Apache 2 and then the task in the\nHandler would get executed and it will restart Apache 2. So this is a simple Playbook and\nwill also be writing similar kind of playbooks later on the Hands-On part. So you'll be learning\nagain. So if it's looking a little gibberish for you will be doing and that on the Hands-On\npart so then it will clear all your doubts. So now let us see how to use ansible and understand\nits applications so we can use ansible for application deployment configuration management\nsecurity and compliance provisioning and orchestration. So let us take a look at them one by one first.\nLet us see how we can use ansible for orchestration. Well orchestration means let's say that we\nhave defined configurations for each of my systems, but I also need to make sure how\nthis configurations will interact with each other. So this is the process of Orchestration\nbut I decide that how the different configurations on different of my systems and my infrastructure\nwould interact with each other in order to maintain a seamless flow of my application\nand your application deployments need to be orchestrated because you've got a front-end\nand back-end Services. You've got databases you've got monitoring networks and storage\nand each of them has their own role to play with with their configuration and deployment\nand you cannot just run all of them is ones and expect that the right thing happens. So\nwhat you need is that you need an orchestration tool that all this task happen in the proper\norder that the database is up before the backend server and the front end server is removed\nfrom the load balancer before it gets upgraded and that your networks would have their proper\nvlans configured. So this is what ansible helps you to do. So, let me give you a simple\nexample so that you can understand it better. Let's say that I want to host a website on\nmy node machines. And this is precisely what we're going to do later on the Hands-On part.\nSo first and in order to do that first, I have to install the necessary software, which\nis the lamp stack and after that I have to deploy all the HTML and PHP files on the web\nserver. And after that I'll be gathering some kind of information from my web pages that\nwill go inside my database server. Now, if you want to perform these all tasks, you have\nto make sure that the necessary software is installed first now, I cannot deploy the HTML\nPHP files on the web servers. If I don't have a web servers if a party is not installed.\nSo this is orchestration where you mention that the task that needs to be carried out\nbefore and the task that needs to be carried out later. So this is what ansible playbooks\nallow you to do. Now. Let's see what provisioning is like provisioning in English means to provide\nwith something that is needed. It is same in case of ansible it. That ansible will make\nsure that all the necessary software is that you need for your application to run is properly\ninstalled in each of the environments of your infrastructure. Let us take a look at this\nexample here to understand what provisioning actually is. Now if I want to provision a\npython web application that I'm hosting on Microsoft Azure and Microsoft is your is very\nsimilar to AWS and it is also a cloud platform on which you can build up all your applications.\nSo let's say so now if I want to host my if I'm developing a python web application for\ncoding I would need the Microsoft is your document database. I would need Visual Studio\nor need to install python also and some kind of software development kit and different\napis for that so ansible so you can list out the name of all the software development kits\nand all this necessary software's that you will require for coding this web that it would\nrequire in order to develop your web application. So you can list out all the necessary software\nis that you'd be needing in ansible playbook in order to develop your web application and\nfor testing your code out you will again need Microsoft Azure document database you would\nagain note visual studio and some kind of testing software. So again, you can list out\nall the software's and ansible Playbook and it will provision your testing environment\nas well. And it's the same thing while you're deploying it on the production server as well\nand Sybil will provision your entire application at all stages at coding stage a testing and\nat the production stage also, so guys, I hope you've understood what provisioning is let\nus move on to the next topic and see how we can achieve configuration management with\nansible now ansible configurations are simple data descriptions of your infrastructure,\nwhich is both human readable and machine possible and app server requires. Nothing more than\nan SSH key in order to start managing systems and you can start managing them without installing.\nAny kind of agent or client software? So you can avoid the problem of managing the management\nwhich is very common in different automation systems. For example, I've got my host machines\nand Apache web servers installed in each of the host machines. I've also got PHP and MySQL\ninstalled if I want to make configuration changes if I want to update a party and update\nmy MySQL I can do it directly. I can push those new configuration details directly onto\nmy host machines or my note machines and my server and you can do it very easily using\nansible playbooks. So let us move on to the next topic and let us see how application\ndeployment has been made easier with ansible now ansible is the simplest way to deploy\nyour applications. It gives you the power to deploy all your multi-tier applications\nwhere reliably and consistently and you can do it all from a common framework. You can\nconfigure all the needed Services as well as push application artifacts from one system.\nWith ansible you can write Play books which are the description of the desired state of\nyour system and it is usually kept in the source control sensible. Then does all the\nhard work for you to get your systems to the state. No matter what state they are currently\nin and play books make all your installations all your upgrades for day-to-day management,\nvery repeatable. So with ansible you can write Play books which are the descriptions of the\ndesired state of the systems. And these are usually kept in the source control and simple\nthen does all the hard work for you to get all your systems in the desired State no matter\nwhat state they're currently in and playbooks make all your installations your upgrades\nand for all your day-to-day Management in a very repeatable and reliable way. So let's\nsay that I am using a version control system like get while I'm developing my app. And\nalso I'm using Jenkins for continuous integration now Jenkins will extract code from get every\ntime there is a new Commit and then making software built and later. This build will\nget deployed in the test server for testing. Now if changes are kept making in the code\nbase continuously. You would have to configure your test and the production server continuously\nas well according to the changes. So what ansible does is that it continuously keeps\non checking the Version Control System here so that it can configure the test and the\nproduction server accordingly and quickly and hence. It makes your application deployment\nlike a piece of cake. So guys, I think you have understood the application deployment.\nDon't worry in the Hands-On part will also be deploying our own applications on different\nservers as well. Now, let us see how we can achieve security with ansible in today's complex.\nIt environment security is Paramount you need security for your systems you need security\nfor your data and not just your data your customers data as well. Not only you must\nbe able to Define what it means for your systems to be. You also need to be able to Simply\napply that security and also you need to constantly monitor your systems in order to ensure that\nthey remain compliant with that security and with ansible. You can simply Define security\nfor your systems using playbooks with playbooks. You can set up firewall rules. You can log\ndown different users or groups and you can even apply custom security policies as well\nnow ansible also works with the Mind Point Group which rights and civil rules to apply\nthese aesthetic now disa stick is a cybersecurity methodology for standardizing security protocols\nwithin your network servers and different computers. And also it is very compliant with\nthe existing SSH and win RM protocols. And this is also a reason why ansible is preferred\nover other configuration management tools and it is also compatible with different security\nverification tools like opens Gap and stigma what tools like opens cap and stigma does\nis that it carries out a timely inspection. All your software inventory and check for\nany kind of vulnerabilities and it allows you to take steps to prevent those attacks\nbefore they actually happen and you can apply the security over your entire infrastructure\nusing ansible. So, how about some Hands-On with ansible? So let us write some ansible\nplaybooks now. So what are we going to do is that we are going to install lamp stack\nand then we're going to host a website on the Apache server and will also collect some\ndata from our webpage and store it in the MySQL server. So guys, let's get started.\nSo here I'm using the Oracle virtualbox manager and here I've created two virtual machines.\nThe first is the ansible control machine and the ansible host machine. So ansible control\nmachine is the machine where I have installed and simple and this is where I'll be writing\nall my playbooks and answer will host one here is going to be my note machine. This\nis where the playbooks are going to get deployed. So in this machine, I'll deploy my website.\nSo I'll be hosting a website in the answer will host one. Just go to my control machine\nand start writing the playbooks. So this is my ansible control machine. Now. Let's go\nto the terminal first. So this is the terminal of my ansible control machine. And now I've\nalready installed ansible here and I've already made an SSH connection with my note machine.\nSo let me hear just become the root user first now, you should know that you do not always\nneed to become the root user in order to use ansible. I'm just becoming the root user for\nmy convenience because I like to get all the root privileges while I'm using ansible, but\nyou can pseudo to any user if you like So let me clear my screen first. Now before we\nstart writing play boo status first check the version of ansible that is installed here.\nAnd for that I'll just use the command ansible - - version. And as you can see here that\nI have got the ansible two point two point zero point zero version here. Now. Let me\nshow you my host inventory file since I've got only one node machine here. So I'm going\nto show you where exactly the IP address of my node machine is being stored. So open the\nhosts file for you now, so I'm just going to open the file and show it to you. So I'm\nusing the G edit editor and the default location of your host inventory file is at sea. I'm\nsupposed \/ posts. And this is your host inventory file and now have mentioned the IP address\nof my host machine here, which is one. Ninety two point one sixty eight point 56.1 02 and\nI have named it under the group name test servers. So always write the name of your\ngroup under the square brackets now, I just have one node machine. So there is only one\nIP address. If you have many node machines, you can just let us down the IP address under\nthis line. It's as simple as that or if you even want to group it under a different name,\nyou can use a different name use another square bracket and put a different name for another\nset of your hosts. Okay. Now, let me clear my screen first. So first, let me just test\nout the SSH connection whether it's working properly or not using ansible. So for that\nI'll just type in the command and Sybil and pink and then the name of the group of my\nhost machines, which is test servers in my case. And thank changed to Paul. It means\nthat an SSH connection is already established between my control machine and my note machine.\nSo we are all ready to write playbooks and start deploying it on the notes. So the first\nthing that I need to do is write a provisioning Playbook now, since I'm going to host a website,\nI would first need to install the necessary software's so I'll be writing a provisioning\nplaybook for that and out provision my node machine using lamp stack. So let us write\na Playbook to install lamp stack on my Note machine now, I've already written that Playbook.\nSo I'm just going to show it to you. I'm using the Gia did editor again and the name of my\nprovisioning playbook is lamp stack. And the extension for AML file is Dot. Yml, and this\nis my playbook. Now. Let me tell you how I have written this Playbook as I told you that\nevery play book starts with three dashes on the top. So here are the three dashes and\nthen I've given a name to this Playbook which is to install Apache PHP and MySQL. Now, I've\nalready got the L in my lamb because I'm using a Ubuntu machine which is a Linux operating\nsystem. So I need to install Apache PHP and MySQL now and then you have to mention the\nhost here on which you want this Playbook to get deployed. So I've mentioned this over\nhere and then I want to escalate my privileges for which I'm using become and become user\nit is because sometimes you want to become another user different from what you are actually\nlogged into the remote machine. So you can use escalating privileges tools like so or\npseudo to gain root privileges. And so and that is why I've used become and become user\nfor that. So I'm becoming the user root and I'm using become true here on the top. What\nit does is that it activates Your Privilege escalation and then you become the root user\non the remote machine and then gather facts true. Now, what it will do is that we gather\nuseful variables about the remote host. Now what exactly it will gather is some sort of\nfiles or some kind of keys which can be used later in a different Playbook. And as you\nknow that every Playbook is a list of tasks that you need to perform. So this is the list\nof all my tasks that I'm going to perform and since it's a provisioning Playbook, which\nmeans I'm only installing the necessary softwares. That will be needed in order to host a website\non my Note machine. So first I'm installing Apache so given the task name as install apache2\nand then I'm using the package module here. And this is the syntax of the package module.\nSo you have to first specify the name of the package that you are going to download which\nis Apache 2 and then you put State equal to present now since we're installing something\nfor the first time and it won't this package to be present in your node machine. So you're\nputting State equal to present now similarly if you want to delete something you can put\nState equal to absent and it works that way so I've installed in Apache PHP module and\nI've installed PHP client PHP Emperor PHP GD library of install a package PHP MySQL.\nAnd finally, I've installed the MySQL server in the similar way that I've installed a party\nto this is a very simple Playbook to provision your node machine and actually all the playbooks\nare simple. So I hope that you have understood how to write a Book now, let me tell you something\nthat you should always keep in mind while you were writing playbooks make sure that\nyou are always extra careful with the indentation because Gamal is a data serialization language\nand it differentiates between elements with different indentations. For example, I've\ngot a name here and a name here also, but you can see that the indentations are different\nit is because this is the name of my entire Playbook while this is just the name of my\nparticular task. So these two are different things and they need to have different indentations\nthe ones with the similar indentations are known as siblings like this one. This is also\ndoing the same thing. This is also installing some kind of package and this is also installing\nsome kind of package. So these are similar, so that's why you should be very careful with\nindentation. Otherwise, it will create a problem for you. So what are we waiting for? Let us\nrun this Playbook clear my screen first. So in order to run a play book and the command\nthat you should be using to run an answerable Playbook is ansible - Playbook And then the\nname of your file, which is lamp stack dot Jama and here we go. And here it is. Okay\nbecause it is able to connect to my note machine. Apache 2 has been installed. And it's done.\nMy playbook is successfully run. And how do I know that? I know that seeing these common\nreturn values. So these common return values like okay changed unreachable and fate. They\ngive me the status summary of how my playbook was run. So okay equal to 8, it means there\nwere eight tasks. That was Run Okay changed equal to 7. It means that something in my\nnote between has been changed because obviously I've install new packages into my note machine.\nSo it's showing changed equal to 7 unreachable is equal to 0 it means that there is zero\nhost that were unreachable and failed equal to 0 it means that zero tasks where fate so\nmy playbook was run successfully on to my note between. So let us check my note machine\nand see if Apache and MySQL has been installed. So let us go to my node machine now. So this\nis my node machine. So let us check knife. Apache server has been installed. So I'm going\nto my web browser. So this is my web browser in my note machine. Let me go to the Local\nHost and check if Apache web server has been downloaded and it's there. It works. Now.\nThis is the default web page of apache2 web server. So now I know for sure that Apache\nwas installed in my note machine now. Let us see if MySQL server has been installed.\nLet me go to my terminal. This is the terminal of my load machine. Now. If you want to check\nif MySQL has installed just use this following command. mice ql user is root then - p sudo\npassword password again for MySQL and there it is. So MySQL server was also successfully\ninstalled in my note machine. So let's go back to my control machine and let's do what\nis left to do. So we're back into our control machine. Now. I've already provisioned my\nnote machine. So let's see what we need to do next now since we are deploying a website\non the Node machine, let me first show you how my first web page looks like let me first\nshow you how my first web page looks like so this is going to be my first web page which\nis index dot HTML and I've got two more PHP files also this salvi actually deploying these\nfiles onto my node machine. So let me just open the first webpage to you. So this is\ngoing to be my first web page. And what I'm going to do is that I'm going to ask for name\nand email because this is a registration page for at Eureka where you have to register with\nyour name and email and I want this name and email to go into my database. So for that\nI need to create a database and also need to create a table for this name and email\ndata to store into so for that will write another play book and we'll be using database\nmodules in that clear the screen first now again, I've already written that Playbook.\nSo let me just show it to you. So using the G edit editor here again and the name of this\nPlaybook is my school module. Okay. So this is my playbook. So like all Playbook it starts\nwith three dashes and here I have mentioned host all now. I just have only one host. I\nknow I could have mentioned either the only one IP address directly or even given the\nname of my group but I've written just all your so that you can know that if you had\nmany group names or you have many notes and you want this Playbook to run on all of your\nnode machines, you can use this all and this Playbook will get deployed on all your note\nmachines. So this is another way of mentioning your hosts and I'm using remote user root\nand this is another method to escalate your privileges. It's similar to become and become\nuser. So on the remote user to have root privileges while this Playbook would run and then the\nlist of the tasks and so what I'm doing in this Playbook is that since I have to connect\nto my MySQL server, which is present in my note machine. I need a particular software\nfor that which is the MySQL python module and I'm Download and install it using tip\nnow dip is the python package manager with which you can install and download python\npackages. But first, I need to install Pippin my note machine. So since I told you that\nthe tasks that you write in a Playbook it gets executed in the same order that you write\nthem. So my first task is to install pip and then I'm using the app module here here. I've\ngiven the name of the package which is python bit and state equal to present and after that.\nI'm installing some other software's using bit and I'm stalling some other related software's\nas well. I'm also installing Library - with blind deaf. And after that using pip, I'm\ninstalling the MySQL python module now notice that so you can consider this as an orchestration\nPlaybook because here I'm making sure that pip has to get installed first and after papers\ninstalled I'm using pip to install another python package. So you see what we did here\nright and then I'm going to use the database modules for Getting a new user to access the\ndatabase and then I'm creating the database named a do so for creating a MySQL user. I've\nused the MySQL user database module that ansible comes with and this is the syntax of the MySQL\nuser module recreate the name of the new user which is edureka, you mentioned the password\nand the preview here. It means what privileges do you want to give it to the new user and\nhere I'm granting all privileges for all database. And since you're creating it for the first\ntime and you want state to be present. Similarly, I'm using the mysqldb module to create a database\nin my MySQL server named ed you so this is the very simple syntax of using mysqldb module.\nWe have to just give the name of the database in DB equal to and state equal to present.\nSo this will create a database named Eddie also and after that I also need to create\na table inside the database for storing my name and email details, right and and unfortunately\nansible does not have any MySQL table creating modules. So what I did is that I've used a\nCommand Module here. We Command Module and directly going to use my SQL queries to create\na table and the syntax is something like this so you can write it down or remember it if\nyou want to use it. So for that since I'm writing a MySQL Query I started with mySQL\nuser Eddie wake up the - us for the user and then for password Etc. Wake up. Now after\n- e just write the query that you need to execute on the MySQL server and write it in\nsingle quotations. So I have written the query to create a table and this is create table\nare EG the name the email and then after that just mention the name of the database on which\nyou want to create this table, which is a do for me. So this is my orchestration PlayBook.\nClear my screen first. The command is ansible - Playbook and the name of your play book,\nwhich is MySQL modding. And here we go. Again, my common return values tell me that the Playbook\nwas done successfully because there are no fail task and no unreachable host and there\nare change task in my note machine. So now all the packages are downloaded now, my node\nmachine is well provisioned. It's properly orchestrated. Now. What are we waiting for?\nLet's deploy your application. Well clear the screen first. So now let me tell you what\nexactly do we need to do in order to deploy my application and in my case, these are just\nthree PHP files and HTML files that I need to deploy it on my Note machine in order to\ndisplay this HTML files and PHP files on my web server in my note machine. What I need\nto do is that I need to copy this files from my control machine to the proper location\nin my notebook machine and we can do that using playbooks. So let me just show you the\nPlaybook to copy files. And the name of my father is deployed website. So this is my\nplaybook to deploy my application and here again, I've used the three dashes and then\nthe name of my playbook is copy the host as you know that it's going to be test servers.\nI'm using privilege escalation again, and I'm using become and become user Again The\nGather facts again true. And here is the list of the task the task is to just copy my file\nfrom my control machine and paste it in my destination machine, which is my node machine\nand for that and for copying I've used a copy module and copy module is a file module that\nansible comes with so this is the syntax of the copy module here. You just need to mention\na source and source is the path where my file is contained in my control machine, which\nis home at Eureka documents. And the name of the file is index dot HTML, and I wanted\nto go too far www HTML and it's index dot HTML, so I should be copying my files. Into\nthis location in order for it to display it on the web page and similarly have copied\nmy other PHP files using the same copy module. I've mentioned the source and destination\nand copying them to the same destination from the same source. So I don't think any of you\nwould have questions here. This is the most easiest Playbook that we have written today.\nSo let us deploy our application now and for that we need to run this play book and before\nthat we need to clear the screen because there are a lot of stuff on our screen right now.\nSo let's run the Playbook. And here we go, and it was very quick because there was nothing\nmuch to do. You just have to copy files from one location to another and these are very\nsmall files. Let us go back to our host machine and see if it's working. So you're back again\nat our host machine. Let's go to my web browser to check that. So let me refresh it and there\nit is. And so here is my first web page. So my application was successfully deployed.\nSo now let us enter our name and email here and check if it is getting entered in my database.\nSo let's put our name and the email. It's why z.com and add it so new record created\nsuccessfully. It means that it is getting inserted into my database. Now, let's go back\nand view it and there it is. So congratulations, you have successfully written playbooks to\ndeploy your application your provision your node machines using playbooks and orchestrated\nthem using playbooks now, even though at the beginning it seemed like a huge task to do\nand so we'll play both made it so easy. Hello everyone. This is Saurabh from Edureka in\ntoday's session will focus on what his puppet. So without any further Ado let us move forward\nand how look at the agenda for today first. We'll see why we need configuration management\nwhile the various problems are industries were facing before configuration management\nwas introduced after that will understand what exactly is configuration management and\nwe'll look at various configuration management tools after We'll focus on puppet and we'll\nsee the puppet architecture along with the various puppet components and finally in our\nhands on part will learn how to deploy my SQL and PHP using puppet. So I'll move forward\nand we'll see what are the various problems before configuration management. So this is\nthe first problem guys, let us understand this with an example suppose. You are a system\nadministrator and your job is to deploy mean stack say on four nodes. All right means dark\nis actually Mongo DB Enterprise angularjs and node.js so you need to deploy means dark\non four notes that is not a big issue. You can manually deploy that and four nodes but\nwhat happens when your infrastructure becomes huge you may need to deploy the same means\ntax a on hundreds of notes. Now, how will you approach the task? You can't do it manually\nbecause if you do it manually, it'll take a lot of time plus they will be wastage of\nresources along with that. There is a chance of human error. I mean, it increases the risk\nof human error. All right, so we'll take the same example forward. And we'll see what are\nthe other problems before configuration management. Now, this is the second problem guys. So it's\nfine like you in the previous step you have deployed means that one hundreds of nodes\nmanually. Now what happens there is an updated version of Mongo DB available and your organization\nwants to shift that updated version. Now, how will you do that? You want to go to the\nupdated version of Mongo DB? So what you'll do you'll actually go and manually update\nmongodb on all the nodes in your infrastructure. Right? So again, that will take a lot of time\nbut now what happens that updated version of the software has certain glitches your\ncompany wants to roll back to the previous version of the software, which is mongo DB\nin this case. So you want to go back to the previous version. Now, how will you do that?\nRemember you have not kept the historical record of Mongo DB during the updating. I\nmean you have updated mongodb biannually on all the nodes. You don't have the record of\nthe previous version of Mongo DB. So what you need to do you need to go and manually\nReinstall mongodb on all the nodes. So rollback was a very painful task. I mean it used to\ntake a lot of time. Now. This is the third problem guys over here what happens you have\nupdated mongodb in the previous step on say development environment and in the testing\nenvironment, but when we talk about the production environment, they're still using the previous\nversion of mongodb. Now what happens there might be certain applications that work that\nare not compatible with the previous version of mongodb All right. So what happened developers\nwrite a code and that works fine in his own environment or beat his own laptop after that.\nIt works fine till testing is well. Now when it reaches production since they're using\nthe older version of Mongo DB which is not compatible with the application that developers\nhave built so it won't work properly there might be certain functions which won't work\nproperly in the production environment. So there is an inconsistency in the Computing\nenvironment due to which the application might work in the development environment, but in\nproduct it is not working properly. Now what I'll do, I'll move forward and I'll tell you\nhow important configuration management is with the help of a use case. So configuration\nmanagement. Add New York Stock Exchange. All right. This is the best example of configuration\nmanagement that I can think of what happened a software glitch prevented the New York Stock\nExchange from Trading stocks for almost 90 minutes this led to millions of dollars of\nloss a new software installation caused the problem. The software was installed on 8 of\nits twenty trading Terminals and the system was tested out the night before however in\nthe morning it failed to operate properly on the a terminals. So there was a need to\nswitch back to the old software you might think that this was a failure of New York\nStock Exchange has configuration management process, but in reality, it was a success\nas a result of proper configuration management process NYSE recovered from that situation\nin 90 minutes, which was pretty fast. Let me tell you guys had the problem continued\nlonger the consequences would have been more severe so because the proper configuration\nmanagement, New York Stock Exchange Painted loss of millions of dollars they were able\nto roll back to the previous version of the software within 90 minutes. So we'll move\nforward and we'll see what exactly configuration management is. So what is configuration management\nconfiguration management is basically a process that helps you to manage changes in your infrastructure\nin a more systematic and structured way. If you're updating a software you keep a record\nof what all things you have updated. What will change is you are making in your infrastructure\nall those things and how you achieve configuration management you achieve that with the help\nof a very important concept called infrastructure as code. Now. What is the infrastructure is\ncode infrastructure as code simply means that you're writing code for infrastructure. Let\nus refer the diagram that is present in front of your screen. Now what happens in infrastructure\nis code you write the code for infrastructure in one central location. You can call it a\nserver. You can call it a master or whatever you want to call it. All right. Now that code\nis deployed onto the dev environment test environment and the product environment. Basically\nyour entire infrastructure. All right, whatever. No, do you want to configure your configure\nthat with the help of that one central location? So let us take an example. All right suppose\nyou want to deploy Apache Tomcat say on all of your notes. So what you'll do in one location\nwill write the code to install Apache tomcat and then you'll push that onto the nodes which\nyou want to configure. What are the advantage you get here. First of all the first problem\nif you can recall that configuring large infrastructure was very hectic job, but because of configuration\nmanagement, it becomes very easy how it becomes easy. You just need to write the code in one\ncentral location and replicate that on hundreds of notes it is that easy. You don't need to\ngo and manually install or update the software on all the nodes. All right. Now the second\nproblem was you cannot roll back to the previous table version in time. But what happens here,\nsince you have everything well documented in the central location rolling back to the\nprevious version was not a time-consuming task. Now the third problem was there was\na variation or inconsistency in Various teams, like Dev team Testament product team like\nthe environment the Computing environment was a different in-depth testing product.\nBut with the help of infrastructure as code what happens all your three environment that\nis there tested product have the same Computing environment. So I hope we all are clear with\nwhat is configuration management and what is infrastructure is code. So we'll move forward\nand we'll see what are the different type of configuration management approaches are\nthere now, there are two types of configuration management approaches one is push configuration.\nAnother is pull configuration. All right. Let me tell you push configuration first input\nconfiguration what happens there's one centralized server and it has all the configurations inside\nit if you want to configure certain amount of nodes. All right, say you want to configure\nfor notes as shown in the diagram. So what happens if you push those configuration to\nthese nodes there are certain commands that you need to execute on that particular central\nlocation and with the help of that command those are configurations, which are present\nwill be pushed onto the nodes now, Let us see what what happens in pull configuration\nin pull configuration. There is one centralized server, but it won't push all the configurations\non to the nodes what happens nodes actually posed the central server at say 5 minutes\nor 10 minutes basically at periodic intervals. All right, so it will pose the central servers\nfor the configurations and after that it will pull the configurations that are there in\nthe central server so over here, you don't need to execute any command nodes will add\nautomatically pull all the configurations that are there in the centralized server and\npop it in Chef both uses full configuration. But when you talk about push configuration\nansible unsourced accuses push configuration, so I'll move forward and we'll look at various\nconfiguration management tools. So these are the four of most widely adopted tools for\nconfiguration management. I have highlighted puppet because in this session, we are going\nto focus on puppet and it uses pull configuration and when we talk about Saul stock, it uses\npush configuration, so does ansible ansible also uses push. Listen Chef also uses the\npulley configuration. All right, so pop it and chef uses pull configuration, but ansible\nand solve Stark uses push configuration. Now, let us move forward and see what exactly puppet\nis. So pop it is basically a configuration management tool that is used to deploy a particular\napplication configure your nodes and manager service. Like they can possibly take your\nservers online and offline as required configure them and deploy a certain package or an application\nonto the node. So right with the help of puppet, you can do that with ease and the architecture\nthat it uses master-slave architecture. Let us understand this with an example. So this\nis Puppet Master over here. All the configurations are present and these are all the puppet agents.\nAll right, so these puppet agents pole the central or the Puppet Master at regular intervals\nand whatever configurations are present. It will pull those configuration basically. So\nlet us move forward and focus on the Puppet Master Slave architecture now, this is a Also\nslave architecture guys over here what happens the puppet agent or the puppet node sends\nfacts to the puppet master and these facts are basically a key value our data pair that\nrepresents some aspect of slave state that aspect can be its IP address time operating\nsystem or whether it's a virtual machine and then Factor gathers those basic information\nabout puppet slave such as Hardware details network settings operating system type and\nversion IP addresses Mark addresses all those things. Now these parts are then made available\nin Puppet Masters manifest as variables now Puppet Master uses those facts that it has\nreceived from the puppet agent or the puppet node to compile a catalog that catalog defines\nhow the slave should be configured and at the catalog is a document that describes a\ndesired state for each resource that Puppet Master manages, honestly, so it is basically\na compilation of all the resources that Puppet Master applies to a given slave as well as\nat the relationship between Those resources so the catalog is compiled by the puppet master\nand then it is sent back to the node and then finally slave provides data about how it has\nimplemented that catalog and if sandbags our report. So basically the node or the agent\nsends the report back that the configurations are complete and they can actually view that\nin the puppet dashboard as well. Now what happens is the connection between the node\nor the puppet agent and the puppet master happens with the help of SSL secure encryption.\nAll right, we'll move forward and we'll see how actually the connection between the puppet\nmaster and puppet node happens. So this is how puppet master and slave connection happens\nwhat happens first of all the puppets slave it requests for the Puppet Master certificate.\nAll right. It sends a request to the master certificate and once Puppet Master receives\nthat request it will send the master certificate and once puppet slave has received the master\ncertificate Puppet Master will again send a request to the slave regarding the its own\ncertificate. All right. So it will request a for the puppet agent to send its own certificate.\nThe puppet slave is generate its own certificate and send it to Puppet Master. Now what puppet\nmaster has to do puppet master has to sign that certificate. Alright. So once it has\nsigned the certificate puppet slave can actually request for the data. All right all the configurations\nand then finally Puppet Master will send those configurations on to the puppets late. This\nis how puppet master and slave communicates. Now, let me show you practically how this\nhappens. I have installed puppet master and puppet slave on my sent to West machines.\nAll right, I'm using 2 virtual machines 14 puppet master and another for puppet sleep.\nSo let us move forward and execute this practically now, this is my Puppet Master virtual machine\nover here. I've already created a puppet master certificate, but there is no puppet agent\ncertificate right now and how will you confirm that there is a command that is puppet. Third\nlist and it will display all the certificates that are pending in puppet master. I mean\nthat are pending for the approval from the master. All right, so currently there are\nno certificates available. So what I'll do is I'll go to my puppet agent and I'll fetch\nthe Puppet Master certificate which are generated earlier and at the same time generate the\npuppet agent certificate and send it to master for signing it. So this is my puppet agent\nvirtual machine now over here as I've told you earlier as well. I'll generate a puppet\nagent certificate and at the same time I'll fetch the Puppet Master certificate and that\nagent certificate will be sent to puppet master and it will sign that puppet my agent certificate.\nSo let us proceed with that for that. I'll type up it agent - t and here we go. All right,\nso it is creating a new SSL key for the puppet agent as you can see in the logs itself. So\nit has sent a Certificate request and this is the fingerprint for that. So exiting no\ncertificate found and wait for sword is disabled. So what I need to do is I need to go back\nto my Puppet Master virtual machine and the signed this particular certificate that is\ngenerated by puppet agent. Now over here if you want to see the list of certificates,\nwhat do you need to do? You need to type up it so at least I have told you earlier as\nwell. So let us see what all certificates are there now, so as you can see that there\nis a certificate that has been sent by puppet agent. All right, so I need to sign this particular\nsort of again. So for that what I will do I'll type pop it. Search sign on the name\nof the certificate that is puppet agent and here we go. So that successfully signed the\ncertificate that was requested by puppet agent. Now what I'll do, I'll go back to my puppet\nagent virtual image and over there. I'll update the changes that have been made in the Puppet\nMaster. Let me first clear my terminal and now again, I'll type puppet agent - tea. All\nright, so we have successfully established a secure connection between puppet master\nand puppet agent. Now. Let me give you a quick recap of what we have discussed a lot first.\nWe saw what are the various problems before configuration management be focused on three\nmajor problems that were there. All right. And after that we saw how important configuration\nmanagement is with the help of a use case of New York Stock Exchange. And finally we\nsaw what exactly configuration management is. And what do you mean by infrastructure\nis code. We also looked at various configuration management tools are namely Chef puppet ansible\nand saltstack and after that we understood what exactly pop it is. And what is the master-slave\narchitecture that it has and how puppet master and puppet slave communicates. All right,\nso I'll move forward and we'll see what use case I have for you today. So what we are\ngoing to do in today's session or we are going to deploy a my SQL and PHP using puppet. So\nfor that what I will do, I'll first a download the predefined modules for my dad. SQL and\nPHP that are there in the puppet Foods. All right, those modules will actually Define\nthe two classes that is PHP and MySQL. Now you cannot deploy the class directly onto\nthe nodes. So what do you need to do? When you in puppet Boniface you need to declare\nthose classes, whatever class you have defined. You need to declare those classes. I'll tell\nyou what our manifest modules you don't need to worry about that. I'm just giving a general\noverview of what we are going to do in today's session. So you just need to declare those\ntwo classes at as PHP and MySQL and finally just deploy that onto the nose it is that\nsimple guys. So as you can see that there will be a code for PHP and MySQL from that\nPuppet Master, it will be deployed onto the nose or the puppet agents will move forward\nand we'll see what are the various phases in which will be implementing the use case.\nAlright. So first we'll define a class has all right classes are nothing but the collection\nof various resources. How will do that will do that with the help of modules that will\nactually download a module from the puppet. Boat and we'll use that module that defines\nwho classes as I've told you PHP and MySQL and then I'm going to declare that class in\nthe Manifest and finally deploy that onto the nodes. All right. So let us move forward\nand before actually doing this it is very important for you to understand certain basics\nof pop it like code basics of puppet like what our classes resources manifest modules\nall those things. So we'll move forward and understand those things one by one. Now. What\nhappens is first of all, I'll explain you resources classes manifests in modules separately.\nBut before that, let me just give you an overview of what are these things? All right, how do\nthey work together? So what happens there are certain resources or write a user is a\nresource of pile is a resource. Basically anything that is there can be considered as\na resource. So multiple resources actually combine together to form a class. So now this\nclass you can declare it in any of the benefits that you want. You can declare it in multiple\nmanifests. All right, and then finally you can bundle all These manifest together to\nform a module. Now. Let me tell you guys it is not mandatory that with you will combine\nthe resources and define a class. You can actually deploy the resources directly. It\nis a good practice if you combine the resources in the form of classes because it becomes\neasier for you to manage the same goes for manifest as well. And I'll tell you how to\ndo that as well. You can write a puppet code and deploy that onto the nodes and at the\nsame time it is not necessary for you to bundle the Manifest that you are using in the form\nof modules. But if you do that, it becomes more manageable and it becomes more structured.\nAll right, so it becomes easier for you to handle multiple manifests. All right. So let\nus move forward and have a look at what exactly are resources and what our class is in puppet.\nNow what our resources anything that is there is a resource a user is a resource other told\nyou about file can be a resource. Basically anything that is there can be considered as\na resource. So puppet code is composed primarily of a resource declarations a resource describes\nsomething about the state of the System it can be such as a certain user or a file should\nexist or a package should be installed now here we have the syntax of the resource. All\nright, first you write the type of the resource. Then you give a name to it in the single quotes\nand various attributes that you want to Define in the example. I've shown you that it will\ncreate a file that is I need d.com and this attribute will make sure that it is present.\nSo let us execute this practically guys. I'll again go back to my Center as virtual machine\nnow over here. What I'll do I'll use the G edit editor you can use whatever editor you\nwant and I'll type the path for my manifest directory and in this directory. I let Define\na file. All right and with the dot DB extension, so I'll just name it as a side dot p p and\nhere we go. Now what head are the resource examples that I've shown you in this light?\nI will just write the same example and the let us see what happens file open the braces\nnow give the path HC. \/ I knit DDOT conf Ina DDOT conf. Colon, and antenna, and now I'm\ngoing to write the attribute, so I'm going to make sure that it is present in sure. The\nDefine is created. Etsy I knit \/ I knit. DDOT conf comma and the now-closed the braces save\nit and close it. Now what you need to do. You need to go to the puppet Asian once more\nand over there. I'm going to execute agent - t command that will update the changes made\nin the Puppet Master. Now we're here. I'll use the puppet agent - t command and let us\nsee if the file I need the dot-coms is created or not. All right, so it has done it successfully\nnow. What I'll do is just to confirm that I'll use LS command for that. I will type\nLS Etsy. Ina DDOT Kant And as you can see that it has been created, right so we have\nunderstood what exactly a resources in puppet, right? So now let us see what our classes\nclasses are nothing but the group of resources. All right, so you group multiple resources\ntogether to form one single sauce and you can declare that class in multiple manifest\nas we have seen earlier. It has a syntax error. Let us see first you need to write class then\ngive a name to that class open the braces write the code in the body and then close\nthe brace is it's very simple and it is pretty much similar to the other coding languages\nthat you if you if you have come across any other coding languages. It is pretty much\nsimilar to the class that you define over there as well. All right, so we have a question\nfrom my uncle he's asking can you specify what exactly the difference between a resource\nand a class classes are actually nothing but the bundle of resources. All right, all those\nResources Group together forms a class and what you can say is a resource describes a\nsingle. Or a package but what happens a class describes everything needed to configure an\nentire service or an application? So we'll move forward and we'll see what our manifest\nso this is puppet manifest now what exactly it is, every slave has got its configuration\ndetails in puppet master and it is written in the native puppet language. These details\nare written in the language that puppet can understand and that language is termed as\nmanifests. So this is Manifest all the puppet programs are basically termed as Manifest.\nSo for example, you can write a manifest in puppet master that creates a file and install\nthe party's over on puppet slaves connected to the Puppet Master. Alright, so you can\nsee I've given you an example over here. It uses a class that is called Apache and this\nclass is defined with the help of predefined modules that are there in puppet port and\nthen various our tributes like Define the virtual hosts in the port and the root directory,\nso Basically, there are two ways to actually declare a class in puppet manifest either.\nYou can just write include and the name of the class or you can if you don't want to\nuse a default attributes of that class, you can make the changes in that by using this\nparticular syntax that is you write the class open the braces and the class name: whatever\nchanges or whatever the attributes that you want apart from the one which are there in\nDeep by default and then finally close the braces. All right. So now I'll execute a manifest\npractically that will install Apache on my notes. All right now need to deploy Apache\nusing puppets. All right. So what I need to do, I need to write the code to deploy apart\na in the Manifest directory. I've already created a file with DOT CPP extension. If\nyou can remember when I was talking about resources, right? So now again, I'll use the\nsame file that is side b p and I'll write the code to deploy a partay. All right. So\nwhat I'll do I'll just I'll use the G editor you can use whatever editor you feel like\nit see Pop It manifest and site. Art p p and here we go. Now over here. I'll just delete\nthe resource that I've defined here. I like my screen to be nice and clean and now I will\nwrite the code to deploy a party so for that I will tight package. httpd : now I need to\nensure it is install. So for that I'll type in sure installed. Give a comma Now I need\nto start this Apache service for that. I'll type service. httpd in short running through\na coma now close the braces the save it and close it. Let me clear my terminal. And now\nwhat I'll do, I'll go to my puppet agent from there. It will pull the configurations that\nare present in my Puppet Master. Now what happens periodically puppet agent actually\npulls the configuration from Puppet Master and it is around 30 minutes, right? It takes\naround half an hour after every half an hour puppet agent pulls the configuration from\nPuppet Master, right so you can configure that as well. If you don't want to do it just\nthrow in a command puppet agent - tea and it will automatically pull the configurations\nare representing the puppet master. So for that I will go to my puppet agent virtual\nmachine now here what I'll do, I'll type a command puppet agent - t and let us see what\nhappens. So it is done now now what I'll do just to confirm that I will open my browser.\nAnd over here, I will type the hostname of my machine which is localhost and let us see\nif a party is installed. All right, so Apache has been successfully installed now, let us\ngo back to our slides and see what exactly modules are. So what our puppet modules puppet\nmodule can be considered as a self-contained bundle of code and data. Let us put it in\nanother way. We can say that puppet module is a collection of manifest and data such\nas Parks files templates Etc. All right, and they have a specific directory structure.\nModules are basically used for organizing your puppet code because they allow you to\nsplit your code into multiple manifest. So they provide you a proper structure in order\nto manage a manifest because in real time, you'll be having multiple manifest to manage\nthose manifests. It is always a good practice to bundle them together in the form of modules.\nSo by default puppet modules are present in the directly \/ HC \/ puppet \/ modules, whatever\nmodules you download from Puppet force will be present in this module directory. All right,\neven if you create your own modules, you have to create in this particular directory. That\nis \/ HC \/ puppet \/ modules. So now let us start the most awaited topic of today's session\nthat is deploying PHP and my SQL using puppet. Now, what I'm going to do is I'm going to\ndownload the two modules one is for PHP and another is for MySQL. So those two modules\nwill actually Define PHP and MySQL class for me now after that I need to declare that class\nin the Manifest. Then site dot PHP file present in the puppet manifest. So I'll declare that\nclass in the Manifest. And then finally, I'll throw in a command puppet agent - teen my\nagent and it will pull those configurations and PHP and MySQL will be deployed. So basically\nwhen you download a module you are defining a class. You cannot directly deploy the class\nyou need to declare it in the Manifest and I will again go back to my sin to icebox now\nover here. What I'll do, I'll download the my SQL module from the puppet forward. So\nforth are all type puppet mode. You'll install Puppet Labs. - my sequel - - give the night\nversion name so I will use three point one zero point zero and here we go. So what is\nhappening here as you can see the saying preparing to install into \/ HC \/ puppet \/ modules, right?\nSo it will be installed in this directories apart from that. It is actually downloading\nthis from the forge a pi dot puppet labs.com. So it is done now, that means that successfully\ninstall MySQL module from Puppet Fort. All right. Let me just clear my terminal and now\nI will install PHP modules for that. I'll type puppet module install. - a PHP - - version\nthat is four point zero point zero - beta 1 and here we go. So it is done. Now that\nmeans we have successfully installed two modules one is PHP and other is my SQL. All right.\nLet me show you where it is present in my machine. So what I'll do, I'll just hit an\nLS command and I'll show you in puppet modules. And here we go. So as you can see that there's\na my SQL module and PHP module that we have just downloaded from Puppet Foods. Now what\nI need to do is I have defined by SQL and PHP class, but I need to declare that in the\nsite dot PHP file present in the puppet manifest. So for that what I will do I'll first use\nthe G edit editor you can use whatever editor that you want. I'm saying it again and again,\nbut you can use whatever editor that you want. I personally prefer G edit and now manifest\nside dot p p and here we go. Now as I told you earlier is well, I like my screen to be\nclean and nice. So I'll just remove this and over here. I will just declare the two classes.\nThat is my secret and PHP. Include my sequel. Server and the next line. I'll include the\nPHP class for that anti PHP. Just save it now close it. Let me clear my terminal now\nwhat I'll do, I'll go to my puppet agent. And from there. I'll hit a command puppet\nagent - t that will pull the configurations from Puppet Master. So let us just proceed\nwith that. Let me first clear my terminal and now I'll tie puppet agent - t and here\nwe go. So we have successfully deployed PHP and MySQL using puppet. All right, let me\njust clear my terminal and I'll just confirm it by typing my sequel - we All right, this\nwill display the version now as just exit from here and now I'll show you the PHP versions\nof adult type PHP - version and here we go. Alright, so this means that we have successfully\ninstalled PHP and MySQL using puppet. So now let me just give you a quick recap of what\nwe have discussed in love. All right. So first we saw why we need configuration management.\nWhat are the various problems that were there before configuration management? And we understood\nthe importance of configuration management with a use case of New York Stock Exchange.\nAll right, after that we saw what exactly configuration management is and we understood\na very important concept called infrastructure as code. Then we focused on various type of\nconfiguration management approaches namely push and pull then we saw various configuration\nmanagement tools are namely puppet chef ansible and Source tag after that. We focus on pop\nit and we saw what exactly puppet is its Master Slave architecture how puppet master and slave\ncommunicates all those things then we understood the puppet code Basics. We understood what\nour resources what a class is Manifest modules and finally in our hands on part. I told you\nhow to deploy PHP and MySQL using puppet My name is Sato. And today we'll be talking about\nNagi ways. So let's move forward and have a look at the agenda for today. So this is\nwhat we'll be discussing. Will Begin by understanding why we need continuous monitoring what is\ncontinuous monitoring and what are the various tools available for continuous monitoring.\nThen we are going to focus on Nagi OS we are going to look at its architecture how it works.\nWe are also going to look at one case study and finally in the demo. I will be showing\nyou how you can monitor a remote host using NRP, which is nothing but nagios remote plug-in\nexecutor. So I hope you all are clear with the agenda. Let's move forward and we'll start\nby understanding why we need continuous monitoring. Well, there are multiple reasons guys, but\nI mentioned for very important reasons why we need continuous monitoring. So let's have\na look at each of these one by one. The first one is failure of see ICD pipelines since\ndevops is a buzzword in the industry right now. And most of the organizations are using\ndevops practices. Obviously, they are implementing see ICD pipelines or it is also called as\ndigital pipelines right now the idea behind these SED pipeline is to make sure that the\nrelease should happen more frequently and it should be more stable in an automated fashion.\nRight because there are a lot of competitors you might have in the market and you want\nto release your product before them. So agility is very very important. And that's why we\nuse eicd pipelines. Now when you implement such a pipeline you realize that there can't\nbe any manual intervention at any step in the process or the entire pipeline slows down.\nSo you will basically defeat the entire purpose manual monitoring slows down your deployment\nPipeline and increases the risk of performance problems propagating in production, right?\nSo I hope you have understood this. If you notice the three points that I've mentioned\nit's pretty self-explanatory rapid introduction of performance problems and errors, right\nbecause you are releasing software and more frequently. So there has to be rapid introduction\nof performance problems rapid introduction of new endpoints causing monitoring issues.\nAgain, this is pretty self-explanatory then the root cause analysis as a number of services\nexpands because you are releasing software more frequently, right? So definitely the\nnumber Services are going to increase and there's a lengthy root cause analysis, you\nknow, because of which you lose a lot of time, right? So let's move forward and we look at\nthe next reason why we need continuous monitoring. For example, we have an application which\nis light, right? We have deployed it on the production server. Now. We are running a p.m.\nSolutions which is basically application performance monitoring. We are monitoring our application\nhow the performance is. Is there any down time all those things? Right? And then we\nfigure out certain issues with our applications on performance issues now to go back basically\nto roll back and to incorporate those changes to remove those bugs developers are going\nto take some time because the process is huge because your application is already live,\nright? You cannot afford any downtime. Now, imagine what if before releasing the software\non a pre production server, which is nothing but the replica of my production server. I\ncan run those APM solutions to figure out how my application is going to perform and\nit actually goes live right so that way whatever issues of their developers will be notified\nbefore and they can take the corrective action. So I hope you have understood my point. The\nnext thing is server Health cannot be compromised at any cost. So I think it's pretty obvious\nguys. Your application is running on a server. You cannot afford any downtime in that particular\nserver or increase in the response time also, right. So you require some sort of a monitoring\nsystem to check your server Health as well. Right? What if your application goes down\nbecause you're so it isn't responding right? So you don't want any scenario like that in\na world like today where everything is so Dynamic, and the competition is growing. Exponentially.\nYou want to give best service to your customers, right? And I think so \/ health is very very\nimportant because that's where your application is running guys are not things. I have to\nstress too much on this right, so we basically require continuous monitoring of a server\nas well. Now, let me just give you a quick recap of the things that we have discussed.\nSo we have understood why we need continuous monitoring by looking at three four examples,\nright? The first thing is we solve what are the issues with see ICD pipeline right? We\ncannot have any sort of manual intervention for monitoring in source of bye. Because you're\ngoing to defeat the purpose of such pipeline. Then we saw that developers have to be notified\nabout the performance issues of the application before releasing it in the market. Then we\nsaw server Health cannot be compromised at any cost. Right? So these are the three major\nreasons why I think continuous monitoring is very important for most of the organization's\nright? Although there are many other reasons as well right now. Let's move forward and\nunderstand what exactly is continuous monitoring because we just talked a lot of scenarios\nwhere Manuel monitoring or a traditional monitoring processes are not going to be enough. Right?\nSo let us understand what exactly is continuous monitoring and how is it different from what\nrelation process so basically continuous monitoring tools resolve any sort of system errors before\nthey have any negative impact on your business. It can be low memory unreachable server, etc.\nEtc. Apart from that. They can also monitor the business processes and the application\nas well as your server which we have just discussed. Right? So continuous monitoring\nis basically an effective system where The entire it infrastructure starting from your\napplication to your business process to your server is monitored in an ongoing way and\nin an automated fashion, right? That's what basically is the Crux of continuous monitoring.\nSo these are the multiple phases given to us by n is T for implementing continuous monitoring\nand is is basically National Institute of Standards and technology. So let me just take\nyou through each of these stages first thing is defined so in to basically develop a monitoring\nstrategy, then what you're going to do you are going to establish measures and Matrix\nand you also going to stablish monitoring and assessment frequencies at how frequently\nare going to monitor it right. Then you are going to implement whatever you have stablished\nthe plan that you have laid down. Then you're going to analyze data and report findings,\nright? So whatever issues that are there you're going to find that pose that you're going\nto respond and mitigate that error and finally you're going to review and update the application\nor whatever you were monitoring right now. Let us move forward and patreon is also given\nus multiple phases involved in continuous monitoring. So let us have a look at those\nold. So one by one The first thing is continuous Discovery. So contentious Discovery is basically\ndiscovering in maintaining near real-time inventory of all networks and information\nassets, including hardware and software if I have to give an example basically identifying\nand tracking confidential and critical data stored on desktops laptops and servers. Right\nnext comes continuous assessment. It basically means automatically scanning and comparing\ninformation assets against industry and data repositories determine oner abilities. That's\nthe entire point of continuous assessment. Right? So one way to do that is prioritizing\nfindings and providing detailed reports right by Department platform Network asset and vulnerability\ntype next comes continuous audit, so continuously evaluating your client server and network\ndevice configurations and comparing them with standard policies is basically what continues\naudit is, right. So basically what you're going to do here is gain insights into problematic\ncontrols using patterns and access permission of sensitive data. Then comes continuous patching.\nIt means automatically deploying and updating software to eliminate vulnerabilities and\nmaintain compliance. Right? So if I have to give you an example may be correcting configuration\nsettings, including network access and provision software according to end users role in policies.\nAll those things next comes continents reporting. So aggregating the scanning results from different\ndepartments scan types and organizations into one Central repository is basically what content\nis reporting is right for automatically analyzing and correlating unusual activities in compliance\nwith regulations. So I think it's pretty easy to understand if I have to repeat it once\nmore I would say continuous Discovery is basically discovering and maintaining an inventory a\nnear real-time inventory of all the network and information assets. Whether it's your\nHardware or software then continuous assessment means automatically scanning and comparing\nthe information assets from Gardens discovery that we have seen against industry and data\nrepositories to determine vulnerabilities continuous audit is basically Continuously\nevaluating your client server and network device with configurations and comparing them\nwith standards and policies Contreras patching is automatically deploying and updating software\nto eliminate vulnerabilities and maintain compliance right patching is basically your\nremedy kind of a thing where you actually respond to the threats that you see or vulnerabilities\nthat you see in your application Garden is reporting is basically aggregating scanning\nresults from different departments scan types are organizations into one Central repository.\nSo these are nothing but the various phases involved in continuous monitoring. Let us\nhave a look at various continents monitoring tools available in the market. So these are\npretty famous tools. I think a lot of you might have heard about these tools one is\nAmazon cloudwatch, which is nothing but a service provided to us by AWS Splunk is also\nvery famous. And we have e LK and argue ways right CLK is basically elastic log stash and\nCabana in this session. We are going to focus on argue is because it's a pretty mature to\nlot of companies have used this tool and it has a major market share as well and it's\nbasically well suited for your entire it Whether it's your application or server or even it's\nyour business process now, let us have a look at what exactly is not your ways and how it\nworks. So now I give which is basically a tool used for continuous monitoring of systems\nyour application your services and business processes Etc in a devops culture right now\nin the event of failure. Nagios can alert technical staff of the problem allowing them\nto begin a remedy ation processes before outages affect business processes and users or customers.\nSo I hope you are getting my point. It can allow the technical staff of the problem and\nthey can begin remediation processes before outages affect their business process or end\nusers or customers right with the argues. You don't have to explain why an answer in\ninfrastructure outage affect your organization's bottom line, right? So let us focus on the\ndiagram that is there in front of your screen. So now use basically runs on a server usually\nas a Daemon or a service and it periodically runs plugins residing in the same server what\nthey do they basically contact hosts on servers or on your network or on the Internet. Now\none can view the status information using the web interface and you can also receive\nemail or SMS notification if something goes wrong, right so basically nagas Damon behaves\nlike a scheduler that runs certain scripts at certain moments. It stores the results\nof those cribs and we'll run other scripts if these results change. I hope you are getting\nmy point here right now. If you're wondering what our plugins of these are nothing but\ncompiled executables or scripts. It can be pearls great shell script Etc that can run\nfrom a command line to check the status of a host or a service noun argue is uses the\nresults from the plugins to determine the current status of the host. And so this is\non your network. Now, let us see various features of Naga ways. Let me just take you through\nall these features one by one. It's pretty scalable and secure and manageable as well.\nIt has a good log in database system. It automatically sends alerts which we just saw it. It takes\nnetwork errors and server crashes. It has easy writing plug-in. You can write your own\nplugins right based on. Requirement yours business need then you can monitor your business\nprocess and it infrastructure with a single pass guys issues can be fixed automatically.\nIf you have configured in such a way then definitely you can fix those issues automatically\nand it also has support for implementing redundant monitoring posts. So I hope you are understood\nthese features there are many more but these are the pretty attractive features and why\nand argue s is so popular is because of these features, let us now discuss the architecture\nof nagios in detail. So basically now argue is has a server agent architecture right now\nusually in a network an argue a server is running on a host which we just saw in the\nprevious diagram, right? So consider this as my host. So now I guess server is running\non a host and plugins interact with local and remote Hood. So here we have plugins.\nSo these will interact with the local resources or services and these will also interact with\nthe remote resources or services or host right. Now. These plugins will send the information\nto the scheduler which will display that in the GUI right now. Let me repeat it. Again.\nNargis is build on a circuit. Good Agent architecture. Right and usually in argue is server is running\non a host and these plugins will interact with the local host or services or even the\nremote host Services. Right? And these plugins will send the information to the scheduler\nnagios process scheduler, which will then display it on the web interface and if something\ngoes wrong the concern teams will be notified Via SMS or through email, right? So I think\nwe have covered quite a lot of theory. So let me just go ahead and open my centralized\nvirtual machine where I've already installed now. Gos, so let me just open my Center as\nvirtual machine first. So this is my Center is virtual machine guys. And this is how the\nnagios dashboard looks like. I'm running it at Port 8000. You can run it wherever you\nwant to explain that in the installation video how you can install it now. If you notice\nthere are a lot of options on the left hand side you can you know, go ahead and play around\nwith it. You'll get a better idea. But let me just focus on few important ones. So here\nwe have a map option here, right? If you click on that, then you can see that you have a\nlocal host and you have a remote host as well. My nagas process is monitoring both the local\nhost and the remote host the remote host is currently down. That's why you see it like\nthis when I will be running it'll be showing you how it basically looks like now if I go\nahead and click on host. You will see all the hoes that I'm currently monitoring some\nmonitoring edureka and Local Host said Eureka is basically a remote server and Local Host\nis currently on which my Onaga server is running right? So obviously it is up at the other\nserver is down. If I click on Services, you can see that these are the services that I'm\nmonitoring for my remote host our monitoring CPU load ping and SSH and for my Local Host.\nI'm watching current load current users HTTP paying root partition SSH swap usage in total\nprocesses. You can add as many services as you want. All you have to do is change the\nhost dot CFG file, which I'm going to show you later. But for now, let us go back to\nour slides will continue from there. So let me just give you a small recap of what all\nthings we have discussed. So we first saw why we need continuous monitoring. We saw\nvarious reasons why Industries need continuous monitoring and how it is different from the\ntraditional monitoring systems. Then we saw what is exactly continuous monitoring and\nwhat are the various phases involved in implementing a continuous monitoring strategy. Then we\nsaw what are the various continuous monitoring tools available in the market and we focus\non argue as we saw what is not gue base how it works? What is its architecture right.\nNow we're going to talk about something called is n RP e nagios remote plug-in executor of\nwhich is basically used for monitoring remote Linux or Unix machines. So it'll allow you\nto execute nagios plugins on those remote machines. Now the main reason for doing this\nis to allow nog you wish to monitor local resources, you know, like CPU load memory\nusage Etc on remote machines now since these public resources are not usually exposed to\nexternal machines and agent like NRP must be installed on the remote Linux or Unix machines.\nSo even I have installed that in my Center ice box, that's why I was able to monitor\nthe remote Linux host that I'm talking about. Also. If you check out my nagas installation\nvideo, I have also explained how you can install NRP now if you notice the diagram here, so\nwhat we have is basically the Jake underscore n RP plug-in residing on the local monitoring\nmachine. This is your local monitoring machine, which we just saw right? So this is where\nmine argue our server is now the Czech underscore in RP plug-in resides in a local monitoring\nmachine where you're not arguing over is right. So the one which we saw is basically my local\nmachine or you can say where my Naga server is, right? So this check underscoring RP plug-in\nresides on that particular machine now this NRP Daemon which you can see in the diagram\nruns on remote machine the remote Linux or Unix machine which in my case was edureka\nif you remember and since I didn't start that machine so it was down right so that NRP Damon\nwill run on that particular machine now, there is a secure socket layer SSL connection between\nmonitoring host and the remote host you can see it in the diagram as well the SSL connection,\nright? So what it is doing it is checking the disk space load HTTP FTP remote services\non the other host site then these are local resources and services. So basically this\nis how an RP Works guys. Do you have and check underscore an Plug in designing in the host\nmachine. You have NRP Daemon running on the remote machine. There's an SSL connection,\nright? Yeah, you have SSL connection and this NRP plug-in basically helps us to monitor\nthat remote machine. That's how it works. Let's look at one very interesting case study.\nThis is from bitten attics. And I found it on the nagios website itself. So if you want\nto check out go ahead and check out their website as well. They have pretty cool case\nstudies the power from Internet Explorer. So there are a lot of other case studies on\ntheir website. So bit etics provides basically Outsource it management and Consulting to\nnonprofit or small to medium businesses right now bitnet has got a project where they were\nsupposed to monitor an online store for an e-commerce retailer with a billion dollar\nannual revenue, which is huge guys. Now, it was not only supposed to you know monitor\nthe store but it also needed to ensure that the cart and the checkout functionality is\nworking fine and was also supposed to check for website deformation and notify the necessary\nstaff if anything went wrong right seems like an easy task but let us see what are the Problems\nthat bitnet X phase now bitnet X hit a roadblock upon realizing that the clients data center\nwas located in New Jersey more than 500 miles away from their staff in New York, right?\nThere was a distance of 500 miles between their their staff is located and the data\ncenter. Now, let us see what are the problems they face because of this now the two areas\nneeded a unique but at the same time a comprehensive monitoring for their Dev test and prod environment\nof the same platform, right and the next challenge was monitoring would be hampered by the firewall\nrestrictions between different applications sites functions Etc. So I think you have a\nlot of you know about this firewalls is basically sometimes can be a nightmare right apart from\nthat most of the notification that were sent to the client what ignored because mostly\nthose are false positive, right? So the client didn't bother to even check those notifications\nnow, what was the solution? So the first solution the thought is adding SSH firewall rules for\nNetwork Operation Center personnel and Equipment second is analyzing web pages to see if there's\nany problem with Occurrences the third and the very important point was converting notification\nto nag, uh alerts and the problem that we saw a false positive was completely removed\nwith this escalation logic. We're converting not as notifications of Nargis alerts and\nescalations with specific time periods for different groups, right? I hope you are getting\nmy point here now configuring event handlers to restart Services before notification, which\nwas basically a fixed for 90% of the issues and using nagios core and multiple servers\nat the NOC facility and each Target is worker was deployed at the application Level with\ndirect access to the host. So whatever bag is worker or agent or remote machine we have\nwas deployed at the application Level and had the direct access to the host or the master\nwhatever you want to call it and they have implemented the same architecture for production\nquality assurance staging and development environments. Now, let's see what was the\nresult now because of this there was a dramatic reduction in notifications. Thanks to the\nevent handlers new configuration. Then there was an increase in up time from 85% Early\n298 personally, which is significant guys, right then they saw a dramatic reduction in\nfalse positive because if the escalation is logic that I was just talking about then fourth\npoint is estimating the need to log into multiple boxes and change configuration file. Thanks\nto nagas configuration maintained in a central repository and post automatically to appropriate\nservice fourth point is estimating the need to log into multiple boxes and change the\nconfiguration files and that happens because the inauguration configuration maintained\nin a central repository or essential master and can be pushed automatically to all these\nslaves to all the servers are slaves are agents whatever you want to call it. So this was\na result of using nog u.s. Right now is the time to check out a demo where what I'll be\ndoing is I'll be monitoring couple of services actually more than a couple of services offer\nremote Linux machine through mine argue Ace hose which I just showed you right? So from\nthere, I'll be monitoring a remote Linux host Caldera Rekha, and I'll be monitoring like\n34 Services you can have whatever you want and let me just show you watch the process\nonce you have installed. I guess what you need to do in order to make sure that you\nhave remote host or a remote machine being monitored by your nagios host. Now in order\nto execute this demo, which I'm going to show you. You must have lamp stack on your system.\nRight Linux Apache MySQL and PHP and I'm going to use Center West 7 here. Let me just quickly\nopen my Center as virtual machine and we'll proceed from there. So guys, this is my sent\nto us virtualbox where I've already installed argue as I've told you earlier as well in\nthis is where mine argue is host is running or you can see the NOG your server is running\nand you can see the dashboard in front of your screen as well. Right? So let me just\nquickly open the terminal first me clear the screen. So let me just show you where I've\ninstalled argue is that this is the path right? If you notice in front of your screen, it's\nin user local Nagi OS what I can do is just clear the screen and I'll show you what our\nlaw directories are inside this so we can go inside this Etsy directory. And inside\nthis I'm going to go inside the objects directory, right? So why I'm doing this is basically\nif I want to add any command for example Ample I want to add the check underscore n RP command.\nThat's how I'm going to monitor my remote Linux host if you remember in the diagram,\nright? So that's what I'm going to do. I'm going to add that particular command. I've\nalready done that. So let me just show you how it looks so just type generator you can\nchoose whatever editor that you like and go inside the commands dot CFG file and let me\njust open it. So these are the various commands that I was talking about. Now, you can just\nhave a look at all these commands. This is to basically notify host a by email if anything\ngoes down anything goes wrong in the host. This is for service. Basically it'll notify\nif there's any problem with the service through email. This will check if my host machine\nis alive. I mean, is it up and running now this command is basically to check the disk\nspace like the local disk, then load rights. You can see all of these things here swap\nFTP. So I've added these commands and you can have a look at all of these commands which\nI've mentioned here and the last command you see is I've added manually because all these\ncommands once you install your get it by default, but the IP take underscore n RP which I'm\nhighlighting right now with my cursor is something which I have added in order to make sure that\nI will monitor the remote clinics horse. Now, let me just go ahead and save this right.\nLet me clear my screen again and I'll go back to my nagios directory. Let me share my screen\nagain now, basically what this will do is this will allow you to use a check and the\nscore an RP command in you're not give service definitions right. Now. What we need to do\nis update the NRP configuration file. So use your favorite editor and open NR P dot c f\ng which you will find in this particular directory itself. So all I have to do is first I'll\nhit LS and then I can just check out the set C directory. Now if you notice there is an\nNR P dot CFG file, right? I've already added it. So I'll just go ahead and show you what\nthe help of G edit or you can use whatever editor that you prefer now over here. You\nneed to find this allowed host directive and add the private IP address of your Nas device\nover to the gamma delimited list is Scroll down you will find something all allowed host.\nRight? So just add a comma and start with the IP address of the machine that you want\nto monitor So currently let me just open it once more. So I'm going to use sudo because\nI don't have the Privileges now in this allowed host directory. All I have to do is comma\nand the IP address of the host said I want to monitor so it is one. Ninety two dot one\nsixty eight dot 1.21. Just go ahead save it come back clear the terminal now save and\nexit. Now this configures in RP to accept requests from your Nas device over why it's\nprivate IP address, right and then just go ahead and restart NRP to put the changes into\neffect now on you and argue server. You need to create a configuration file for each of\nthe remote host that you monitor as I was mentioning before is well now where you're\ngoing to find it in HC servers directory and let me just go ahead and open that for you.\nLet me go to the server's directory. Now if you notice here, there is a deer a card or\nCFG file. This is basically the host. We'll be monitoring right now. If I go ahead and\nshow you what I have written here is basically first what I have done is I have defined the\nhost. It's basically a Linux server and the name of that. So what is Eddie raker allies?\nWhatever you want to give this is the IP address maximum check attempts the periods. I want\nto check it 24\/7 notification interval is what I have mentioned here and notification\nperiod so this is basically about all my host now in that hose what all services are going\nto monitor our new monitor generic services, like pink then I want to monitor SSH then\nI'm going to monitor CPU load is when these are the three services that I'll be monitoring\nand you can find that in your side C. So was that a tree over there? You have to create\na proper configuration file for all of the hose that you want to monitor Let Me Clear\nMy terminal again the just to show you. My remote machine is well, let me just open that.\nSo this is my remote machine guys over here. I've already installed NRP so over here, I'm\njust going to show you how you can restart an RP systemctl restart. And rpe service and\nhere we go the asking for the password. I've given that a man not a piece of its has started\nactually have restarted again. I've already started it before as well. Let me just show\nyou how my nagios dashboard looks like in my server. Now. This is my dashboard again.\nIf I go to my host tab, you can see that we are monitoring to host a dinner a kind localhost.\nErica is the one which I just showed you which is up and running right? I can go ahead and\ncheck out this map Legacy map viewer as well which basically tells me that my a direct\nas remote host then also I have various sources that are monitoring. So if you remember I\nwas monitoring CPU load ping and SSH which you can see it over here as well. Right? So\nthis is all it for today's session. I hope you guys have enjoyed listening to this video.\nIf you have any questions, you can go ahead and mention that in the comment section. And\nif you're looking to gain hands-on experience and devops, you can go ahead and check out\nour website www.guitariq.com \/ devops. You can view upcoming patches and enroll for the\nThat will set you on the path of becoming a successful devops engineer, and if you're\nstill curious to know more about the divorce roles and responsibilities, you can check\nout the videos mentioned in the description. Thank you and happy learning.",
        "videoTranscriptLog":"english:['en']. "
    },
    {
        "channelId":"UC8butISFwT-Wl7EV0hUK0BQ",
        "channelName":"freeCodeCamp.org",
        "videoId":"fqMOX6JJhGo",
        "videoTitle":"Docker Tutorial for Beginners - A Full DevOps Course on How to Run Applications in Containers",
        "videoPublishYear":2019,
        "videoPublishMonth":8,
        "videoPublishDay":16,
        "videoPublishTime":"13:48:15",
        "videoPublishedOn":"2019-08-16T13:48:15Z",
        "videoPublishedOnInSeconds":1565963295,
        "videoViewCount":2725452,
        "videoLikeCount":45966,
        "videoCommentCount":1153,
        "videoCategoryId":27,
        "videoDefaultAudioLanguage":"en",
        "videoDuration":"PT2H10M19S",
        "videoDurationInSeconds":7819,
        "videoContentType":"Video",
        "videoDimension":"2d",
        "videoDefinition":"hd",
        "videoCaption":"true",
        "videoLicensedContent":true,
        "videoProjection":"rectangular",
        "channelCustomUrl":"@freecodecamp",
        "channelPublishYear":2014,
        "channelPublishMonth":12,
        "channelPublishDay":16,
        "channelPublishTime":"21:18:48",
        "channelPublishedOn":"2014-12-16T21:18:48Z",
        "channelPublishedOnInSeconds":1418764728,
        "channelCountry":"US",
        "channelViewCount":854650390,
        "channelSubscriberCount":10700000,
        "channelVideoCount":1839,
        "videoPublishedWeekDay":"Friday",
        "videoDurationClassification":"Extended",
        "channelAgeInYears":10.3485285705,
        "channelNormalizedViewCount":1.0,
        "channelNormalizedSubscriberCount":0.9067796215,
        "channelNormalizedVideoCount":0.1042422868,
        "channelNormalizedChannelAge":0.5316292476,
        "channelGrowthScore":149.141691497,
        "videoAgeInDays":2073.5258101852,
        "videoViewsPerDay":1314.4046653763,
        "videoLikeToViewRatio":0.0168654594,
        "videoCommentToViewRatio":0.0004230491,
        "videoEngagementScore":65771.6757451459,
        "channelGrowthScoreRank":2,
        "videoEngagementScoreRank":2,
        "country_code":"US",
        "country_name":"United States",
        "continent":"North America",
        "continent_code":"NA",
        "it_hub_country":"Yes",
        "videoTranscript":"Hello and welcome to the Docker for beginners\u00a0\ncourse. My name is moonshot 100, and I will be\u00a0\u00a0 your instructor for this course. I'm a DevOps\u00a0\nand cloud trainer at code cloud comm, which is\u00a0\u00a0 an interactive hands on online learning platform.\u00a0\nI've been working in the industry as a consultant\u00a0\u00a0 for over 13 years and have helped hundreds of\u00a0\n1000s of students learn technology in a fun and\u00a0\u00a0 interactive way. In this course, you will learn\u00a0\nDocker through a series of lectures that use\u00a0\u00a0 animation illustration, and some fun analogies\u00a0\nthat simplify complex concepts with demos that\u00a0\u00a0 will show you how to install and get started with\u00a0\nDocker. And most importantly, we have hands on\u00a0\u00a0 labs that you can access right in your browser.\u00a0\nI will explain more about it in a bit. But first,\u00a0\u00a0 let's look at the objectives of this course.\u00a0\nIn this course, we first try to understand what\u00a0\u00a0 containers are, what Docker is, and why you might\u00a0\nneed it and what it can do for you. We will see\u00a0\u00a0 how to run a Docker container how to build your\u00a0\nown Docker image, we will see networking in Docker\u00a0\u00a0 and how to use Docker compose what Docker registry\u00a0\nis how to deploy your own private registry. And we\u00a0\u00a0 then look at some of these concepts in depth. And\u00a0\nwe try to understand how Docker really works under\u00a0\u00a0 the hood. We look at Docker for Windows and Mac\u00a0\nbefore finally getting a basic introduction to\u00a0\u00a0 container orchestration tools like Docker, swarm,\u00a0\nand Kubernetes. Here's a quick note about hands on\u00a0\u00a0 labs. First of all, to complete this course, you\u00a0\ndon't have to set up your own labs. Well, you may\u00a0\u00a0 set it up if you wish to, if you wish to have your\u00a0\nown environment, and we have a demo as well. But\u00a0\u00a0 as part of this course, we provide real labs that\u00a0\nyou can access right in your browser anywhere,\u00a0\u00a0 anytime and as many times as you want. The labs\u00a0\ngive you instant access to a terminal to a Docker\u00a0\u00a0 host and an accompanying quiz portal. The quiz\u00a0\nportal asks a set of questions such as exploring\u00a0\u00a0 the environment and gathering information. Or you\u00a0\nmight be asked to perform an action such as run\u00a0\u00a0 Docker container. The quiz portal then validates\u00a0\nyour work and gives you feedback instantly. every\u00a0\u00a0 lecture in this course is accompanied by\u00a0\nsuch challenging interactive quizzes that\u00a0\u00a0 makes learning Docker a fun activity. So I hope\u00a0\nyou're as thrilled as I am to get started. So let\u00a0\u00a0 us begin. We're going to start by looking at\u00a0\na high level overview on why you need Docker,\u00a0\u00a0 and what it can do for you. Let me start by\u00a0\nsharing how I got introduced to Docker. And one\u00a0\u00a0 of my previous projects, I had this requirement\u00a0\nto set up an end to end application stack,\u00a0\u00a0 including various different technologies, like\u00a0\na web server using node j s, and a database such\u00a0\u00a0 as MongoDB, and a messaging system like Redis,\u00a0\nand an orchestration tool like Ansible, we had\u00a0\u00a0 a lot of issues developing this application stack\u00a0\nwith all these different components. First of all,\u00a0\u00a0 their compatibility with the underlying OS was an\u00a0\nissue, we had to ensure that all these different\u00a0\u00a0 services were compatible with the version of OS\u00a0\nwe were planning to use. There have been times\u00a0\u00a0 when certain version of the services were not\u00a0\ncompatible with the OS. And we've had to go back\u00a0\u00a0 and look at different OS that was compatible with\u00a0\nall of these different services. Secondly, we had\u00a0\u00a0 to check the compatibility between the services\u00a0\nand the libraries and dependencies on the OS.\u00a0\u00a0 We've had issues where one service requires one\u00a0\nversion of a dependent library, whereas another\u00a0\u00a0 service requires another version, the architecture\u00a0\nof our application changed over time, we've had to\u00a0\u00a0 upgrade to newer versions of these components, or\u00a0\nchange the database, etc. And every time something\u00a0\u00a0 changed, we had to go through the same process\u00a0\nof checking compatibility between these various\u00a0\u00a0 components, and the underlying infrastructure.\u00a0\nThis compatibility matrix issue is usually\u00a0\u00a0 referred to ask the matrix from hell. Next, every\u00a0\ntime we had a new developer on board, we found it\u00a0\u00a0 really difficult to set up a new environment,\u00a0\nthe new developers had to follow a large set\u00a0\u00a0 of instructions and run hundreds of commands to\u00a0\nfinally set up their environments, we had to make\u00a0\u00a0 sure they were using the right operating system,\u00a0\nthe right versions of each of these components.\u00a0\u00a0 And each developer had to set all that up\u00a0\nby himself each time. He also had different\u00a0\u00a0 development tests and production environments. One\u00a0\ndeveloper may be comfortable using one or less and\u00a0\u00a0 the others may be comfortable using another one.\u00a0\nAnd so we couldn't guarantee that the application\u00a0\u00a0 that we were building would run the same way in\u00a0\ndifferent environments. And so all of this made\u00a0\u00a0 our life in developing better Building and\u00a0\nshipping the application really difficult.\u00a0\u00a0 So I needed something that could help us with the\u00a0\ncompatibility issue. And something that will allow\u00a0\u00a0 us to modify or change these components without\u00a0\naffecting the other components and even modify the\u00a0\u00a0 underlying operating systems as required. And\u00a0\nthat search landed me on Docker. with Docker,\u00a0\u00a0 I was able to run each component in a\u00a0\nseparate container with its own dependencies,\u00a0\u00a0 and its own libraries, all on the same VM\u00a0\nand the OS, but within separate environments,\u00a0\u00a0 or containers. We just had to build the Docker\u00a0\nconfiguration once and all our developers could\u00a0\u00a0 now get started with a simple Docker run command.\u00a0\nirrespective of what the underlying operating\u00a0\u00a0 system they're on. All they needed to do was\u00a0\nto make sure they had Docker installed on their\u00a0\u00a0 systems. So what are containers, containers\u00a0\nare completely isolated environments. As in\u00a0\u00a0 they can have their own processes or services,\u00a0\ntheir own network interfaces, their own mounts,\u00a0\u00a0 just like washing machines, except they all share\u00a0\nthe same OS kernel. We will look at what that\u00a0\u00a0 means in a bit. But it's also important to note\u00a0\nthat containers are not new with Docker containers\u00a0\u00a0 have existed for about 10 years now and some of\u00a0\nthe different types of containers are Aleksey LSD\u00a0\u00a0 like CFS, etc. Docker utilizes Aleksey containers.\u00a0\nSetting up these container environments is hard as\u00a0\u00a0 they are very low level and that is where Docker\u00a0\noffers a high level two, with several powerful\u00a0\u00a0 functionalities making it really easy for end\u00a0\nusers like us. To understand how Docker works,\u00a0\u00a0 let us revisit some basic concepts of operating\u00a0\nsystems First, if you look at operating systems\u00a0\u00a0 like Ubuntu, Fedora, Susi or CentOS, they all\u00a0\nconsist of two things, an OS kernel and a set\u00a0\u00a0 of software. The OS kernel is responsible for\u00a0\ninteracting with the underlying hardware, while\u00a0\u00a0 the OS kernel remains the same, which is Linux. In\u00a0\nthis case, it's the software above it that makes\u00a0\u00a0 these operating systems different. This software\u00a0\nmay consist of a different user interface drivers,\u00a0\u00a0 compilers, file managers, developer tools, etc. So\u00a0\nyou have a common Linux kernel shared across all\u00a0\u00a0 OSS and some custom software that differentiate\u00a0\noperating systems from each other. We said earlier\u00a0\u00a0 that Docker containers share the underlying\u00a0\nkernel. So what does that actually mean? Sharing\u00a0\u00a0 the kernel? Let's say we have a system with an\u00a0\nUbuntu OS with Docker installed on it. Docker\u00a0\u00a0 can run any flavor of OS on top of it, as long\u00a0\nas they're all based on the same kernel. In this\u00a0\u00a0 case, Linux. If the underlying OS is Ubuntu,\u00a0\nDocker can run a container based on another\u00a0\u00a0 distribution like Debian Fedora Susi or CentOS\u00a0\neach Docker container only has the additional\u00a0\u00a0 software that we just talked about in the\u00a0\nprevious slide that makes these operating systems\u00a0\u00a0 different. And Docker utilizes the underlying\u00a0\nkernel of the Docker host, which works with all\u00a0\u00a0 OSS above. So what is an OS that do not share the\u00a0\nsame kernel as this windows. And so you won't be\u00a0\u00a0 able to run a Windows based container on a Docker\u00a0\nhost with Linux on it. For that you will require\u00a0\u00a0 Docker on a Windows Server. Now it is when I say\u00a0\nthis, that most of my students go, Hey, hold on\u00a0\u00a0 there. That's not true. And they install Docker\u00a0\non Windows, run a container based on Linux and go\u00a0\u00a0 see it's possible. Well, when you install Docker\u00a0\non Windows and run a Linux container on Windows,\u00a0\u00a0 you're not really running a Linux container on\u00a0\nWindows, Windows runs a Linux container on a Linux\u00a0\u00a0 virtual machine under the hoods. So it's really\u00a0\na Linux container on Linux virtual machine on\u00a0\u00a0 Windows. We discuss more about this on the Docker\u00a0\non Windows or Mac later during this course. Now,\u00a0\u00a0 you might ask, isn't that a disadvantage then not\u00a0\nbeing able to run another kernel? On the OS? The\u00a0\u00a0 answer is no. Because unlike hypervisors,\u00a0\nDocker is not meant to virtualize and run\u00a0\u00a0 different operating systems and kernels on the\u00a0\nsame hardware. The main purpose of Docker is\u00a0\u00a0 to package and containerize applications and\u00a0\nto ship them and to run them anywhere anytime,\u00a0\u00a0 as many times as you want. So that brings us\u00a0\nto the differences between virtual machines\u00a0\u00a0 and containers, something that we tend to do\u00a0\nespecially those from a virtualization background.\u00a0\u00a0 As you can see on the right, in case of Docker,\u00a0\nwe have the underlying hardware, infrastructure\u00a0\u00a0 and then the OS and then Docker installed on\u00a0\nthe OS. Docker then manages the containers that\u00a0\u00a0 run with libraries and dependencies alone.\u00a0\nIn case of virtual machines, we have the\u00a0\u00a0 hypervisor like ESX on the hardware, and then\u00a0\nthe virtual machines on them. As you can see,\u00a0\u00a0 each virtual machine has its own oil inside it.\u00a0\nThen the dependencies and then the application.\u00a0\u00a0 The overhead causes higher utilization of\u00a0\nunderlying resources as there are multiple\u00a0\u00a0 virtual operating systems and kernels running. The\u00a0\nvirtual machines also consumed higher disk space,\u00a0\u00a0 as each VM is heavy, and is usually in gigabytes\u00a0\nin size, whereas Docker containers are lightweight\u00a0\u00a0 and are usually in megabytes in size. This allows\u00a0\nDocker containers to boot up faster, usually in a\u00a0\u00a0 matter of seconds, whereas VMs as we know, takes\u00a0\nminutes to boot up as it needs to boot up the\u00a0\u00a0 entire operating system. It's also important\u00a0\nto note that Docker has less isolation as\u00a0\u00a0 more resources are shared between the containers\u00a0\nlike kernel, whereas VMs have complete isolation\u00a0\u00a0 from each other. Since VMs, don't rely on the\u00a0\nunderlying OS or kernel, you can run different\u00a0\u00a0 types of applications built on different services\u00a0\nsuch as Linux based or Windows based apps on the\u00a0\u00a0 same hypervisor. So those are some differences\u00a0\nbetween the two. Now, having said that, it's not\u00a0\u00a0 an either container or virtual machine situation.\u00a0\nIts containers and virtual machines. Now, when you\u00a0\u00a0 have large environments with 1000s of application\u00a0\ncontainers running on 1000s of Docker hosts,\u00a0\u00a0 you will often see containers provisioned on\u00a0\nvirtual Docker hosts. That way, we can utilize the\u00a0\u00a0 advantages of both technologies, we can use the\u00a0\nbenefits of virtualization, to easily provision\u00a0\u00a0 or decommission Docker hosts, as required, at the\u00a0\nsame time make use of the benefits of Docker to\u00a0\u00a0 easily provision applications and quickly scale\u00a0\nthem as required. But remember that in this case,\u00a0\u00a0 we will not be provisioning that many virtual\u00a0\nmachines as we used to before, because earlier,\u00a0\u00a0 we provisioned a virtual machine for each\u00a0\napplication. Now, you might provision a virtual\u00a0\u00a0 machine for hundreds or 1000s of containers. So\u00a0\nhow is it done? There are lots of containerized\u00a0\u00a0 versions of applications readily available as of\u00a0\ntoday. So most organizations have their products\u00a0\u00a0 containerized and available in a public Docker\u00a0\nrepository called Docker Hub, or Docker store.\u00a0\u00a0 For example, you can find images of most common\u00a0\noperating systems, databases, and other services\u00a0\u00a0 and tools. Once you identify the images you need,\u00a0\nand you install Docker on your host. Bringing up\u00a0\u00a0 an application is as easy as running a Docker run\u00a0\ncommand with the name of the image. In this case,\u00a0\u00a0 running a Docker run Ansible command will run an\u00a0\ninstance of Ansible on the Docker host. Similarly\u00a0\u00a0 run an instance of MongoDB Redis and node j s\u00a0\nusing the Docker run command. If we need to run\u00a0\u00a0 multiple instances of the web service, simply\u00a0\nadd as many instances as you need and configure\u00a0\u00a0 a load balancer of some kind in the front.\u00a0\nIn case one of the instances were to fail,\u00a0\u00a0 simply destroy that instance and launch anyone.\u00a0\nThere are other solutions available for handling\u00a0\u00a0 such cases that we will look at later during\u00a0\nthis course. And for now, don't focus too much\u00a0\u00a0 on the commands. We'll get to that in a bit.\u00a0\nWe've been talking about images and containers,\u00a0\u00a0 let's understand the difference between the\u00a0\ntwo. An image is a package or a template,\u00a0\u00a0 just like a VM template that you might have worked\u00a0\nwithin the virtualization world, it is used to\u00a0\u00a0 create one or more containers. Containers are\u00a0\nrunning instances of images that are isolated and\u00a0\u00a0 have their own environments and set of processes.\u00a0\nAs we have seen before, a lot of products have\u00a0\u00a0 been dockerized already, in case you cannot find\u00a0\nwhat you're looking for. You could create your own\u00a0\u00a0 image and push it to Docker Hub repository, making\u00a0\nit available for public. So if you look at it,\u00a0\u00a0 traditionally, developers developed applications,\u00a0\nthen they hand it over to ops team to deploy and\u00a0\u00a0 manage it in production environments. They do\u00a0\nthat by providing a set of instructions such\u00a0\u00a0 as information about how the host must be set up,\u00a0\nwhat prerequisites are to be installed on the host\u00a0\u00a0 and how the dependencies are to be configured\u00a0\netc. Since the ops team did not really develop\u00a0\u00a0 the application on their own, they struggle\u00a0\nwith setting it up. When they hit an issue,\u00a0\u00a0 they work with the developers to resolve it.\u00a0\nwith Docker, developers and operations teams work\u00a0\u00a0 hand in hand to transform the guide into a Docker\u00a0\nfile with both of their requirements. This Docker\u00a0\u00a0 file is then used to create an image for their\u00a0\napplications. This image can now run on any host\u00a0\u00a0 with Docker installed on it, and is guaranteed\u00a0\nto run the same way everywhere. So the ops\u00a0\u00a0 team can now simply use the image to deploy the\u00a0\napplication. Since the image was already working,\u00a0\u00a0 when the developer built it, and operations are\u00a0\nhave not modified it. It continues to work the\u00a0\u00a0 same way when deployed in production. And that's\u00a0\none example of how a tool like Docker contributes\u00a0\u00a0 to the DevOps culture. Well, that's it for now.\u00a0\nIn the upcoming lecture, we will look at how to\u00a0\u00a0 get started with Docker. We'll now see how to get\u00a0\nstarted with Docker. Now Docker has two editions,\u00a0\u00a0 the Community Edition and the Enterprise Edition.\u00a0\nThe Community Edition is the set of free Docker\u00a0\u00a0 products. The Enterprise Edition is the certified\u00a0\nand supported container platform that comes with\u00a0\u00a0 enterprise add ons like the image management image\u00a0\nsecurity, universal control plane for managing\u00a0\u00a0 and orchestrating container runtimes. But of\u00a0\ncourse, these come with a price. We will discuss\u00a0\u00a0 more about container orchestration later in this\u00a0\ncourse, and along with some alternatives. For now,\u00a0\u00a0 we will go ahead with the Community Edition.\u00a0\nThe Community Edition is available on Linux,\u00a0\u00a0 Mac, Windows, or on cloud platforms like AWS, or\u00a0\nAzure. In the upcoming demo, we will take a look\u00a0\u00a0 at how to install and get started with Docker on\u00a0\na Linux system. Now, if you are on Mac or Windows,\u00a0\u00a0 you have two options, either install a Linux VM\u00a0\nusing VirtualBox or some kind of virtualization\u00a0\u00a0 platform. And then follow along with the upcoming\u00a0\ndemo, which is really the most easiest way to get\u00a0\u00a0 started with Docker. The second option is\u00a0\nto install Docker desktop for Mac or adopt\u00a0\u00a0 Docker desktop for Windows, which are native\u00a0\napplications. So if that is really what you\u00a0\u00a0 want the check out the Docker for Mac and the\u00a0\nwindows sections towards the end of this course,\u00a0\u00a0 and then head back here. Once you're all\u00a0\nset up. We will now head over to our demo,\u00a0\u00a0 and we will take a look at how to install Docker\u00a0\non a Linux machine. In this demo, we look at how\u00a0\u00a0 to install and get started with Docker. First\u00a0\nof all, identify a system physical or virtual\u00a0\u00a0 machine or laptop that has a supported operating\u00a0\nsystem. In my case, I have an Ubuntu VM. Go to\u00a0\u00a0 Doc's dot Docker comm and click on Get Docker.\u00a0\nYou will be taken to the Docker engine Community\u00a0\u00a0 Edition page. That is the free version that we're\u00a0\nafter. From the left hand menu, select your system\u00a0\u00a0 type. I choose Linux In my case, and then select\u00a0\nyour OS flavor. I choose Ubuntu read through the\u00a0\u00a0 prerequisites and requirements. Your Ubuntu system\u00a0\nmust be 64 bit and one of the supported versions\u00a0\u00a0 like this called cosmic bionic or sannio. In my\u00a0\ncase, I have a bionic version to confirm view the\u00a0\u00a0 Etsy release file. Next uninstaller any older\u00a0\nversion if one exists, so let's just make sure\u00a0\u00a0 that there's none on my host. So I'll just copy\u00a0\nand paste that command. And I confirm that there\u00a0\u00a0 are no older version that exists on my system. The\u00a0\nnext step is to set up a repository and install\u00a0\u00a0 the software. Now there are two ways to go about\u00a0\nthis. The first is using the package manager by\u00a0\u00a0 first updating the repository using the apt get\u00a0\nupdate command, then installing the prerequisite\u00a0\u00a0 packages, and then adding Dockers official GPG\u00a0\nkeys and then installing Docker. But I'm not\u00a0\u00a0 going to go that route. There is an easier way.\u00a0\nIf you scroll all the way to the bottom you will\u00a0\u00a0 find the instructions to install Docker using the\u00a0\nconvenience script. It's a script that automates\u00a0\u00a0 the entire installation process and works on\u00a0\nmost operating systems. Run the first command\u00a0\u00a0 to download a copy of the script and then run the\u00a0\nsecond command to execute the script to install\u00a0\u00a0 Docker automatically. Give it a few minutes to\u00a0\ncomplete the installation. The installation is\u00a0\u00a0 now successful. Let us now check the version\u00a0\nof Docker using the Docker version command. We\u00a0\u00a0 have installed version 19.0 3.1. We will now run\u00a0\na simple container to ensure everything is working\u00a0\u00a0 as expected. For this, head over to Docker Hub at\u00a0\nhub Docker Comm. Here you will find a list of the\u00a0\u00a0 most popular Docker images like nginx MongoDB,\u00a0\nAlpine, no jazz Redis, etc. Let's search for a\u00a0\u00a0 fun image called we'll say we'll say is Dockers\u00a0\nversion of kousei which is basically A simple\u00a0\u00a0 application that trains a cow saying something. In\u00a0\nthis case, it happens to be a well copy the Docker\u00a0\u00a0 run command given here. Remember to add sudo\u00a0\nand we will change the message to hello world.\u00a0\u00a0 On running this command, Docker pulls the image\u00a0\nof the willsey application from Docker Hub and\u00a0\u00a0 runs it. And we have our avail, saying, hello.\u00a0\nGreat. We're all set. Remember, for the purpose\u00a0\u00a0 of this course, you don't really need to set up\u00a0\na Docker system on your own. We provide hands\u00a0\u00a0 on labs that you will get access to but if you\u00a0\nwish to experiment on your own and follow along,\u00a0\u00a0 feel free to do so. We now look at some of the\u00a0\nDocker commands. At the end of this lecture,\u00a0\u00a0 you will go through a hands on quiz where you will\u00a0\npractice working with these commands. Let's start\u00a0\u00a0 by looking at Docker run command. The Docker run\u00a0\ncommand is used to run a container from an image\u00a0\u00a0 running the Docker run nginx command will run an\u00a0\ninstance of the nginx application from the Docker\u00a0\u00a0 host if it already exists. If the image is not\u00a0\npresent on the host, he will go out to Docker Hub\u00a0\u00a0 and pull the image down. But this is only done\u00a0\nthe first time. For the subsequent executions,\u00a0\u00a0 the same image will be reduced. The docker\u00a0\nps command lists all running containers and\u00a0\u00a0 some basic information about them. Such as\u00a0\nthe container ID, the name of the image we\u00a0\u00a0 use to run the containers, the current status\u00a0\nand the name of the container. Each container\u00a0\u00a0 automatically gets a random ID and Name created\u00a0\nfor it by Docker, which in this case is silly\u00a0\u00a0 summit. To see all containers running or not\u00a0\nuse the dash eight option. This output all\u00a0\u00a0 running as well as previously stopped or exited\u00a0\ncontainers. We'll talk about the command and\u00a0\u00a0 port fields shown in this output later in this\u00a0\ncourse. For now let's just focus on the basic\u00a0\u00a0 commands. To stop a running container use the\u00a0\nDocker stop command, but you must provide either\u00a0\u00a0 the container ID or the container name in the\u00a0\nstop command. If you're not sure of the name,\u00a0\u00a0 run the docker ps command to get it on success.\u00a0\nYou will see the name printed out and running\u00a0\u00a0 docker ps again will show no running containers.\u00a0\nRunning docker ps dash a, however shows the\u00a0\u00a0 container silly summit and that it is now an exit\u00a0\nstate a few seconds ago. Now what if we don't\u00a0\u00a0 want this container lying around consuming space?\u00a0\nWhat if we want to get rid of it for good? Use the\u00a0\u00a0 Docker rm command to remove a stopped or exited\u00a0\ncontainer permanently. If it prints the name back,\u00a0\u00a0 we're good. Run the docker ps command again to\u00a0\nverify that it's no longer present. Good. But\u00a0\u00a0 what about the nginx image that was downloaded? At\u00a0\nfirst? We're not using that anymore. So how do we\u00a0\u00a0 get rid of that image? But first, how do we see a\u00a0\nlist of images present on our host run the Docker\u00a0\u00a0 images command to see a list of available images\u00a0\nand their sizes. On our host we have four images\u00a0\u00a0 nginx Redis, Ubuntu and Alpine. We will talk about\u00a0\ntags later in this course when we discuss about\u00a0\u00a0 images. To remove an image that you no longer\u00a0\nplan to use. Run the Docker Rmi command. Remember,\u00a0\u00a0 you must ensure that no containers are running\u00a0\noff of that image before attempting to remove\u00a0\u00a0 the image. You must stop and delete all dependent\u00a0\ncontainers to be able to delete an image. When we\u00a0\u00a0 ran the Docker run command earlier, it downloaded\u00a0\nthe Ubuntu image as it couldn't find one locally.\u00a0\u00a0 What if we simply want to download the image and\u00a0\nkeep so when we run the run Docker run command,\u00a0\u00a0 we don't want to wait for it to download. Use the\u00a0\nDocker pull command to only pull the image and\u00a0\u00a0 not run the container. So in this case, the\u00a0\nDocker pull Ubuntu command pulls the Ubuntu\u00a0\u00a0 image and stores it on our host. Let's look at\u00a0\nanother example. Say you were to run a Docker\u00a0\u00a0 container from an Ubuntu image. When you run the\u00a0\nDocker run Ubuntu command it runs an instance of\u00a0\u00a0 Ubuntu image and exits immediately. If you were to\u00a0\nlist the running containers, you wouldn't see the\u00a0\u00a0 container running. If you list all containers,\u00a0\nincluding those that are stopped, you will see\u00a0\u00a0 that the new container you ran is in an exit\u00a0\nstate. Now why is that? Unlike virtual machine\u00a0\u00a0 containers are not meant to host an operating\u00a0\nsystem. Containers are meant to run a specific\u00a0\u00a0 task or process such as to host an instance of a\u00a0\nweb server or application server or a database,\u00a0\u00a0 or simply to carry some kind of computation\u00a0\nor analysis task. Once the task is complete,\u00a0\u00a0 the container exits a container only lives as\u00a0\nlong as the process inside it is alive. If the\u00a0\u00a0 web service inside the container is stopped, or\u00a0\ncrash, then the container exits. This is why when\u00a0\u00a0 you run a container from an Ubuntu image, it\u00a0\nstops immediately. Because Ubuntu is just an\u00a0\u00a0 image of an operating system that is used as the\u00a0\nbase image. For other applications. There is no\u00a0\u00a0 process or application running in it by default.\u00a0\nIf the image isn't running any service, as is\u00a0\u00a0 the case with Ubuntu, you could instruct Docker\u00a0\nto run a process with the Docker run command.\u00a0\u00a0 For example, a sleep command with a duration\u00a0\nof five seconds. When the container starts,\u00a0\u00a0 it runs the sleep command and goes into sleep for\u00a0\nfive seconds post with the sleep command exit,\u00a0\u00a0 and the container stops. What we just saw was\u00a0\nexecuting a command when we run the container,\u00a0\u00a0 but what if we would like to execute a command\u00a0\non a running container. For example, when I run\u00a0\u00a0 the docker ps command, I can see that there is\u00a0\na running container which uses the Ubuntu image\u00a0\u00a0 and sleeps 400 seconds. Let's say I would like to\u00a0\nsee the contents of a file inside this particular\u00a0\u00a0 container. I could use the Docker exec command to\u00a0\nexecute a command on my Docker container, in this\u00a0\u00a0 case to print the contents of the Etsy hosts file.\u00a0\nFinally, let's look at one more option before we\u00a0\u00a0 head over to the practice exercises. I'm now going\u00a0\nto run a Docker image I developed for a simple web\u00a0\u00a0 application. The repository name is cloud slash\u00a0\nsimple web app. It runs a simple web server that\u00a0\u00a0 listens on port 8080. When you run a Docker run\u00a0\ncommand like this, it runs in the foreground or\u00a0\u00a0 in an attached mode, meaning you will be attached\u00a0\nto the console or the standard out of the Docker\u00a0\u00a0 container. And you will see the output of the\u00a0\nweb service on your screen. You won't be able\u00a0\u00a0 to do anything else on this console other than\u00a0\nview the output until this Docker container\u00a0\u00a0 stops. It won't respond to your inputs. press the\u00a0\nctrl plus c combination to stop the container and\u00a0\u00a0 the application hosted on the container exits and\u00a0\nyou get back to your prompt. Another option is to\u00a0\u00a0 run the Docker container in the detached mode\u00a0\nby providing the dash D option. This will run\u00a0\u00a0 the Docker container in the background mode, and\u00a0\nyou will be back to your prompt immediately. The\u00a0\u00a0 container will continue to run in the backend,\u00a0\nrun the docker ps command to view the running\u00a0\u00a0 container. Now if you would like to attach back to\u00a0\nthe running container later, run the Docker attach\u00a0\u00a0 command and specify the name or ID of the Docker\u00a0\ncontainer. Now remember, if you're specifying the\u00a0\u00a0 ID of a container in any Docker command, you can\u00a0\nsimply provide the first few characters alone,\u00a0\u00a0 just so it is different from the other container\u00a0\nIDs on the host. In this case, I specify a 043 D.\u00a0\u00a0 Now don't worry about accessing the UI of the web\u00a0\nserver for now. We will look more into that in the\u00a0\u00a0 upcoming lectures. For now let's just understand\u00a0\nthe basic commands will now get our hands dirty\u00a0\u00a0 with the Docker COI. So let's take a look at how\u00a0\nto access the practice lab environments. Next.\u00a0\u00a0 Let me now walk you through the hands on lab\u00a0\npractice environment. The links to access the labs\u00a0\u00a0 associated with this course are available at code\u00a0\ncloud at code cloud.com slash p slash Docker dash\u00a0\u00a0 labs. This link is also given in the description\u00a0\nof this video. Once you're on this page, use the\u00a0\u00a0 links given there to access the labs associated\u00a0\nto your lecture. Each lecture has its own lab. So\u00a0\u00a0 remember to choose the right lab for your lecture.\u00a0\nThe labs open up right in your browser, I would\u00a0\u00a0 recommend to use Google Chrome while working with\u00a0\nthe labs. The interface consists of two parts,\u00a0\u00a0 a terminal on the left and a quiz portal on the\u00a0\nright. The quiz portal on the right gives you\u00a0\u00a0 challenges to solve. Follow the quiz and try and\u00a0\nanswer the questions asked and complete the tasks\u00a0\u00a0 given to you. Each scenario consists of anywhere\u00a0\nfrom 10 to 20 questions that need to be answered.\u00a0\u00a0 Within 30 minutes to an hour. At the top, you have\u00a0\nthe question numbers below that is the remaining\u00a0\u00a0 time for your lab below that is the question.\u00a0\nIf you are not able to solve the challenge,\u00a0\u00a0 look for hints in the head section, you may skip\u00a0\na question by hitting the skip button in the top\u00a0\u00a0 right corner. But remember that you will not be\u00a0\nable to go back to a previous question once you\u00a0\u00a0 have skipped. If the quiz portal gets stuck for\u00a0\nsome reason, click on the quiz portal tab at the\u00a0\u00a0 top to open the quiz portal in a separate window.\u00a0\nThe terminal gives you access to a real system\u00a0\u00a0 running Docker, you can run any Docker command\u00a0\nhere and run your own containers or applications.\u00a0\u00a0 You will typically be running commands to solve\u00a0\nthe task assigned in the quiz portal. You may play\u00a0\u00a0 around and experiment with this environment. But\u00a0\nmake sure you do that after you've gone through\u00a0\u00a0 the quiz so that your work does not interfere with\u00a0\nthe tasks provided by the quiz. So let me walk you\u00a0\u00a0 through a few questions. There are two types of\u00a0\nquestions. Each lab scenario starts with a set of\u00a0\u00a0 exploratory multiple choice questions where you're\u00a0\nasked to explore and find information in the given\u00a0\u00a0 environment and select the right answer. This is\u00a0\nto get you familiarized with the setup. You're\u00a0\u00a0 then asked to perform tasks like run a container,\u00a0\nstop them, delete them, build your own image,\u00a0\u00a0 etc. Here, the first question asks us to find\u00a0\nthe version of Docker server engine running on\u00a0\u00a0 the host. Run the Docker version command in\u00a0\nthe terminal and identify the right version.\u00a0\u00a0 Then select the appropriate option from the given\u00a0\nchoices. Another example is the fourth question\u00a0\u00a0 where it asks you to run a container using the\u00a0\nRedis image. If you're not sure of the command,\u00a0\u00a0 click on hence and it will show you a hint. We\u00a0\nnow run a Redis container using the Docker run\u00a0\u00a0 Redis command, wait for the container to run. Once\u00a0\ndone, click on Check to check your work. You have\u00a0\u00a0 now successfully completed the task. Similarly,\u00a0\nfollow along and complete all tasks. Once the\u00a0\u00a0 lab exercise is completed. Remember to leave a\u00a0\nfeedback and let us know how it went. A few things\u00a0\u00a0 to note. These are publicly accessible labs that\u00a0\nanyone can access. So if you catch yourself logged\u00a0\u00a0 out during a peak hour, please wait for some time\u00a0\nand try again. Also remember to not store any\u00a0\u00a0 private or confidential data on these systems.\u00a0\nRemember that this environment is for learning\u00a0\u00a0 purposes only and is only alive for an hour, after\u00a0\nwhich the lab is destroyed. So does all your work.\u00a0\u00a0 But you may start over and access these labs as\u00a0\nmany times as you want. until you feel confident.\u00a0\u00a0 I will also post solutions to these lab quizzes.\u00a0\nSo if you run into issues, you may refer to\u00a0\u00a0 those. That's it for now, head over to the first\u00a0\nchallenge. And I will see you on the other side.\u00a0\u00a0 We will now look at some of the other Docker run\u00a0\ncommands. At the end of this lecture, you will go\u00a0\u00a0 through a hands on quiz where you will practice\u00a0\nworking with these commands. We learned that we\u00a0\u00a0 could use the Docker run Redis command to run a\u00a0\ncontainer running a Redis service, in this case,\u00a0\u00a0 the latest version of Redis, which happens to be\u00a0\n5.0 dot five as of today. But what if we want to\u00a0\u00a0 run another version of Redis like for example,\u00a0\nan older version, say 4.0. Then you specify the\u00a0\u00a0 version separated by a colon. This is called a\u00a0\ntag. In that case, Docker pulls an image of the\u00a0\u00a0 photo zero version of Redis and runs that. Also,\u00a0\nnotice that if you don't specify any tag as in the\u00a0\u00a0 first command, Docker will consider the default\u00a0\ntag to be latest. Latest is a tag associated\u00a0\u00a0 to the latest version of that software, which\u00a0\nis governed by the authors of their software.\u00a0\u00a0 So as a user, how do you find information about\u00a0\nthese versions and what is the latest? At Docker\u00a0\u00a0 hub.com look up an image and you will find all the\u00a0\nsupport tags in its description. Each version of\u00a0\u00a0 the software can have multiple short and long tags\u00a0\nassociated with it, as seen here. In this case,\u00a0\u00a0 the version 5.0 dot five also has the latest tag\u00a0\non it. Let's now look at inputs. I have a simple\u00a0\u00a0 prompt application that when run asked for my name\u00a0\nand on entering my name prints a welcome message\u00a0\u00a0 if If I were to Docker eyes this application\u00a0\nand run it as a Docker container like this,\u00a0\u00a0 it wouldn't wait for the prompt. It just prints\u00a0\nwhatever the application is supposed to bring on\u00a0\u00a0 standard out. That is because by default, the\u00a0\nDocker container does not listen to a standard\u00a0\u00a0 input. Even though you're attached to its console,\u00a0\nit is not able to read any input from you. It\u00a0\u00a0 doesn't have a terminal to read inputs from it\u00a0\nruns in a non interactive mode. If you'd like\u00a0\u00a0 to provide your input, you must map the standard\u00a0\ninput of your host to the Docker container using\u00a0\u00a0 the dash I parameter. The dash I parameter is\u00a0\nfor interactive mode. And when I input my name,\u00a0\u00a0 it prints the expected output. But there is\u00a0\nsomething still missing from this, the prompt\u00a0\u00a0 when we run the app, at first, it asked us for our\u00a0\nname. But when dockerized that prompt is missing,\u00a0\u00a0 even though it seems to have accepted my input.\u00a0\nThat is because the application prompt on the\u00a0\u00a0 terminal and we have not attest to the containers\u00a0\nterminal. For this use the dash t option as well.\u00a0\u00a0 The dash T stands for a pseudo terminal. So with\u00a0\nthe combination of dash IMT. We're now attached\u00a0\u00a0 to the terminal, as well as in an interactive mode\u00a0\non the container. We will now look at Port mapping\u00a0\u00a0 or port publishing on containers. Let's go back to\u00a0\nthe example where we run a simple web application\u00a0\u00a0 in a Docker container on my Docker host. Remember\u00a0\nthe underlying host where Docker is installed is\u00a0\u00a0 called Docker host or Docker engine. When we run\u00a0\na containerized web application it runs and we're\u00a0\u00a0 able to see that the server is running. But how\u00a0\ndoes the user access my application. As you can\u00a0\u00a0 see, my application is listening on port 5000. So\u00a0\nI could access my application by using Port 5000.\u00a0\u00a0 But what IP do I use to access it from a web\u00a0\nbrowser. There are two options available. One\u00a0\u00a0 is to use the IP of the Docker container. Every\u00a0\nDocker container gets an IP assigned by default,\u00a0\u00a0 in this case it is 172 dot 17 dot 0.2. Remember\u00a0\nthat this is an internal IP and is only accessible\u00a0\u00a0 within the Docker host. So if you open a browser\u00a0\nfrom within the Docker host, you can go to HTTP,\u00a0\u00a0 colon forward slash forward slash 172 dot 17 dot\u00a0\n0.1 colon 5000 to access the IP address. But since\u00a0\u00a0 this is an internal IP users outside of the Docker\u00a0\nhost cannot access it using this IP. For this,\u00a0\u00a0 we could use the IP of the Docker host, which\u00a0\nis 190 2.1 68 dot five. But for that to work,\u00a0\u00a0 you must have mapped the port inside the Docker\u00a0\ncontainer to a free port on the Docker host.\u00a0\u00a0 For example, if I want the users to access my\u00a0\napplication through Port 80, on my Docker host,\u00a0\u00a0 I could map Port 80 of local host to Port 5000 on\u00a0\nthe Docker container using the dash p parameter\u00a0\u00a0 in my run command like this. And so the user can\u00a0\naccess my application by going to the URL HTTP,\u00a0\u00a0 colon slash slash 190 2.1 68 dot 1.5 colon 80. And\u00a0\nall traffic on port 80 on my Docker host, will get\u00a0\u00a0 routed to Port 5000 inside the Docker container.\u00a0\nThis way you can run multiple instances of your\u00a0\u00a0 application and map them to different ports on\u00a0\nthe Docker host or run instances of different\u00a0\u00a0 applications on different ports. For example, in\u00a0\nthis case, I'm running an instance of MySQL that\u00a0\u00a0 runs a database on my host and listens on the\u00a0\ndefault MySQL port, which happens to be 3306,\u00a0\u00a0 or another instance of MySQL on another port 8306.\u00a0\nSo you can run as many applications like this,\u00a0\u00a0 and map them to as many ports as you want. And\u00a0\nof course, you cannot map to the same port on the\u00a0\u00a0 Docker host more than once. We will discuss more\u00a0\nabout port mapping and networking of containers\u00a0\u00a0 in the network lecture later on. Let's now look\u00a0\nat how data is persisted in a Docker container.\u00a0\u00a0 For example, let's say you were to run a MySQL\u00a0\ncontainer. When databases and tables are created,\u00a0\u00a0 the data files are stored in location slash four\u00a0\nlib MySQL inside the Docker container. Remember,\u00a0\u00a0 the Docker container has its own isolated file\u00a0\nsystem and any changes to any files happen within\u00a0\u00a0 the kernel. tainer let's assume you dump a lot\u00a0\nof data into the database. What happens if you\u00a0\u00a0 were to delete the MySQL container and remove\u00a0\nit. As soon as you do that, the container along\u00a0\u00a0 with all the data inside it gets blown away,\u00a0\nmeaning all your data is gone. If you would\u00a0\u00a0 like to persist data, you would want to map a\u00a0\ndirectory outside the container on the Docker\u00a0\u00a0 host to a directory inside the container. In this\u00a0\ncase, I create a directory called slash OBT slash\u00a0\u00a0 data dir and map that to var lib MySQL inside\u00a0\nthe Docker container using the dash v option,\u00a0\u00a0 and specifying the directory on the Docker host,\u00a0\nfollowed by a colon and the directory inside the\u00a0\u00a0 Docker container. This way when Docker container\u00a0\nruns, it will implicitly mount the external\u00a0\u00a0 directory to a folder inside the Docker container.\u00a0\nThis way all your data will now be stored in the\u00a0\u00a0 external volume at slash RPT slash data directory.\u00a0\nAnd this will remain even if you delete the Docker\u00a0\u00a0 container. The docker ps command is good enough\u00a0\nto get basic details about containers like\u00a0\u00a0 their names and IDs. But if you'd like to see\u00a0\nadditional details about a specific container,\u00a0\u00a0 use the Docker inspect command and provide the\u00a0\ncontainer name or ID. It returns all details\u00a0\u00a0 of a container in a JSON format, such as the state\u00a0\nmounts, configuration data, network settings, etc.\u00a0\u00a0 Remember to use it when you're required to find\u00a0\ndetails on a container. And finally, how do we see\u00a0\u00a0 the logs of a container we run in the background.\u00a0\nFor example, I ran my simple web application using\u00a0\u00a0 the dash D parameter and it ran the container\u00a0\nin a detached mode. How do I view the logs\u00a0\u00a0 which happens to be the contents written to the\u00a0\nstandard out of that container. Use the Docker\u00a0\u00a0 logs command and specify the container ID or\u00a0\nname like this. Well, that's it for this lecture,\u00a0\u00a0 how to work with the challenges and practice\u00a0\nworking with Docker commands. Let's start with\u00a0\u00a0 a simple web application written in Python. This\u00a0\npiece of code is used to create a web application\u00a0\u00a0 that displays a web page with a background color.\u00a0\nIf you look closely into the application code,\u00a0\u00a0 you will see a line that sets the background\u00a0\ncolor to red. Now, that works just fine. However,\u00a0\u00a0 if you decide to change the color in the future,\u00a0\nyou will have to change the application code. It\u00a0\u00a0 is a best practice to move such information out of\u00a0\nthe application code and into say an environment\u00a0\u00a0 variable called app color. The next time you run\u00a0\nthe application set an environment variable called\u00a0\u00a0 add color to a desired value. And the application\u00a0\nnow has a new color. Once your application gets\u00a0\u00a0 packaged into a Docker image, you will then\u00a0\nrun it with the Docker run command followed\u00a0\u00a0 by the name of the image. However, if you wish to\u00a0\npass the environment variable as he did before,\u00a0\u00a0 he would now use the Docker run commands dash\u00a0\ne option to set an environment variable within\u00a0\u00a0 the container. To deploy multiple containers with\u00a0\ndifferent colors. He would run the Docker command\u00a0\u00a0 multiple times and set a different value for the\u00a0\nenvironment variable each time. So how do you find\u00a0\u00a0 the environment variable set on a container that's\u00a0\nalready running? Use the Docker inspect command to\u00a0\u00a0 inspect the properties of a running container.\u00a0\nUnder the config section, you will find the list\u00a0\u00a0 of Environment Variables set on the container.\u00a0\nWell that's it for this lecture on configuring\u00a0\u00a0 environment variables in Docker. Hello, and\u00a0\nwelcome to this lecture on Docker images. In this\u00a0\u00a0 lecture, we're going to see how to create your\u00a0\nown image. Now before that, why would you need to\u00a0\u00a0 create your own image? It could either be because\u00a0\nyou cannot find a component or a service that you\u00a0\u00a0 want to use as part of your application on Docker\u00a0\nHub already, or you and your team decided that the\u00a0\u00a0 application you're developing will be dockerized\u00a0\nfor ease of shipping and deployment. In this case,\u00a0\u00a0 I'm going to containerize an application a simple\u00a0\nweb application that I have built using the Python\u00a0\u00a0 flask framework. First we need to understand\u00a0\nwhat we are containerizing or what application\u00a0\u00a0 we are creating an image for How the application\u00a0\nis built. So start by thinking what you might do.\u00a0\u00a0 If you want to deploy the application manually,\u00a0\nwe write down the steps required in the right\u00a0\u00a0 order. I'm creating an image for a simple web\u00a0\napplication. If I were to set it up manually,\u00a0\u00a0 I would start with an operating system like\u00a0\nUbuntu, then update the source repositories using\u00a0\u00a0 the abt command, then install dependencies using\u00a0\nthe abt command, then install Python dependencies\u00a0\u00a0 using the PIP command, then copy over the source\u00a0\ncode of my application to a location like RPT\u00a0\u00a0 and then finally, run the web server using the\u00a0\nfloss command. Now that I have the instructions,\u00a0\u00a0 create a Docker file using this. Here's a\u00a0\nquick overview of the process of creating\u00a0\u00a0 your own image. First, create a Docker file named\u00a0\nDocker file and write down the instructions for\u00a0\u00a0 setting up your application in it, such as\u00a0\ninstalling dependencies, where to copy the\u00a0\u00a0 source code from and to and what the entry\u00a0\npoint of the application is, etc. Once done,\u00a0\u00a0 build your image using the Docker build command\u00a0\nand specify the Docker file as input as well\u00a0\u00a0 as a tag name for the image. This will create an\u00a0\nimage locally on your system. To make it available\u00a0\u00a0 on the public Docker Hub registry, run the Docker\u00a0\npush command and specify the name of the image you\u00a0\u00a0 just created. In this case, the name of the image\u00a0\nis my account name which is mm shot, followed by\u00a0\u00a0 the image name, which is my custom app. Now\u00a0\nlet's take a closer look at that Docker file.\u00a0\u00a0 Docker file is a text file written in a specific\u00a0\nformat that Docker can understand. It's in an\u00a0\u00a0 instruction and arguments format. For example,\u00a0\nin this Docker file, everything on the left in\u00a0\u00a0 caps is an instruction. In this case from run,\u00a0\ncopy and entry point are all instructions. Each\u00a0\u00a0 of these instruct Docker to perform a specific\u00a0\naction while creating the image. Everything on the\u00a0\u00a0 right is an argument to those instructions. The\u00a0\nfirst line from Ubuntu defines what the base OS\u00a0\u00a0 should be for this container. Every Docker image\u00a0\nmust be based off of another image, either an OS\u00a0\u00a0 or another image that was created before based\u00a0\non an OS, you can find official releases of all\u00a0\u00a0 operating systems on Docker Hub. It's important\u00a0\nto note that all Docker files must start with the\u00a0\u00a0 from instruction. The run instruction instructs\u00a0\nDocker to run a particular command on those base\u00a0\u00a0 images. So at this point, Docker runs the abt get\u00a0\nupdate commands to fetch the updated packages,\u00a0\u00a0 and installs required dependencies on the image.\u00a0\nThen the copy instruction copies files from the\u00a0\u00a0 local system onto the Docker image. In this\u00a0\ncase, the source code of our application is in\u00a0\u00a0 the current folder, and I'll be copying it over\u00a0\nto the location property source code inside the\u00a0\u00a0 Docker image. And finally, entry point allows\u00a0\nus to specify a command that will be run when\u00a0\u00a0 the image is run as a container. When Docker\u00a0\nbuilds the images, it builds these in a layered\u00a0\u00a0 architecture. Each line of instruction creates\u00a0\na new layer in the Docker image with just the\u00a0\u00a0 changes from the previous layer. For example, the\u00a0\nfirst layer is a base Ubuntu OS, followed by the\u00a0\u00a0 second instruction that creates a second layer\u00a0\nwhich installs all the IPT packages. And then\u00a0\u00a0 the third instruction creates a third layer with\u00a0\na Python packages followed by the fourth layer\u00a0\u00a0 that copies the source code over and the final\u00a0\nlayer that updates the entry point of the image.\u00a0\u00a0 Since each layer only stores the changes from\u00a0\nthe previous layer, it is reflected in the size\u00a0\u00a0 as well. If you look at the base Ubuntu image, it\u00a0\nis around 120 MB in size. The IPT packages that\u00a0\u00a0 are installed is around 300 Mb and the remaining\u00a0\nlayers are small. You can see this information if\u00a0\u00a0 you run the Docker history command followed by the\u00a0\nimage name. When you run the Docker build command,\u00a0\u00a0 you can see the various steps involved and the\u00a0\nresult of each task. All the layers built are\u00a0\u00a0 cast. So the layered architecture helps you\u00a0\nrestart Docker build from that particular step\u00a0\u00a0 in case it fails. Or if you were to add new steps\u00a0\nin the build process, you wouldn't have to start\u00a0\u00a0 all over again. All the layers built out cached by\u00a0\nDocker. So in case a particular step was to fail,\u00a0\u00a0 for example, in this case, step three failed, and\u00a0\nyou were to fix the issue and rerun Docker build,\u00a0\u00a0 it will reuse the previous layers from\u00a0\ncache and continue to build the remaining\u00a0\u00a0 layers. The same is true if you were to add\u00a0\nadditional steps in the Docker file. This way,\u00a0\u00a0 rebuilding your image is faster. And you don't\u00a0\nhave to wait for Docker to rebuild the entire\u00a0\u00a0 image each time. This is helpful, especially when\u00a0\nyou update source code of your application. As it\u00a0\u00a0 may change more frequently, only the layers above\u00a0\nthe updated layers needs to be rebuilt. We just\u00a0\u00a0 saw a number of products containerized such as\u00a0\ndatabases, development tools, operating systems,\u00a0\u00a0 etc. But that's just not it. You can containerize\u00a0\nalmost all of the application even simple ones,\u00a0\u00a0 like browsers or utilities, like curl\u00a0\napplications like Spotify, Skype,\u00a0\u00a0 etc. Basically, you can containerize everything\u00a0\nand going forward and see that's how everyone\u00a0\u00a0 is going to run applications. Nobody is going to\u00a0\ninstall anything anymore going forward. Instead,\u00a0\u00a0 they're just going to run it using Docker. And\u00a0\nwhen they don't need it anymore, get rid of it\u00a0\u00a0 easily without having to clean up too much. In\u00a0\nthis lecture, we will look at commands arguments\u00a0\u00a0 and entry points in Docker. Let's start with a\u00a0\nsimple scenario. Say you were to run a Docker\u00a0\u00a0 container from an Ubuntu image. When you run the\u00a0\nDocker run Ubuntu command it runs an instance of\u00a0\u00a0 Ubuntu image and exits immediately. If you were to\u00a0\nlist the running containers, you wouldn't see the\u00a0\u00a0 container running. If you list all containers,\u00a0\nincluding those that are stopped, you will see\u00a0\u00a0 that the new container you ran is in an exited\u00a0\nstate. Now why is that? Unlike virtual machines,\u00a0\u00a0 containers are not meant to host an operating\u00a0\nsystem. Containers are meant to run a specific\u00a0\u00a0 task or process, such as to host an instance of a\u00a0\nweb server or application server or a database or\u00a0\u00a0 simply to carry out some kind of computation\u00a0\nor analysis. Once the task is complete, the\u00a0\u00a0 container exits a container only lives as long as\u00a0\nthe process inside ID is alive. If the web service\u00a0\u00a0 inside the container is stopped or crashes, the\u00a0\ncontainer exits. So who defines what process\u00a0\u00a0 is run within the container. If you look at the\u00a0\nDocker file for popular Docker images like ngi Nx,\u00a0\u00a0 you will see an instruction called CMD, which\u00a0\nstands for command that defines the program that\u00a0\u00a0 will be run within the container when it starts.\u00a0\nFor the ngi nx image. It is the ngi nx command for\u00a0\u00a0 the MySQL image. It is the MySQL D command. What\u00a0\nwe tried to do earlier was to run a container with\u00a0\u00a0 a plain Ubuntu Operating System. Let us look at\u00a0\nthe Docker file for this image. You will see that\u00a0\u00a0 it uses bash as the default command. Now bash is\u00a0\nnot really a process like a web server or database\u00a0\u00a0 server. It is a shell that listens for inputs\u00a0\nfrom a terminal. If it cannot find the terminal it\u00a0\u00a0 exits. When we ran the Ubuntu container earlier,\u00a0\nDocker created a container from the Ubuntu image\u00a0\u00a0 and launch the bash program. By default, Docker\u00a0\ndoes not attach a terminal to a container when\u00a0\u00a0 it is run. And so the bash program does not find\u00a0\nthe terminal. And so it exits. Since the process\u00a0\u00a0 that was started when the container was created,\u00a0\nfinished the container exits as well. So how\u00a0\u00a0 do you specify a different command to start the\u00a0\ncontainer? One option is to append a command to\u00a0\u00a0 the Docker run command. And that way it overrides\u00a0\nthe default command specified within the image.\u00a0\u00a0 In this case, I run the Docker run Ubuntu command\u00a0\nwith the sleep five command as the added option.\u00a0\u00a0 This way when the container starts, it runs the\u00a0\nsleep program waits for five seconds and then\u00a0\u00a0 exits. But how do you make that change permanent?\u00a0\nSay you want the image to always run the sleep\u00a0\u00a0 command when it starts. You will then create\u00a0\nyour own image from the base Ubuntu image and\u00a0\u00a0 specify a new command. There are different ways of\u00a0\nspecifying the command either the command simply\u00a0\u00a0 as a In a shell form, or in a JSON array format\u00a0\nlike this, but remember, when you specify in a\u00a0\u00a0 JSON array format, the first element in the array\u00a0\nshould be the executable. In this case, the sleep\u00a0\u00a0 program did not specify the command and parameters\u00a0\ntogether like this, the command and its parameters\u00a0\u00a0 should be separate elements in the list. So I now\u00a0\nbuild my new image using the Docker build command,\u00a0\u00a0 and name it as a boon to sleeper. I could now\u00a0\nsimply run the Docker boon to sleeper command and\u00a0\u00a0 get the same results. It always sleeps for five\u00a0\nseconds and exits. But what if I wish to change\u00a0\u00a0 the number of seconds it sleeps? Currently, it is\u00a0\nhard coded to five seconds. As we learned before,\u00a0\u00a0 One option is to run the Docker run command with\u00a0\nthe new command appended to it, in this case,\u00a0\u00a0 sleep 10. And so the command that will be run\u00a0\nat startup will be sleep 10. But it doesn't look\u00a0\u00a0 very good. The name of the image, Ubuntu sleeper\u00a0\nin itself implies that the container will sleep,\u00a0\u00a0 so we shouldn't have to specify the sleep command\u00a0\nagain. Instead, we would like it to be something\u00a0\u00a0 like this Docker run Ubuntu sleeper 10, we\u00a0\nonly want to pass in the number of seconds the\u00a0\u00a0 container should sleep and sleep command should\u00a0\nbe invoked automatically. And that is where the\u00a0\u00a0 entry point instruction comes into play. The entry\u00a0\npoint instruction is like the command instruction.\u00a0\u00a0 As in, you can specify the program that will be\u00a0\nrun when the container starts. And whatever you\u00a0\u00a0 specify on the command line, in this case, 10\u00a0\nwill get appended to the entry point. So the\u00a0\u00a0 command that will be run when the container starts\u00a0\nis sleep 10. So that's the difference between the\u00a0\u00a0 two. In case of the CMD instruction, the command\u00a0\nline parameters passed will get replaced entirely.\u00a0\u00a0 Whereas in case of entry point, the command line\u00a0\nparameters will get appended. Now, in the second\u00a0\u00a0 case, what if I run the open to sleeper image\u00a0\ncommand without appending the number of seconds,\u00a0\u00a0 then the command at startup will be just sleep and\u00a0\nyou get the error that the opposite is missing.\u00a0\u00a0 So how do you configure a default value for the\u00a0\ncommand if one was not specified in the command\u00a0\u00a0 line, that's where you would use both entry point\u00a0\nas well as the command instruction. In this case,\u00a0\u00a0 the command instruction will be appended to\u00a0\nthe entry point instruction. So at startup, the\u00a0\u00a0 command would be sleep five, if you didn't specify\u00a0\nany parameters in the command line. If you did,\u00a0\u00a0 then that will override the command instruction.\u00a0\nAnd remember for this to happen, you should always\u00a0\u00a0 specify the entry point and command instructions\u00a0\nin a JSON format. Finally, what if you really\u00a0\u00a0 really want to modify the entry point during\u00a0\nruntime, say from sleep to an imaginary sleep 2.0\u00a0\u00a0 command? Well, in that case, you can override it\u00a0\nby using the entry point option in the Docker run\u00a0\u00a0 command. The final command at startup would then\u00a0\nbe sleep 2.0 10. Well, that's it for this lecture,\u00a0\u00a0 and I will see you in the next. We now look at\u00a0\nnetworking in Docker. When you install Docker,\u00a0\u00a0 it creates three networks automatically. Bridge\u00a0\nno and host bridge is the default network a\u00a0\u00a0 container gets attached to if you would like to\u00a0\nassociate the container with any other network,\u00a0\u00a0 specify the network information using the network\u00a0\ncommand line parameter like this. We will now look\u00a0\u00a0 at each of these networks. The bridge network is\u00a0\na private internal network created by Docker on\u00a0\u00a0 the host all containers attached to this network\u00a0\nby default, and they get an internal IP address,\u00a0\u00a0 usually in the range 172 dot 17 series. The\u00a0\ncontainers can access each other using this\u00a0\u00a0 internal IP if required. To access any of these\u00a0\ncontainers from the outside world, map the ports\u00a0\u00a0 of these containers to port on the Docker host,\u00a0\nas we have seen before. Another way to access\u00a0\u00a0 the containers externally is to associate the\u00a0\ncontainer to the host network. This takes out\u00a0\u00a0 any network isolation between the Docker host and\u00a0\nthe Docker container. Meaning if you were to run\u00a0\u00a0 a web server on port 5000. In a web app container,\u00a0\nit is automatically as accessible on the same port\u00a0\u00a0 externally without requiring any port mapping\u00a0\nas the web container uses the hosts network.\u00a0\u00a0 This will also mean that unlike before, you will\u00a0\nnow not be able to run multiple web containers on\u00a0\u00a0 the same host on the same port, as the ports\u00a0\nare now common to all containers in the host\u00a0\u00a0 network. With the non network, the containers\u00a0\nare not attached to any network and doesn't\u00a0\u00a0 have any access to the external network, or other\u00a0\ncontainers. They run in an isolated network. So\u00a0\u00a0 we just saw the default burst network where the\u00a0\nnetwork ID 172 dot 70 dot 0.1. So all containers\u00a0\u00a0 associated to this default network will be able to\u00a0\ncommunicate to each other. But what if we wish to\u00a0\u00a0 isolate the containers within the Docker host, for\u00a0\nexample, the first two web containers on internal\u00a0\u00a0 network 172 and the second two containers on a\u00a0\ndifferent internal network, like 182. By default,\u00a0\u00a0 Docker only creates one internal bridge network,\u00a0\nwe could create our own internal network using\u00a0\u00a0 the command Docker network, create and specify\u00a0\nthe driver which is bridge in this case, and the\u00a0\u00a0 subnet for that network followed by the custom\u00a0\nisolated network name. Run the Docker network ls\u00a0\u00a0 command to list all networks. So how do we see the\u00a0\nnetwork settings and the IP address assigned to an\u00a0\u00a0 existing container? Run the Docker inspect command\u00a0\nwith the ID or name of the container, and you will\u00a0\u00a0 find a section on network settings. There you can\u00a0\nsee the type of network the container is attached\u00a0\u00a0 to its internal IP address, MAC address, and other\u00a0\nsettings. Containers can reach each other using\u00a0\u00a0 their names. For example, in this case, I have a\u00a0\nweb server and a MySQL database container running\u00a0\u00a0 on the same node. How can I get my web server to\u00a0\naccess the database on the database container. One\u00a0\u00a0 thing I could do is to use the internal IP address\u00a0\nsigned to the MySQL container, which in this case\u00a0\u00a0 is 172 dot 70 dot 0.3. But that is not very ideal\u00a0\nbecause it is not guaranteed that the container\u00a0\u00a0 will get the same IP when the system reboots. The\u00a0\nright way to do it is to use the container name.\u00a0\u00a0 All containers in a Docker host can resolve each\u00a0\nother with the name of the container. Docker has\u00a0\u00a0 a built in DNS server that helps the containers\u00a0\nto resolve each other using the container name.\u00a0\u00a0 Note that the built in DNS server always runs at\u00a0\naddress 127 dot 0.0 dot 11. So how does Docker\u00a0\u00a0 implement networking? What's the technology behind\u00a0\nit? Like how are the containers isolated within\u00a0\u00a0 the host? Docker uses network namespaces that\u00a0\ncreates a separate namespace for each container.\u00a0\u00a0 It then uses virtual Ethernet pairs to connect\u00a0\ncontainers together. Well, that's all we can talk\u00a0\u00a0 about it for now. More about these are advanced\u00a0\nconcepts that we discussed in the advanced course\u00a0\u00a0 on Docker on code cloud. That's all for now.\u00a0\nFrom this lecture on networking, head over to the\u00a0\u00a0 practice test and practice working with networking\u00a0\nin Docker. I will see you in the next lecture.\u00a0\u00a0 Hello and welcome to this lecture and we are\u00a0\nlearning advanced Docker concepts. In this\u00a0\u00a0 lecture we're going to talk about Docker storage\u00a0\ndrivers and file systems. We're going to see where\u00a0\u00a0 and how Docker stores data and how it manages\u00a0\nfile systems of the containers. Let us start with\u00a0\u00a0 how Docker stores data on the local file system.\u00a0\nWhen you install Docker on a system, it creates\u00a0\u00a0 this folder structure at var lib Docker. You have\u00a0\nmultiple folders under it called au Fs containers,\u00a0\u00a0 image volumes, etc. This is where Docker stores\u00a0\nall its data by default. When I say data, I mean\u00a0\u00a0 files related to images and containers running on\u00a0\nthe Docker host. For example, all files related to\u00a0\u00a0 containers are stored under the containers folder.\u00a0\nAnd the files related to images are stored under\u00a0\u00a0 the image folder. Any volumes created by the\u00a0\nDocker containers. I created under the volumes\u00a0\u00a0 folder. Well, don't worry about that for now. We\u00a0\nwill come back to that in a bit. For now, let's\u00a0\u00a0 just understand where Docker stores its files,\u00a0\nand in what format. So how exactly does Docker\u00a0\u00a0 store the files of an image and a container?\u00a0\nTo understand that we need to understand\u00a0\u00a0 Dockers layered architecture. Let's quickly recap\u00a0\nsomething we learned when Docker builds images,\u00a0\u00a0 it builds these in a layered architecture. Each\u00a0\nline of instruction in the Docker file creates\u00a0\u00a0 a new layer in the Docker image with just the\u00a0\nchanges from the previous layer. For example,\u00a0\u00a0 the first layer is a base Ubuntu Operating System,\u00a0\nfollowed by the second instruction that creates a\u00a0\u00a0 second layer, which installs all the Add packages.\u00a0\nAnd then the third instruction creates a third\u00a0\u00a0 layer, which with the Python packages, followed\u00a0\nby the fourth layer that copies the source code\u00a0\u00a0 over and then finally the fifth layer that\u00a0\nupdates the entry point of the image. Since\u00a0\u00a0 each layer only stores the changes from the\u00a0\nprevious layer, it is reflected in the size as\u00a0\u00a0 well. If you look at the base, a boon to image it\u00a0\nis around 120 megabytes in size. The abt packages\u00a0\u00a0 that are installed is around 300 Mb, and then\u00a0\nthe remaining layers are small. To understand\u00a0\u00a0 the advantages of this layered architecture, let's\u00a0\nconsider a second application. This application\u00a0\u00a0 has a different Docker file. But it's very similar\u00a0\nto our first application, as it uses the same base\u00a0\u00a0 image as Ubuntu uses the same Python and flask\u00a0\ndependencies, but uses a different source code\u00a0\u00a0 to create a different application. And so\u00a0\na different entry point as well. When I run\u00a0\u00a0 the Docker build command to build a new image for\u00a0\nthis application, since the first three layers of\u00a0\u00a0 both the applications are the same, Docker is not\u00a0\ngoing to build the first three layers. Instead,\u00a0\u00a0 it reuses the same three layers it built\u00a0\nfor the first application from the cache,\u00a0\u00a0 and only creates the last two layers with the\u00a0\nnew sources and the new entry point. This way,\u00a0\u00a0 Docker builds images faster and efficiently saves\u00a0\ndisk space. This is also applicable if you were to\u00a0\u00a0 update your application code. Whenever you update\u00a0\nyour application code, such as the app dot p y. In\u00a0\u00a0 this case, Docker simply reuses all the previous\u00a0\nlayers from cash, and quickly rebuilds the\u00a0\u00a0 application image by updating the latest source\u00a0\ncode, thus saving us a lot of time during rebuilds\u00a0\u00a0 and updates. Let's rearrange the layers bottom up\u00a0\nso we can understand it better. At the bottom we\u00a0\u00a0 have the base Ubuntu layer, then the packages,\u00a0\nthen the dependencies, and then the source code\u00a0\u00a0 of the application, and then the entry point. All\u00a0\nof these layers are created when we run the Docker\u00a0\u00a0 build command to form the final Docker image. So\u00a0\nall of these are the Docker image layers. Once the\u00a0\u00a0 build is complete, you cannot modify the contents\u00a0\nof these layers and so they are read only and you\u00a0\u00a0 can only modify them by initiating a new build.\u00a0\nWhen you run a container based off of this image\u00a0\u00a0 using the Docker run command, Docker creates a\u00a0\ncontainer based off of these layers and creates\u00a0\u00a0 a new writable layer on top of the image layer.\u00a0\nThe writable layer is used to store data created\u00a0\u00a0 by the container, such as log files written by the\u00a0\napplications, any temporary files generated by the\u00a0\u00a0 container, or just any file modified by the user\u00a0\non that container. The life of this layer though,\u00a0\u00a0 is only as long as the container is alive. When\u00a0\nthe container is destroyed. This layer and all\u00a0\u00a0 of the changes stored in it are also destroyed.\u00a0\nRemember that the same image layer is shared by\u00a0\u00a0 all containers created using this image. If I were\u00a0\nto log into the newly created container and say,\u00a0\u00a0 create a new file called temp dot txt, it will\u00a0\ncreate that file in the container layer which is\u00a0\u00a0 read and write. We just said that the files in the\u00a0\nimage layer are read only meaning you cannot edit\u00a0\u00a0 anything in those layers. Let's take an example of\u00a0\nour application code. Since we bake our code into\u00a0\u00a0 the image, the code is part of the image layer\u00a0\nand as such is read only after running a container\u00a0\u00a0 What if I wish to modify the source code to say\u00a0\ntest to change. Remember, the same image layer may\u00a0\u00a0 be shared between multiple containers created from\u00a0\nthis image. So does it mean that I cannot modify\u00a0\u00a0 this file inside the container. Now, I can still\u00a0\nmodify this file. But before I save the modified\u00a0\u00a0 file, Docker automatically creates a copy of the\u00a0\nfile in the readwrite layer, and I will then be\u00a0\u00a0 modifying a different version of the file in the\u00a0\nreadwrite layer. All future modifications will\u00a0\u00a0 be done on this copy of the file in the readwrite\u00a0\nlayer. This is called copy on write mechanism. The\u00a0\u00a0 image layer being read only just means that the\u00a0\nfiles in these layers will not be modified in the\u00a0\u00a0 image itself. So the image will remain the same\u00a0\nall the time, until you rebuild the image using\u00a0\u00a0 the Docker build command. What happens when we\u00a0\nget rid of the container, all of the data that was\u00a0\u00a0 stored in the container layer also gets deleted,\u00a0\nthe changes we made to the app.pi and the new temp\u00a0\u00a0 file we created will also get removed. So what if\u00a0\nwe wish to persist this data? For example, if we\u00a0\u00a0 were working with a database, and we would like\u00a0\nto preserve the data created by the container,\u00a0\u00a0 we could add a persistent volume to the container.\u00a0\nTo do this, first create a volume using the Docker\u00a0\u00a0 volume create command. So when I run the Docker\u00a0\nvolume create data underscore volume command,\u00a0\u00a0 it creates a folder called Data underscore volume\u00a0\nunder the var lib Docker volumes directory. Then\u00a0\u00a0 when I run the Docker container using the Docker\u00a0\nrun command, I could mount this volume inside\u00a0\u00a0 the Docker containers rewrite layer using the\u00a0\ndash v option like this. So I would do a Docker\u00a0\u00a0 run dash v then specify my newly created volume\u00a0\nname followed by a colon and the location inside\u00a0\u00a0 my container, which is the default location\u00a0\nwhere MySQL stores data and that is where lib\u00a0\u00a0 MySQL and then the image name MySQL. This will\u00a0\ncreate a new container and mount the data volume\u00a0\u00a0 we created into var lib MySQL folder inside the\u00a0\ncontainer. So all data written by the database\u00a0\u00a0 is in fact stored on the volume created on the\u00a0\nDocker host. Even if the container is destroyed,\u00a0\u00a0 the data is still active. Now what if you didn't\u00a0\nrun the Docker volume create command to create the\u00a0\u00a0 volume before the Docker run command. For example,\u00a0\nif I run the Docker run command to create a new\u00a0\u00a0 instance of MySQL container with the volume data\u00a0\nunderscore Volume Two, which I have not created\u00a0\u00a0 yet, Docker will automatically create a volume\u00a0\nnamed data underscore Volume Two and mounted\u00a0\u00a0 to the container. You should be able to see all\u00a0\nthese volumes if you list the contents of the var\u00a0\u00a0 lib Docker volumes folder. This is called volume\u00a0\nmounting. As we are mounting a volume created by\u00a0\u00a0 Docker under the var lib Docker volumes folder.\u00a0\nBut what if we had our data already at another\u00a0\u00a0 location for example, let's say we have some\u00a0\nexternal storage on the Docker host at four\u00a0\u00a0 slash data. And we would like to store database\u00a0\ndata on that volume and not in the default var lib\u00a0\u00a0 Docker volumes folder. In that case, we will run\u00a0\na container using the command Docker run dash v.\u00a0\u00a0 But in this case, we will provide the complete\u00a0\npath to the folder we would like to mount that\u00a0\u00a0 is for slash data forward slash MySQL and so it\u00a0\nwill create a container and mount the folder to\u00a0\u00a0 the container. This is called bind mounting. So\u00a0\nthere are two types of mounts a volume mounting\u00a0\u00a0 and a bind mount volume mount mount a volume\u00a0\nfrom the volumes directory and bind mount mounts\u00a0\u00a0 a directory from any location on the Docker host.\u00a0\nOne final point note before I let you go using the\u00a0\u00a0 dash V is an old style. The new way is to use dash\u00a0\nmount option. The dash dash mount is the preferred\u00a0\u00a0 way as it is more verbose. So you have to specify\u00a0\neach parameter in a key equals value format. For\u00a0\u00a0 example, the previous command can be written with\u00a0\nthe dash mount option as this using the type,\u00a0\u00a0 source and target options. The type in this\u00a0\ncase is bind. The source is the location on my\u00a0\u00a0 host and target is the location on my container.\u00a0\nSo who He's responsible for doing all of these\u00a0\u00a0 operations, maintaining the layered architecture,\u00a0\ncreating a writable layer moving files across\u00a0\u00a0 layers to enable, copy and write etc. It's the\u00a0\nstorage drivers. So Docker uses storage drivers\u00a0\u00a0 to enable layered architecture. Some of the common\u00a0\nstorage drivers are au Fs btrfs DFS device mapper\u00a0\u00a0 Overlay and overlay to the selection of the\u00a0\nstorage driver depends on the underlying OS\u00a0\u00a0 being used, for example, with a boon to the\u00a0\ndefault storage driver is au Fs whereas this\u00a0\u00a0 storage driver is not available on other operating\u00a0\nsystems like Fedora or CentOS. In that case,\u00a0\u00a0 device mapper may be a better option. Docker\u00a0\nwill choose the best storage driver available\u00a0\u00a0 automatically based on the operating system. The\u00a0\ndifferent storage drivers also provide different\u00a0\u00a0 performance and stability characteristics. So you\u00a0\nmay want to choose one that fits the needs of your\u00a0\u00a0 application, and your organization. If you would\u00a0\nlike to read more on any of these stories drivers,\u00a0\u00a0 please refer to the links in the\u00a0\nattached documentation. For now,\u00a0\u00a0 that is all from the Docker architecture concepts.\u00a0\nSee you in the next lecture. Hello, and welcome to\u00a0\u00a0 this lecture on Docker compose. Going forward, we\u00a0\nwill be working with configurations in yamo file,\u00a0\u00a0 so it is important that you are comfortable with\u00a0\ntmo. Let's recap a few things real quick course we\u00a0\u00a0 first learned how to run a Docker container using\u00a0\nthe Docker run command. If we needed to set up a\u00a0\u00a0 complex application running multiple services, a\u00a0\nbetter way to do it is to use Docker compose with\u00a0\u00a0 Docker compose, we could create a configuration\u00a0\nfile in yamo format called Docker compose dot yamo\u00a0\u00a0 and put together the different services and the\u00a0\noptions specific to this to running them in this\u00a0\u00a0 file. Then we could simply run a Docker compose\u00a0\nup command to bring up the entire application\u00a0\u00a0 stack is easier to implement, run and maintain\u00a0\nas all changes are always stored in the Docker\u00a0\u00a0 compose configuration file. However, this is all\u00a0\nonly applicable to running containers on a single\u00a0\u00a0 Docker host. And for now, don't worry about the\u00a0\nyamo file, we will take a closer look at the yamo\u00a0\u00a0 file in a bit and see how to put it together.\u00a0\nThat was a really simple application that I\u00a0\u00a0 put together. Let us look at a better example.\u00a0\nI'm going to use the same sample application\u00a0\u00a0 that everyone uses to demonstrate Docker. It's\u00a0\na simple yet comprehensive application developed\u00a0\u00a0 by Docker to demonstrate the various features\u00a0\navailable in running an application stack on\u00a0\u00a0 Docker. So let's first get familiarized with the\u00a0\napplication, because we will be working with the\u00a0\u00a0 same application in different sections through\u00a0\nthe rest of this course. This is a sample voting\u00a0\u00a0 application which provides an interface for a user\u00a0\nto vote and another interface to show the results.\u00a0\u00a0 The application consists of various components\u00a0\nsuch as the voting app, which is a web application\u00a0\u00a0 developed in Python, to provide the user with\u00a0\nan interface to choose between two options,\u00a0\u00a0 a cat and the dog. When you make a selection,\u00a0\nthe vote is stored in Redis. For those of you\u00a0\u00a0 who are new to Redis Redis, in this case serves as\u00a0\na database in memory. This vote is then processed\u00a0\u00a0 by the worker which is an application written in\u00a0\ndotnet. The worker application takes the new vote\u00a0\u00a0 and updates the persistent database, which is\u00a0\na Postgres SQL, in our case, the Postgres SQL\u00a0\u00a0 simply has a table with a number of votes for\u00a0\neach category, cats and dogs. In this case,\u00a0\u00a0 it increments the number of votes for cats as\u00a0\nour vote was for cats. Finally, the result of\u00a0\u00a0 the vote is displayed in a web interface, which is\u00a0\nanother web application developed in Node JS. This\u00a0\u00a0 resulting application rates the count of votes\u00a0\nfrom the Postgres SQL database and displays it\u00a0\u00a0 to the user. So that is the architecture and\u00a0\ndata flow of this simple voting application\u00a0\u00a0 stack. As you can see, this sample application is\u00a0\nbuilt with a combination of different services,\u00a0\u00a0 different development tools, and multiple\u00a0\ndifferent development platforms. So As Python,\u00a0\u00a0 no js, dotnet, etc. This sample application will\u00a0\nbe used to showcase how easy it is to set up an\u00a0\u00a0 entire application stack consisting of diverse\u00a0\ncomponents in Docker. Let us keep aside Docker\u00a0\u00a0 swarm services and stacks for a minute and see\u00a0\nhow we can put together this application stack\u00a0\u00a0 on a single Docker engine using first Docker run\u00a0\ncommands, and then Docker compose. Let us assume\u00a0\u00a0 that all images of applications are already\u00a0\nbuilt and are available on Docker repository.\u00a0\u00a0 Let's start with the data layer. First, we run\u00a0\nthe Docker run command to start an instance of\u00a0\u00a0 Redis. By running the Docker run Redis command,\u00a0\nwe will add the dash D parameter to run this\u00a0\u00a0 container in the background. And we will also name\u00a0\nthe container Redis. Now naming the containers is\u00a0\u00a0 important. Why is that important? Hold that\u00a0\nthought we will come to that in a bit. Next,\u00a0\u00a0 we will deploy the Postgres SQL database by\u00a0\nrunning the Docker run Postgres command. This\u00a0\u00a0 time too, we will add the dash D option to run\u00a0\nthis in the background and name this container\u00a0\u00a0 DB for database. Next, we will start with the\u00a0\napplication services. We will deploy a front end\u00a0\u00a0 app for voting interface by running an instance\u00a0\nof voting app image, run the Docker run command\u00a0\u00a0 and name the instance vote. Since this is a web\u00a0\nserver, it has a web UI instance running on port\u00a0\u00a0 80. We will publish that port to 5000 on the host\u00a0\nsystem, so we can access it from a browser. Next,\u00a0\u00a0 we will deploy the result web application\u00a0\nthat shows the results to the user. For this\u00a0\u00a0 we deploy a container using the results stash\u00a0\napp image and publish Port 80 to Port 5001 on\u00a0\u00a0 the host. This way, we can access the web UI\u00a0\nof the resulting app on a browser. Finally,\u00a0\u00a0 we deploy the worker by running an instance of\u00a0\nthe worker image. Okay, now this is all good. And\u00a0\u00a0 we can see that all the instances are running on\u00a0\nthe host. But there is some problem, it just does\u00a0\u00a0 not seem to work. The problem is that we have\u00a0\nsuccessfully run all the different containers,\u00a0\u00a0 but we haven't actually linked them together. As\u00a0\nin we haven't told the voting web application to\u00a0\u00a0 use this particular Redis instance, there could\u00a0\nbe multiple Redis instances running. We haven't\u00a0\u00a0 told the worker and the resulting app to use this\u00a0\nparticular Postgres SQL database that we ran. So\u00a0\u00a0 how do we do that? That is where we use links.\u00a0\nLink is a command line option, which can be used\u00a0\u00a0 to link two containers together. For example, the\u00a0\nvoting app web service is dependent on the Redis\u00a0\u00a0 service. When the web server starts, as you can\u00a0\nsee, in this piece of code on the web server, it\u00a0\u00a0 looks for a Redis service running on host Redis.\u00a0\nBut the voting app container cannot resolve a host\u00a0\u00a0 by the name Redis. To make the voting app aware\u00a0\nof the Redis service, we add a link option while\u00a0\u00a0 running the voting app container to link it to the\u00a0\nRedis container, adding a dash dash link option to\u00a0\u00a0 the Docker run command and specifying the name\u00a0\nof the Redis container which is which in this\u00a0\u00a0 case is Redis followed by a colon and the name\u00a0\nof the host that the voting app is looking for,\u00a0\u00a0 which is also Redis. In this case, remember that\u00a0\nthis is why we named the container when we ran\u00a0\u00a0 it the first time so we could use its name while\u00a0\ncreating a link. What this is, in fact doing is\u00a0\u00a0 it creates an entry into the EDC host file on\u00a0\nthe voting app container, adding an entry with\u00a0\u00a0 the host name Redis with the internal IP of the\u00a0\nRedis container. Similarly, we add a link for\u00a0\u00a0 the result app to communicate with the database by\u00a0\nadding a link option to refer the database by the\u00a0\u00a0 name dB. As you can see, in this source code of\u00a0\nthe application, it makes an attempt to connect\u00a0\u00a0 to a Postgres database on host dB. Finally, the\u00a0\nworker application requires access to both the\u00a0\u00a0 Redis as well as the Postgres database. So we\u00a0\nadd two links to the worker application, one\u00a0\u00a0 link to link the Redis and the other link to link\u00a0\nPostgres database. Note that using links this way,\u00a0\u00a0 is deprecated and the support may be removed in\u00a0\nfuture in Docker. This is because as we will see\u00a0\u00a0 in some time Advanced and newer concepts in\u00a0\nDocker swarm and networking supports better\u00a0\u00a0 ways of achieving what we just did here with\u00a0\nlinks. But I wanted to mention it anyways, so\u00a0\u00a0 you learn the concept from the very basics. Once\u00a0\nwe have the Docker run commands tested and ready,\u00a0\u00a0 it is easy to generate a Docker compose file from\u00a0\nit. We start by creating a dictionary of container\u00a0\u00a0 names, we will use the same name we used in the\u00a0\nDocker run commands. So we take all the names and\u00a0\u00a0 create a key with each of them. Then under each\u00a0\nitem, we specify which image to use. The key is\u00a0\u00a0 the image and the value is the name of the image\u00a0\nto use. Next, inspect the commands and see what\u00a0\u00a0 are the other options used. We published ports.\u00a0\nSo let's move those ports under the respective\u00a0\u00a0 containers. So we create a property called ports\u00a0\nand list all the ports that you would like to\u00a0\u00a0 publish under that. Finally, we are left with\u00a0\nlinks. So whichever container requires the link,\u00a0\u00a0 create a property under it called links and\u00a0\nprovide an array of links such as Redis, or TP.\u00a0\u00a0 Note that you could also specify the name of the\u00a0\nlink this way without the semicolon and and the\u00a0\u00a0 target target name, and it will create a link with\u00a0\nthe same name as the target name. specifying the\u00a0\u00a0 DB colon DB is similar to simply specifying dB,\u00a0\nwe will assume the same value to create a link.\u00a0\u00a0 Now that we're all done with our Docker compose\u00a0\nfile, bringing up the stack is really simple from\u00a0\u00a0 the Docker compose up command to bring up the\u00a0\nentire application stack. When we looked at the\u00a0\u00a0 example of the sample voting application, we\u00a0\nassumed that all images are already built out\u00a0\u00a0 of the five different components. Two of them\u00a0\nRedis and Postgres images we know are already\u00a0\u00a0 available on Docker Hub. There are official images\u00a0\nfrom Redis and Postgres. But the remaining three\u00a0\u00a0 are our own application, it is not necessary\u00a0\nthat they are already built and available in\u00a0\u00a0 the Docker registry. If we would like to instruct\u00a0\nDocker compose to run a Docker build, instead of\u00a0\u00a0 trying to pull an image, we can replace the image\u00a0\nline with a built line and specify the location\u00a0\u00a0 of a directory, which contains the application\u00a0\ncode, and a Docker file with instructions to\u00a0\u00a0 build the Docker image. In this example, for the\u00a0\nvoting app, I have all the application code in a\u00a0\u00a0 folder named vote which contains all application\u00a0\ncode, and a Docker file. This time when you run\u00a0\u00a0 the Docker compose up command, it will first\u00a0\nbuild the images, give a temporary name for it,\u00a0\u00a0 and then use those images to run containers using\u00a0\nthe options you specified before. Similarly use\u00a0\u00a0 build option to build the two other services\u00a0\nfrom the respective folders. We will now look\u00a0\u00a0 at different versions of Docker compose file. This\u00a0\nis important because you might see Docker compose\u00a0\u00a0 files in different formats at different places\u00a0\nand wonder why some look different. Docker compose\u00a0\u00a0 evolved over time, and now supports a lot more\u00a0\noptions than it did in the beginning. For example,\u00a0\u00a0 this is the trimmed down version of the Docker\u00a0\ncompose file we used earlier. This is in fact,\u00a0\u00a0 the original version of Docker compose file known\u00a0\nas version one. This had a number of limitations.\u00a0\u00a0 For example, if you wanted to deploy containers on\u00a0\na different network other than the default bridge\u00a0\u00a0 network, there was no way of specifying\u00a0\nthat in this version of the file. Also,\u00a0\u00a0 say you have a dependency or startup order of some\u00a0\nkind. For example, your database container must\u00a0\u00a0 come up first and only then I should the voting\u00a0\napplication be started. There was no way you could\u00a0\u00a0 specify that in the version one of the Docker\u00a0\ncompose file support for these came in version\u00a0\u00a0 two. With version two and up, the format of the\u00a0\nfile also changed a little bit. You no longer\u00a0\u00a0 specify your stack information directly as you\u00a0\ndid before. It is all encapsulated in Services\u00a0\u00a0 section. So create a property called services\u00a0\nin the root of the file, and then move all the\u00a0\u00a0 services underneath that. You will still use the\u00a0\nsame Docker compose up command to bring up your\u00a0\u00a0 application stack. But how does Docker compose\u00a0\nknow what version of the file you're using? You're\u00a0\u00a0 free to use version one or version two depending\u00a0\non your needs. So how does the Docker compose,\u00a0\u00a0 know what format you're using. For version two\u00a0\nand up, you must specify the version of Docker\u00a0\u00a0 compose file you're intending to use by specifying\u00a0\nthe version at the top of the file. In this case,\u00a0\u00a0 version, colon two. Another difference is with\u00a0\nnetworking. In version one, Docker compose\u00a0\u00a0 attaches all the containers, it runs to the\u00a0\ndefault bridged network. And then use links to\u00a0\u00a0 enable communication between the containers as\u00a0\nwe did before. With version two Docker compose\u00a0\u00a0 automatically creates a dedicated bridged network\u00a0\nfor this application, and then attaches all\u00a0\u00a0 containers to that new network. All containers are\u00a0\nthen able to communicate to each other using each\u00a0\u00a0 other's service name. So you basically don't need\u00a0\nto use links in version two of Docker compose,\u00a0\u00a0 you can simply get rid of all the links you\u00a0\nmentioned in version one when you convert a\u00a0\u00a0 file from version one to version two. And finally,\u00a0\nmotion to also introduces a depends on feature.\u00a0\u00a0 If you wish to specify a startup order. For\u00a0\ninstance, say the voting web application is\u00a0\u00a0 dependent on the Redis service. So you need to\u00a0\nensure that Redis container is started first,\u00a0\u00a0 and only then the voting web application must\u00a0\nbe started. We could add a depends on property\u00a0\u00a0 to the voting application and indicate that it\u00a0\nis dependent on Redis. Then comes version three,\u00a0\u00a0 which is the latest as of today. version three\u00a0\nis similar to version two in the structure,\u00a0\u00a0 meaning it has a version specification at the top\u00a0\nand the Services section under which you put all\u00a0\u00a0 your services just like in version two, make sure\u00a0\nto specify the version number as three at the top.\u00a0\u00a0 version three comes with support for Docker swarm,\u00a0\nwhich we will see later on. There are some options\u00a0\u00a0 that were removed and added. To see details on\u00a0\nthose you can refer to the documentation section\u00a0\u00a0 using the link in the reference page. Following\u00a0\nthis lecture. We will see version three in much\u00a0\u00a0 detail later, when we discuss about Docker\u00a0\nstacks. Let's talk about networks in Docker\u00a0\u00a0 compose. Getting back to our application. So far,\u00a0\nwe've been just deploying all containers on the\u00a0\u00a0 default bridge network. Let us say we modify the\u00a0\narchitecture a little bit to contain the traffic\u00a0\u00a0 from the different sources. For example, we would\u00a0\nlike to separate the user generated traffic from\u00a0\u00a0 the applications internal traffic. So we create\u00a0\na front end network dedicated for traffic from\u00a0\u00a0 users, and a back end network dedicated for\u00a0\ntraffic within the application. We then connect\u00a0\u00a0 the user facing applications which are the voting\u00a0\napp and the result app to the front end network\u00a0\u00a0 and all the components to an internal back end\u00a0\nnetwork. So back in our Docker compose file,\u00a0\u00a0 note that I have actually stripped out\u00a0\nthe port section for simplicity's sake,\u00a0\u00a0 there's still there, but they're just not shown\u00a0\nhere. The first thing we need to do if we were\u00a0\u00a0 to use networks is to define the networks we are\u00a0\ngoing to use. In our case, we have two networks,\u00a0\u00a0 front end and back end. So create a new property\u00a0\ncalled networks at the root level, and listen to\u00a0\u00a0 the services in the Docker compose file and add\u00a0\na map of networks we are planning to use. Then,\u00a0\u00a0 under each service, create a network's property\u00a0\nand provide a list of networks that service must\u00a0\u00a0 be attached to. In case of Redis and dB. It's\u00a0\nonly the back end network. In case of the front\u00a0\u00a0 end applications such as the voting app and the\u00a0\nresult app, they're required to be a test to both\u00a0\u00a0 a front end and back end network. You must also\u00a0\nadd a section for worker container to be added\u00a0\u00a0 to the back end network. I have just omitted that\u00a0\nin this slide due to space constraints. Now that\u00a0\u00a0 you have seen Docker compose files, head over\u00a0\nto the coding exercises and practice developing\u00a0\u00a0 some Docker compose files. That's it for this\u00a0\nlecture. And I will see you in the next lecture.\u00a0\u00a0 We will now look at Docker registry. So what is\u00a0\na registry if the containers Where the rain then\u00a0\u00a0 they will rain from the Docker registry, which\u00a0\nare the clouds. That's where Docker images are\u00a0\u00a0 stored. It's a central repository of all Docker\u00a0\nimages. Let's look at a simple nginx container.\u00a0\u00a0 We run the Docker run nginx command to run\u00a0\nan instance of the nginx image. Let's take a\u00a0\u00a0 closer look at that image name. Now the name is\u00a0\nnginx. But what is this image and where is this\u00a0\u00a0 image pulled from? This name follows Dockers image\u00a0\nnaming convention nginx. Here is the image or the\u00a0\u00a0 repository name. When you say nginx, it's actually\u00a0\nnginx slash nginx. The first part stands for the\u00a0\u00a0 user or account name. So if you don't provide an\u00a0\naccount or a repository name, it assumes that it\u00a0\u00a0 is the same as the given name, which in this case\u00a0\nis nginx. The usernames is usually your Docker\u00a0\u00a0 Hub account name or if it is an organization,\u00a0\nthen it's the name of the organization. If you\u00a0\u00a0 were to create your own account and create\u00a0\nyour own repositories or images under it,\u00a0\u00a0 then you would use a similar pattern. Now, where\u00a0\nare these images stored and pulled from? Since\u00a0\u00a0 we have not specified the location where these\u00a0\nimages are to be pulled from, it is assumed to be\u00a0\u00a0 on Dockers default registry, Docker Hub, the DNS\u00a0\nname for which is docker.io. The registry is where\u00a0\u00a0 all the images are stored. Whenever you create a\u00a0\nnew image or update an existing image, you push\u00a0\u00a0 it to the registry and every time anyone deploys\u00a0\nthis application, it is pulled from that registry.\u00a0\u00a0 There are many other popular registries as well.\u00a0\nFor example, Google's registry is that dcr.io,\u00a0\u00a0 where a lot of Kubernetes related images are\u00a0\nstored, like the ones used for performing end to\u00a0\u00a0 end tests on the cluster. These are all publicly\u00a0\naccessible images that anyone can download,\u00a0\u00a0 and access. When you have applications built in\u00a0\nhouse that shouldn't be made available to the\u00a0\u00a0 public. hosting an internal private registry may\u00a0\nbe a good solution. Many cloud service providers\u00a0\u00a0 such as AWS, Azure, or GCP, provide a private\u00a0\nregistry by default, when you open an account with\u00a0\u00a0 them. on any of these solutions via Docker Hub, or\u00a0\nGoogle registry or your internal private registry,\u00a0\u00a0 you may choose to make a repository private,\u00a0\nso that it can only be accessed using a set\u00a0\u00a0 of credentials from Dockers perspective. To run a\u00a0\ncontainer using an image from a private registry.\u00a0\u00a0 you first log into your private registry using\u00a0\nthe Docker login command, input your credentials.\u00a0\u00a0 Once successful, run the application using private\u00a0\nregistry as part of the image name like this. Now,\u00a0\u00a0 if you did not log into the private registry, it\u00a0\nwill come back saying that the image cannot be\u00a0\u00a0 found. So remember to always log in before pulling\u00a0\nor pushing to a private registry. We said that\u00a0\u00a0 cloud providers like AWS or GCP provide a private\u00a0\nregistry when you create an account with them.\u00a0\u00a0 But what if you're running your application on\u00a0\npremise and don't have a private registry? How do\u00a0\u00a0 you deploy your own private registry within your\u00a0\norganization, the Docker registry, itself another\u00a0\u00a0 application, and of course, is available as a\u00a0\nDocker image. The name of the image is registry,\u00a0\u00a0 and it exposes the API on port 5000. Now that you\u00a0\nhave your custom registry running at Port 5000,\u00a0\u00a0 on this Docker host, how do you push your own\u00a0\nimage to it? Use the Docker image tag command\u00a0\u00a0 to tag the image with the private registry URL\u00a0\nin it. In this case, since it's running on the\u00a0\u00a0 same Docker host, I can use local host semi colon\u00a0\n5000 followed by the image name. I can then push\u00a0\u00a0 my image to my local private registry using the\u00a0\ncommand Docker push and the new image name with\u00a0\u00a0 the Docker registry information in it. From there\u00a0\non, I can pull my image from anywhere within this\u00a0\u00a0 network using either localhost if you're on the\u00a0\nsame host, or the IP or domain name of my Docker\u00a0\u00a0 host, if I'm accessing from another host in my\u00a0\nenvironment. Well, that's it for this lecture,\u00a0\u00a0 head over to the practice test and practice\u00a0\nworking with private Docker registries.\u00a0\u00a0 Welcome to this lecture on Docker engine.\u00a0\nIn this lecture, we will take a deeper look\u00a0\u00a0 at Dockers architecture, how it actually runs\u00a0\napplications in isolated containers and how it\u00a0\u00a0 works under the hood. Docker engine as we have\u00a0\nlearned before you simply refer to a host with\u00a0\u00a0 Docker installed on it. When you install Docker\u00a0\non a Linux host, you're actually installing three\u00a0\u00a0 different components. The Docker daemon, the REST\u00a0\nAPI server, and the Docker CLA. The Docker daemon\u00a0\u00a0 is a background process that manages Docker\u00a0\nobjects such as the images, containers, volumes\u00a0\u00a0 and networks. The Docker REST API server is the\u00a0\nAPI interface that programs can use to talk to the\u00a0\u00a0 daemon and provide instructions, you could create\u00a0\nyour own tools using this REST API. And the Docker\u00a0\u00a0 CLA is nothing but the command line interface\u00a0\nthat we've been using, until now to perform\u00a0\u00a0 actions such as running a container stopping\u00a0\ncontainers, destroying images, etc. It uses\u00a0\u00a0 the REST API to interact with the Docker daemon.\u00a0\nSomething to note here is that the Docker CLA need\u00a0\u00a0 not necessarily be on the same host. It could be\u00a0\non another system like a laptop, and can still\u00a0\u00a0 work with a remote Docker engine. Simply use the\u00a0\ndash H option on the Docker command and specify\u00a0\u00a0 the remote Docker engine address and a port\u00a0\nas shown here. For example, to run a container\u00a0\u00a0 based on nginx on a remote Docker host, run the\u00a0\ncommand Docker dash H equals 10.1 23 dot 2.1 colon\u00a0\u00a0 2375 run ngi nx. Now let's try and understand how\u00a0\nexactly our applications containerized in Docker,\u00a0\u00a0 how does it work under the hood, Docker uses\u00a0\nnamespaces to isolate workspace, process IDs,\u00a0\u00a0 network, inter process communication, mounds\u00a0\nand Unix time sharing systems are created in\u00a0\u00a0 their own namespace thereby providing isolation\u00a0\nbetween containers. Let's take a look at one of\u00a0\u00a0 the namespace isolation technique. process ID\u00a0\nnamespaces. Whenever a Linux system boots up,\u00a0\u00a0 it starts with just one process with a process\u00a0\nID of one. This is the root process and kicks\u00a0\u00a0 off all the other processes in the system.\u00a0\nBy the time the system boots up completely,\u00a0\u00a0 we have a handful of processes running. This can\u00a0\nbe seen by running the PS command to list all the\u00a0\u00a0 running processes. The process IDs are unique\u00a0\nand two processes cannot have the same process\u00a0\u00a0 ID. Now if we were to create a container, which is\u00a0\nbasically like a child system within the current\u00a0\u00a0 system, the child system needs to think that it\u00a0\nis an independent system on its own. And it has\u00a0\u00a0 its own set of processes originating from a root\u00a0\nprocess with a process ID of one. But we know that\u00a0\u00a0 there is no hard isolation between the containers\u00a0\nand the underlying host. So the processes running\u00a0\u00a0 inside the container are in fact processes running\u00a0\non the underlying host. And so two processes\u00a0\u00a0 cannot have the same process ID of one. This is\u00a0\nwhere namespaces come into play. With process\u00a0\u00a0 ID namespaces. Each process can have multiple\u00a0\nprocess IDs associated with it. For example,\u00a0\u00a0 when the processes start in the container, it's\u00a0\nactually just another set of processes on the\u00a0\u00a0 base Linux system. And it gets the next available\u00a0\nprocess ID in this case, five and six. However,\u00a0\u00a0 they also get another process ID starting with P\u00a0\nID one in the container namespace which is only\u00a0\u00a0 visible inside the container. So the container\u00a0\nthings that it has its own route process tree. And\u00a0\u00a0 so it is an independent system. So how does that\u00a0\nrelate to an actual system? How do you see this on\u00a0\u00a0 a host? Let's say I were to run an nga next server\u00a0\nas a container. We know that the nginx container\u00a0\u00a0 runs and nginx service. If we were to list all the\u00a0\nservices inside the Docker container, we see that\u00a0\u00a0 the next service running with a process ID of one.\u00a0\nThis is the process ID of the service inside of\u00a0\u00a0 the container namespace. If we list the services\u00a0\non the Docker host, we will see the same service\u00a0\u00a0 but with a different process ID that indicates\u00a0\nthat all processes are in fact running on the same\u00a0\u00a0 host but separated into their own containers using\u00a0\nnamespaces. So we learned that the underlying\u00a0\u00a0 Docker host as well as the containers share the\u00a0\nsame system resources such as CPU and memory. How\u00a0\u00a0 much of the resources are dedicated to the host\u00a0\nand the containers? And how does Docker manage\u00a0\u00a0 and share the resources between the containers.\u00a0\nBy default, there is no restriction as to how\u00a0\u00a0 much of a resource a container can use. And\u00a0\nhence, a container may end up utilizing all\u00a0\u00a0 of the resources on the underlying host. But there\u00a0\nis a way to restrict the amount of CPU or memory\u00a0\u00a0 a container can use. Docker uses thi groups or\u00a0\ncontrol groups to restrict the amount of hardware\u00a0\u00a0 resources allocated to each container. This can\u00a0\nbe done by providing the dash dash CPUs option\u00a0\u00a0 to the Docker run command. Providing a value of\u00a0\npoint five will ensure that the container does\u00a0\u00a0 not take up more than 50% of the host CPU at any\u00a0\ngiven time. The same goes with memory. Setting a\u00a0\u00a0 value of 100 M to the dash dash memory option\u00a0\nlimits the amount of memory the container can\u00a0\u00a0 use to 100 megabytes. If you're interested in\u00a0\nreading more on this topic, refer to the links\u00a0\u00a0 I posted in the reference page. That's it for now\u00a0\non Docker engine. In the next lecture, we talked\u00a0\u00a0 about other advanced topics on Docker storage and\u00a0\nfile systems. See you in the next lecture. During\u00a0\u00a0 this course, we learned that containers share the\u00a0\nunderlying OS kernel. And as a result, we cannot\u00a0\u00a0 have a Windows container running on Linux hosts\u00a0\nor vice versa. We need to keep this in mind while\u00a0\u00a0 going through this lecture, as it's very important\u00a0\nconcept and most beginners tend to have an issue\u00a0\u00a0 with it. So what are the options available\u00a0\nfor Docker on Windows? There are two options\u00a0\u00a0 available. The first one is Docker on Windows\u00a0\nusing Docker toolbox. And the second one is the\u00a0\u00a0 Docker desktop for Windows option. We will look at\u00a0\neach of these now. Let's take a look at the first\u00a0\u00a0 option. Docker toolbox. This was the original\u00a0\nsupport for Docker on Windows. Imagine that you\u00a0\u00a0 have a Windows laptop and no access to any Linux\u00a0\nsystem whatsoever. But you would like to try\u00a0\u00a0 Docker, you don't have access to a Linux system in\u00a0\nthe lab or in the cloud. What would you do? What\u00a0\u00a0 I did was to install a virtualization software\u00a0\non my Windows system like Oracle VirtualBox or\u00a0\u00a0 VMware Workstation and deploy a Linux VM on it\u00a0\nsuch as Ubuntu or Debian, then install Docker on\u00a0\u00a0 the Linux VM and then play around with it. This\u00a0\nis what the first option really does. It doesn't\u00a0\u00a0 really have anything much to do with Windows,\u00a0\nyou cannot create Windows based Docker images,\u00a0\u00a0 or run Windows based Docker containers. You\u00a0\nobviously cannot run Linux container directly on\u00a0\u00a0 Windows either. You're just working with Docker on\u00a0\na Linux virtual machine on a Windows host. Docker,\u00a0\u00a0 however, provides us with a set of tools to make\u00a0\nthis easy which is called as the Docker toolbox.\u00a0\u00a0 The Docker toolbox contains a set of tools like\u00a0\nOracle VirtualBox, Docker engine, Docker machine,\u00a0\u00a0 Docker compose and a user interface called\u00a0\nkite Matic. This will help you get started\u00a0\u00a0 by simply downloading and running the Docker\u00a0\ntoolbox executable. Ever install VirtualBox,\u00a0\u00a0 deploy a lightweight VM called boot to Docker,\u00a0\nwhich has Docker running in it already so that\u00a0\u00a0 you are all set to start with Docker easily\u00a0\nand with within a short period of time. Now\u00a0\u00a0 what about requirements, you must ensure that\u00a0\nyour operating system is 64 bit Windows seven\u00a0\u00a0 or higher and that the virtualization\u00a0\nis enabled on the system. Now remember,\u00a0\u00a0 Docker toolbox is legacy version for older\u00a0\nWindows systems that do not meet requirements\u00a0\u00a0 for the newer Docker for Windows option. The\u00a0\nsecond option is the newer option called Docker\u00a0\u00a0 desktop for Windows. In the previous option,\u00a0\nwe saw that we had Oracle VirtualBox installed\u00a0\u00a0 on Windows and then a Linux system and then Docker\u00a0\non that Linux system. Now with Docker for Windows,\u00a0\u00a0 we take out Oracle VirtualBox and use the native\u00a0\nvirtualization technology available with Windows\u00a0\u00a0 called Microsoft Hyper V. During the installation\u00a0\nprocess for Docker for Windows, it will still\u00a0\u00a0 automatically create a Linux system underneath.\u00a0\nBut this time it is created on the Microsoft Hyper\u00a0\u00a0 V instead of Oracle VirtualBox and have Docker\u00a0\nrunning on that system. Because of this dependency\u00a0\u00a0 on Hyper V. This option is only supported for\u00a0\nWindows 10 Enterprise or Professional Edition\u00a0\u00a0 and on Windows Server 2016. Because both\u00a0\nthese operating systems come with Hyper V\u00a0\u00a0 support by default. Now here's the most important\u00a0\npoint. So far whatever we've been discussing,\u00a0\u00a0 with Dockers support for Windows. It is strictly\u00a0\nfor Linux containers, Linux applications packaged\u00a0\u00a0 into Linux Docker images. We're not talking\u00a0\nabout Windows applications. Windows images\u00a0\u00a0 or Windows containers. Both the options we just\u00a0\ndiscussed will help you run a Linux container on a\u00a0\u00a0 Windows host. With Windows Server 2016, Microsoft\u00a0\nannounced support for Windows containers for the\u00a0\u00a0 first time. You can now package applications\u00a0\nWindows applications into Windows Docker\u00a0\u00a0 containers and run them on a Windows Docker host\u00a0\nusing Docker desktop for Windows. When you install\u00a0\u00a0 Docker desktop for Windows, the default option\u00a0\nis to work with Linux containers. But if you\u00a0\u00a0 would like to run Windows containers, then you\u00a0\nmust explicitly configure Docker for Windows to\u00a0\u00a0 switch to using Windows containers. In early\u00a0\n2016, Microsoft announced windows containers.\u00a0\u00a0 Now you could create Windows based images and\u00a0\nrun Windows containers on a Windows Server just\u00a0\u00a0 like how you would run Linux containers on a\u00a0\nLinux system. You can create windows images,\u00a0\u00a0 containerize your applications and share them\u00a0\nthrough the Docker store as well. Unlike in Linux,\u00a0\u00a0 there are two types of containers in Windows. The\u00a0\nfirst one is a Windows Server container, which\u00a0\u00a0 works exactly like Linux containers where the OS\u00a0\nkernel is shared with the underlying operating\u00a0\u00a0 system to allow better security boundary between\u00a0\ncontainers and to have kernels with different\u00a0\u00a0 versions and configurations to coexist. The\u00a0\nsecond option was introduced known as the Hyper\u00a0\u00a0 V isolation. With Hyper V isolation each container\u00a0\nis run within a highly optimized virtual machine\u00a0\u00a0 guaranteeing complete kernel isolation between the\u00a0\ncontainers and the underlying host. Now while in\u00a0\u00a0 the Linux world, you had a number of base images\u00a0\nfor Linux systems such as Ubuntu, Debian, Fedora,\u00a0\u00a0 Alpine etc. If you remember that, that is what you\u00a0\nspecify at the beginning of the Docker file. In\u00a0\u00a0 the windows world, we have two options the Windows\u00a0\nServer Core and Nano Server. A Nano Server is a\u00a0\u00a0 headless deployment option for Windows Server\u00a0\nwhich runs at a fraction of size of the full\u00a0\u00a0 operating system. You can think of this like the\u00a0\nAlpine image in Linux. The Windows Server Core,\u00a0\u00a0 though is not as light weight as you might\u00a0\nexpect it to be. Finally, Windows containers\u00a0\u00a0 are supported on Windows Server 2016 Nano\u00a0\nServer and Windows 10 professional Enterprise\u00a0\u00a0 Edition. Remember on Windows 10 professional\u00a0\nand Enterprise Edition only supports Hyper V\u00a0\u00a0 isolated containers. Meaning as we just discussed,\u00a0\nevery container deployed is deployed on a highly\u00a0\u00a0 optimized virtual machine. Well, that's it about\u00a0\nDocker on Windows. Now before I finish, I want to\u00a0\u00a0 point out one important fact. We saw two ways of\u00a0\nrunning a Docker container using VirtualBox or\u00a0\u00a0 Hyper V. But remember, VirtualBox and Hyper\u00a0\nV cannot coexist on the same windows host.\u00a0\u00a0 So if you started off with Docker toolbox with\u00a0\nVirtualBox and if you plan to migrate to Hyper V,\u00a0\u00a0 remember you cannot have both solutions at the\u00a0\nsame time. There is a migration guide available\u00a0\u00a0 on Docker documentation page on how to migrate\u00a0\nfrom VirtualBox to Hyper V. That's it for now.\u00a0\u00a0 Thank you and I will see you the next lecture. We\u00a0\nnow look at Docker on Mac Docker on Mac is similar\u00a0\u00a0 to Docker on Windows. There are two options to\u00a0\nget started Docker on Mac using Docker toolbox,\u00a0\u00a0 or Docker desktop for Mac option. Let's look at\u00a0\nthe first option. Docker toolbox. This was the\u00a0\u00a0 original support for Docker on Mac. It is Docker\u00a0\non Linux VM created using VirtualBox on Mac as\u00a0\u00a0 Windows It has nothing to do with Mac applications\u00a0\nor Mac based images or Mac containers. It purely\u00a0\u00a0 runs Linux containers on a Mac OS. Docker toolbox\u00a0\ncontains a set of tools like Oracle VirtualBox,\u00a0\u00a0 Docker engine, Docker machine, Docker compose,\u00a0\nand a user interface called cadmatic. When you\u00a0\u00a0 download and install the Docker toolbox\u00a0\nexecutable, installs VirtualBox deploys\u00a0\u00a0 lightweight VM called boot to Docker, which has\u00a0\nDocker running in it already. This requires mac\u00a0\u00a0 os 10.8 or newer. The second option is the newer\u00a0\noption called Docker desktop for Mac with Docker\u00a0\u00a0 desktop for Mac, we take out Oracle VirtualBox\u00a0\nand use hypercard virtualization technology\u00a0\u00a0 during the installation process for Docker for\u00a0\nMac, it will still automatically create a Linux\u00a0\u00a0 system underneath. But this time it is created on\u00a0\nhypercard instead of Oracle VirtualBox and have\u00a0\u00a0 Docker running on that system. This requires Mac\u00a0\nOS Sierra 10 or 12 or newer, and my and the Mac\u00a0\u00a0 hardware must be 2010 or newer. Finally, remember\u00a0\nthat all of this is to be able to run the Linux\u00a0\u00a0 container on Mac. As of this recording, there are\u00a0\nno Mac based images or containers. Well, that's it\u00a0\u00a0 with Docker on Mac for now. We will now try to\u00a0\nunderstand what container orchestration is. So\u00a0\u00a0 far in this course, we have seen that with Docker,\u00a0\nyou can run a single instance of the application\u00a0\u00a0 with a simple Docker run command. In this case,\u00a0\nto run a Node JS based application, you run the\u00a0\u00a0 Docker run node j s command. But that's just one\u00a0\ninstance of your application on one Docker host,\u00a0\u00a0 what happens when the number of users increase,\u00a0\nand that instance is no longer able to handle the\u00a0\u00a0 load, you deploy additional instance of your\u00a0\napplication by running the Docker run command\u00a0\u00a0 multiple times. So that's something you have to\u00a0\ndo yourself, you have to keep a close watch on\u00a0\u00a0 the load and performance of your application and\u00a0\ndeploy additional instances yourself. And not just\u00a0\u00a0 that, you have to keep a close watch on the health\u00a0\nof these applications. If a container was to fail,\u00a0\u00a0 you should be able to detect that and run the\u00a0\nDocker run command again to deploy another\u00a0\u00a0 instance of that application. What about the\u00a0\nhealth of the Docker host itself? What if the host\u00a0\u00a0 crashes and is inaccessible? The containers hosted\u00a0\non that host become inaccessible to so what do you\u00a0\u00a0 do in order to solve these issues, you will need\u00a0\na dedicated engineer who can sit and monitor the\u00a0\u00a0 state performance and health of the containers and\u00a0\ntake necessary actions to remediate the situation.\u00a0\u00a0 But when you have large applications deployed\u00a0\nwith 10s of 1000s of containers that that's\u00a0\u00a0 not a practical approach. So you can build your\u00a0\nown scripts. And that will help you tackle these\u00a0\u00a0 issues. To some extent. container orchestration\u00a0\nis just a solution for that. It is a solution\u00a0\u00a0 that consists of a set of tools and scripts\u00a0\nthat can help host containers in a production\u00a0\u00a0 environment. Typically, a container orchestration\u00a0\nsolution consists of multiple Docker hosts that\u00a0\u00a0 can host containers. That way, even if one fails,\u00a0\nthe application is still accessible through the\u00a0\u00a0 others. A container orchestration solution\u00a0\neasily allows you to deploy hundreds or 1000s\u00a0\u00a0 of instances of your application with a single\u00a0\ncommand. This is a command used for Docker swarm,\u00a0\u00a0 we will look at the command itself in a bit. Some\u00a0\norchestration solutions can help you automatically\u00a0\u00a0 scale up the number of instances when users\u00a0\nincrease and scale down the number of instances\u00a0\u00a0 when the demand decreases. Some solutions can even\u00a0\nhelp you in automatically adding additional hosts\u00a0\u00a0 to support the user load, and not just clustering\u00a0\nand scaling. The container orchestration solutions\u00a0\u00a0 also provide support for advanced networking\u00a0\nbetween these containers across different hosts,\u00a0\u00a0 as well as load balancing user requests across\u00a0\ndifferent house. They also provide support for\u00a0\u00a0 sharing storage between the house as well as\u00a0\nsupport for configuration management and security\u00a0\u00a0 within the cluster. There are multiple container\u00a0\norchestration solutions available today. Docker\u00a0\u00a0 has Docker swarm Kubernetes from Google and mezzos\u00a0\nfrom patchy while Docker swarm is really easy to\u00a0\u00a0 set up and get started. It lacks some of the\u00a0\nadvanced auto scaling features required for\u00a0\u00a0 complex production grade applications. mezzos on\u00a0\nthe other hand, is quite difficult to set up and\u00a0\u00a0 get started, but supports many advanced features.\u00a0\nKubernetes is arguably the most popular of it all\u00a0\u00a0 is a bit difficult to set up and get started but\u00a0\nprovides a lot of options to customize deployments\u00a0\u00a0 and has support for many different vendors.\u00a0\nKubernetes is now supported on all public cloud\u00a0\u00a0 service providers like GCP, Azure and AWS and\u00a0\nthe Kubernetes project is one of the top ranked\u00a0\u00a0 projects on GitHub and upcoming lectures. We will\u00a0\ntake a quick look at Docker swarm and Kubernetes.\u00a0\u00a0 We will now get a quick introduction to Docker\u00a0\nswarm. Docker swarm has a lot of concepts to\u00a0\u00a0 cover and requires its own course. But we will\u00a0\ntry to take a quick look at some of the basic\u00a0\u00a0 details so you can get a brief idea on what it is.\u00a0\nwith Docker swarm. You could now combine multiple\u00a0\u00a0 Docker machines together into a single cluster.\u00a0\nDocker swarm will take care of distributing your\u00a0\u00a0 services or your application instances into\u00a0\nseparate hosts for high availability For load\u00a0\u00a0 balancing across different systems and hardware\u00a0\nto set up a Docker swarm, you must first have\u00a0\u00a0 hosts or multiple hosts with Docker installed on\u00a0\nthem, then you must designate one host to be the\u00a0\u00a0 manager or the master or the swarm manager as it\u00a0\nis called, and others as slaves or workers. Once\u00a0\u00a0 you're done with that, run the Docker swarm in\u00a0\nit command on the swarm manager, and that will\u00a0\u00a0 initialize the swarm manager. The output will also\u00a0\nprovide the command to be run on the workers to\u00a0\u00a0 copy the command and run it on the worker nodes\u00a0\nto join the manager. After joining the swarm,\u00a0\u00a0 the workers are also referred to as nodes. And\u00a0\nyou're now ready to create services and deploy\u00a0\u00a0 them on the swarm cluster. So let's get into\u00a0\nsome more details. As you already know, to run\u00a0\u00a0 an instance of my web server, I run the Docker run\u00a0\ncommand and specify the name of the image I wish\u00a0\u00a0 to run. This creates a new container instance of\u00a0\nmy application and serves my web server. Now that\u00a0\u00a0 we have learned how to create a swarm cluster, how\u00a0\ndo I utilize my cluster to run multiple instances\u00a0\u00a0 of my web server. Now one way to do this would\u00a0\nbe to run the Docker run command on each worker\u00a0\u00a0 node. But that's not ideal as I might have to log\u00a0\ninto each node and run this command. And there,\u00a0\u00a0 there could be hundreds of nodes that will have\u00a0\nto set up load balancing myself, I like to monitor\u00a0\u00a0 the state of each instance myself. And if\u00a0\ninstances were to fail, I'll have to restart them\u00a0\u00a0 myself. So it's going to be an impossible task.\u00a0\nAnd that is where Docker swarm orchestration comes\u00a0\u00a0 in. Docker swarm orchestrator does all of this for\u00a0\nus. So far, we've only set up the swarm cluster,\u00a0\u00a0 but we haven't seen orchestration in action.\u00a0\nThe key component of swarm orchestration is\u00a0\u00a0 the Docker a service. Docker services are one or\u00a0\nmore instances of a single application or service\u00a0\u00a0 that runs across the saw the nodes in the swarm\u00a0\ncluster for example. In this case, I could create\u00a0\u00a0 a Docker service to run multiple instances of my\u00a0\nweb server application across worker nodes in my\u00a0\u00a0 swarm cluster. For this, I run the Docker service,\u00a0\ncreate command on the manager node and specify my\u00a0\u00a0 image name there, which is my web server in this\u00a0\ncase, and use the option replicas to specify the\u00a0\u00a0 number of instances of my web server I would like\u00a0\nto run across the cluster, since I specified three\u00a0\u00a0 replicas, and I get three instances of my web\u00a0\nserver distributed across the different worker\u00a0\u00a0 nodes. Remember, the Docker service command\u00a0\nmust be run on the manager node and not on the\u00a0\u00a0 worker node. The Docker service create command is\u00a0\nsimilar to the Docker run command in terms of the\u00a0\u00a0 options passed, such as the dash e environment\u00a0\nvariable, the dash p for publishing ports,\u00a0\u00a0 the network option to attach container to\u00a0\na network, etc. Well, that's a high level\u00a0\u00a0 introduction to Docker swarm, there is a lot more\u00a0\nTo know such as configuring multiple managers,\u00a0\u00a0 overlay networks, etc. As I mentioned, it requires\u00a0\nits own separate course. Well, that's it for now.\u00a0\u00a0 In the next lecture, we will look at Kubernetes at\u00a0\na high level. We will now get a brief introduction\u00a0\u00a0 to basic Kubernetes concepts. Again Kubernetes\u00a0\nrequires its own course, well, a few courses,\u00a0\u00a0 at least five, but we will try to get a brief\u00a0\nintroduction to it here. with Docker, you were\u00a0\u00a0 able to run a single instance of an application\u00a0\nusing the Docker COI by running the Docker run\u00a0\u00a0 command, which is great. running an application\u00a0\nhas never been so easy before. with Kubernetes\u00a0\u00a0 using the Kubernetes COI, known as cube control,\u00a0\nyou can run 1000 instances of the same application\u00a0\u00a0 with a single command Kubernetes can scale it up\u00a0\nto 2000 with another command Kubernetes can be\u00a0\u00a0 even configured to do this automatically so that\u00a0\ninstances and the infrastructure itself can scale\u00a0\u00a0 up and down. based on user load Kubernetes can\u00a0\nupgrade these 2000 instances of the application\u00a0\u00a0 in a rolling upgrade fashion, one at a time\u00a0\nwith a single command. If something goes wrong,\u00a0\u00a0 it can help you roll back these images with a\u00a0\nsingle command Kubernetes can help you test new\u00a0\u00a0 features of your application. By only upgrading\u00a0\na percentage of these instances through a B\u00a0\u00a0 testing methods. The Kubernetes open architecture\u00a0\nprovides support for many, many different network\u00a0\u00a0 and storage vendors. Any network or storage brand\u00a0\nthat you can think of has a plugin for Kubernetes\u00a0\u00a0 Kubernetes supports a variety of authentication\u00a0\nand authorization mechanisms. All major cloud\u00a0\u00a0 service providers have native support for\u00a0\nKubernetes. So what's the relation between\u00a0\u00a0 Docker and Kubernetes Kubernetes uses Docker\u00a0\nhost to host applications in the form of Docker\u00a0\u00a0 containers. Well, it need not be Docker all the\u00a0\ntime. Kubernetes supports as natives to Dockers as\u00a0\u00a0 well, such as rocket, or a crier. But let's take\u00a0\na quick look at the Kubernetes architecture. A\u00a0\u00a0 Kubernetes cluster consists of a set of nodes, let\u00a0\nus start with nodes. A node is a machine physical\u00a0\u00a0 or virtual on which a Kubernetes the Kubernetes\u00a0\nsoftware set of tools are installed. And node is a\u00a0\u00a0 worker machine. And that is where containers will\u00a0\nbe launched by Kubernetes. But what if the node\u00a0\u00a0 on which the application is running fails? Well,\u00a0\nobviously, our application goes down. So you need\u00a0\u00a0 to have more than one nodes. A cluster is a set of\u00a0\nnodes grouped together this way, even if one node\u00a0\u00a0 fails, you have your application still accessible\u00a0\nfrom the other nodes. Now we have a cluster but\u00a0\u00a0 who is responsible for managing this cluster?\u00a0\nWhere is the information about the members of\u00a0\u00a0 the cluster stored? How are the nodes monitored?\u00a0\nWhen a node fails? How do you move the workload of\u00a0\u00a0 the failed nodes to another worker node, that's\u00a0\nwhere the master comes in. The Master is a node\u00a0\u00a0 with the Kubernetes control plane components\u00a0\ninstalled. The master watches over the notes are\u00a0\u00a0 in the cluster and is responsible for the actual\u00a0\norchestration of containers on the worker nodes.\u00a0\u00a0 When you install Kubernetes on a system, you're\u00a0\nactually installing the following components,\u00a0\u00a0 an API server and actually server, a cubelet\u00a0\nservice container runtime engine like Docker, and\u00a0\u00a0 a bunch of controllers and the scheduler. The API\u00a0\nserver acts as the front end for Kubernetes. The\u00a0\u00a0 users management devices command line interfaces\u00a0\nall talk to the API server to interact with the\u00a0\u00a0 Kubernetes cluster. Next is the Etsy d a key value\u00a0\nstore. The Etsy D is a distributed reliable key\u00a0\u00a0 value store used by Kubernetes to store all data\u00a0\nused to manage the cluster. Think of it this way,\u00a0\u00a0 when you have multiple nodes and multiple\u00a0\nmasters in your cluster. etsy D stores all\u00a0\u00a0 that information on all the nodes in the cluster\u00a0\nin a distributed manner. NCD is responsible for\u00a0\u00a0 implementing a locks within the cluster to ensure\u00a0\nthere are no conflicts between the masters. The\u00a0\u00a0 scheduler is responsible for distributing work\u00a0\nor containers across multiple nodes. It looks\u00a0\u00a0 for newly created containers and assigns them\u00a0\nto nodes. The controllers are the brain behind\u00a0\u00a0 orchestration, they're responsible for noticing\u00a0\nand responding when nodes containers or endpoints\u00a0\u00a0 goes down. The controllers makes decisions\u00a0\nto bring up new containers. In such cases,\u00a0\u00a0 the container runtime is the underlying software\u00a0\nthat is used to run containers. In our case,\u00a0\u00a0 it happens to be Docker. And finally, cubelet is\u00a0\nthe agent that runs on each node in the cluster.\u00a0\u00a0 The agent is responsible for making sure that the\u00a0\ncontainers are running on the nodes as expected.\u00a0\u00a0 And finally, we also need to learn a little bit\u00a0\nabout one of the command line utilities known\u00a0\u00a0 as the cube command line tool or the cube control\u00a0\ntool or cube cuddle as it is also called the cube\u00a0\u00a0 control tool is the Kubernetes CLA, which is used\u00a0\nto deploy and manage applications on a Kubernetes\u00a0\u00a0 cluster to get cluster related information to get\u00a0\nthe status of the nodes in the cluster, and many\u00a0\u00a0 other things. The Cube control run command is\u00a0\nused to deploy an application on the cluster.\u00a0\u00a0 The Cube control cluster info command is used to\u00a0\nview information about the cluster. And the cube\u00a0\u00a0 control get nodes command is used to list all the\u00a0\nnodes part of the cluster. So to run hundreds of\u00a0\u00a0 instances of your application across hundreds\u00a0\nof nodes. All I need is a single Kubernetes\u00a0\u00a0 command like this. Well, that's all we have for\u00a0\nnow, a quick introduction to Kubernetes and its\u00a0\u00a0 architecture. We currently have three courses on\u00a0\ncode cloud on Kubernetes that will take you from\u00a0\u00a0 the absolute beginner to a certified expert.\u00a0\nSo have a look at it when you get a chance.\u00a0\u00a0 So we're at the end of this beginner's course\u00a0\nto Docker. I hope you had a great learning\u00a0\u00a0 experience. If so please leave a comment\u00a0\nbelow. If you'd like my way of teaching,\u00a0\u00a0 you will love my other courses hosted on my site\u00a0\nat code cloud. We have courses for Docker swarm\u00a0\u00a0 Kubernetes advanced courses on Kubernetes\u00a0\ncertification, as well as openshift. We have\u00a0\u00a0 courses for automation tools like Ansible\u00a0\nChef and Puppet and many more on the way,\u00a0\u00a0 visit code cloud@www.cloud.com. Well, thank you so\u00a0\nmuch for your time, and until next time, goodbye.",
        "videoTranscriptLog":"english:['en', 'en']. "
    },
    {
        "channelId":"UCHIbErciyS3Hs0kjAz-at5Q",
        "channelName":"Technical Suneja",
        "videoId":"mN-85fQkRAs",
        "videoTitle":"How She Became a DevOps Engineer After 3 Year  Zero Coding ! Learn DevOps Skills in 180 Days ",
        "videoPublishYear":2023,
        "videoPublishMonth":3,
        "videoPublishDay":18,
        "videoPublishTime":"13:30:17",
        "videoPublishedOn":"2023-03-18T13:30:17Z",
        "videoPublishedOnInSeconds":1679146217,
        "videoViewCount":778609,
        "videoLikeCount":15312,
        "videoCommentCount":535,
        "videoCategoryId":27,
        "videoDefaultAudioLanguage":"en",
        "videoDuration":"PT30M52S",
        "videoDurationInSeconds":1852,
        "videoContentType":"Video",
        "videoDimension":"2d",
        "videoDefinition":"hd",
        "videoCaption":"true",
        "videoLicensedContent":true,
        "videoProjection":"rectangular",
        "channelCustomUrl":"@technicalsuneja",
        "channelPublishYear":2016,
        "channelPublishMonth":6,
        "channelPublishDay":14,
        "channelPublishTime":"18:47:50",
        "channelPublishedOn":"2016-06-14T18:47:50Z",
        "channelPublishedOnInSeconds":1465930070,
        "channelCountry":"IN",
        "channelViewCount":59257269,
        "channelSubscriberCount":560000,
        "channelVideoCount":978,
        "videoPublishedWeekDay":"Saturday",
        "videoDurationClassification":"Very Long",
        "channelAgeInYears":8.8529253869,
        "channelNormalizedViewCount":0.0693345497,
        "channelNormalizedSubscriberCount":0.0474572235,
        "channelNormalizedVideoCount":0.0554106171,
        "channelNormalizedChannelAge":0.4529288058,
        "channelGrowthScore":13.2441196292,
        "videoAgeInDays":763.538287037,
        "videoViewsPerDay":1019.7379911382,
        "videoLikeToViewRatio":0.01966584,
        "videoCommentToViewRatio":0.0006871228,
        "videoEngagementScore":51047.2713224567,
        "channelGrowthScoreRank":16,
        "videoEngagementScoreRank":3,
        "country_code":"IN",
        "country_name":"India",
        "continent":"Asia",
        "continent_code":"AS",
        "it_hub_country":"Yes",
        "videoTranscript":"If there's an ongoing match on hotstar Virat Kholi is arriving, then all of a sudden viewers will increase. They'll increase too much and there'll be a big spike. Then, how will it handle? To handle that thing in back-end whenever developers write the code, they test it in their system. Alright! Then when that code goes into operation, they say that code isn't working. But the developers had tester, so to solve this thing It is called - Blame game- to solve it,  Docker came into existence. I had done this mistake as well. I wanted a job quickly, then I thought of looking for tools required for the job so, I jumped to the tools. I forgot the steps before, so I left it. I spent 1 year, 2 years, as soon as it reaches the client, it crashes. So, time was wasted, your money was also wasted. So, to solve these things, actually DevOps arrived. DevOps is something that fills the gap between the developers and operations. It helps them to talk with each other helps them to communicate. Tell us, does the company hire freshers for this role? If yes, then how can a fresher take the opportunity in DevOps profile? What is your experience in it? Welcome to a new podcast. I've brought a DevOps engineer. The story of today's guest will inspire you a lot. How? She completed her B.Tech in Electronics and she got a job in it. She started with Rs. 9000. She worked with that job profile for 3 years. You can say, it was a kind of support job. And within 3 years she reached from 9k - 22k. After that, she got fed up, she felt like that's enough & she doesn't want to do this job. And then her interest was build into DevOps. She researched a lot that you'll get to know in this podcast. And DevOps profile is very fascinating in itself. Its in demand. So, what does a DevOps engineer do? What is CICD? What is Kubernetes? What is Docker? We'll talk about all these things with her. Do we need DSA or not? Her interview experience, resources, we've discussed a lot. You'll get to know about a lot of things about DevOps profile in this podcast. So, like this video, share and comment Like Aim is kept to be of 8k. Let's do it for more such interesting podcasts. Let's talk with Megha & know her entire journey. Why did she feel after 3 years that I should go into DevOps? Let's talk with her. So, Megha welcome to our Youtube channel. And as you're a DevOps engineer & our audience I was getting a lot of comments that Brother, please talk on DevOps engineer. Share a roadmap of DevOps. And tell us how fascinating is this profile. So, today we'll get to know from your experience about DevOps. I won't take much time. First start with your introduction. Where do you belong to? When did you finish your graduation? Tell us about yourself. Okay! So, Hi Ajay & first of all thank you. For inviting me & I'm very happy to be part of it. And talking about my background or my journey. then my name is Megha & I belong to Bengaluru. So, I have done ECE- Electronics and Computer engineering. And in this domain, I've worked for almost 3 years. So, after completing engineering & working in this domain for 3 years I faced a situation when I wasn't getting much of technical or career growth. So, then I thought of exploring the career options. Then I started to search in the IT world -IT domain that what should I do? What are the profiles there? I'll be frank I had never heard of the words -DevOps- in my life before. Means, for the first time I got to know it when I searched on Google if there's actually something named DevOps. That even offers you a job. And it was blooming at that time & its' like numerous vacancies were created. So, I thought - why not- let's try. And I started to learn. I learned DevOps concept then I found it very interesting. I thought of making my career in this domain and I went on learning. And finally I got the job. So, the first company I worked in was Infosys & right now, at present I'm working as a senior software engineer at CGI. So, this is kind of my journey by which I actually landed in DevOps. And it was initially tough. Because I was coming from a NON-IT background, its not easy. But I started from zero. But, its like starting was tough but anyhow I managed to do it and still continue to working in it. So, Megha as you told us you had Electronics domain, then was your first job in Infosys or were you working before anywhere else before that? So, Infosys was my first job in DevOps. But before that, actually I was working in Electronics domain. So, the company which manufactures elecronics, right! Small-scale industries I worked there. So, I worked there as a testing engineer then I used to even handle store. So, the LED products that you see at your homes, right! LED lights that you use - which watt should that be of what should be its current - we used to manufacture all of it. I used to test. And if you want to manufacture any product then the electronic components required for example, resistor or capacitor, PCB, all of it. I had to order all of this in knots. So, I used to maintain the store. If any of the component is missing then the entire production line has to stop. So, it was kind of like - a pressure was created in some situations. But, anyhow I used to manage it. So, I've actually worked with these things in the electronics domain. How much time did you work in this entire job specially that you're telling about? Electronics..... 3 years okay! I worked for 3 years almost. Yeah! So, you worked 3 years, then was the salary fine there. How much did you get in the starting? So, talking about salary it was like 9k. I still remember means the cheque that we got for first salary, it was like 9 k. But there wasn't much growth So, that's why I thought of switching. So, lot of audience is watching you therefore as an example they're also able to relate, so as soon as you completed 3 years where did you reach from 9k? So, at the most I reached 22 k only. Not more than that. Okay! Within a span of 3 years. So, it was difficult    *okay!*  But.... No, its fine. But I think you're good at your current place. Sometimes I feel that, to gain something in life, destiny leads you experience it and then as you think accordingly You put your hardwork into the work & accordingly you get the things. Okay! So, what was the work there means was it a support type work? Yes! that was kind of support. I had to more testing, I had to see the specification of product before dispatch. And I had to maintain stores much. If all the components are according to the build-of materials or not? I had to maintain those things. Then at a point you were fed up that enough, I'm done. Yes!! I was really fed up. At a point I even thought that its enough, I can't do it anymore. Now I've to look for other options. So, tell me one thing, as you know there are multiple fields in IT. There's software development, web development is another different domain. There's Software testing. What did you think about DevOps after 3 years? You even left the job & took such a high risk. Its alright, if the salary was slow but didn't your family say anything that why are you leaving the job, Megha? I think you shouldn't leave it. Tell us how did you handle that, how did you prepare your mind that Alright! I'm ready to take the risk. I'll also prepare myself in DevOps. And why did you choose DevOps? Okay! One fine day, I was very frustrated I felt that if I look for 5 years or 10 years later If I'll be still working here or will I do anything else or will I be able to learn anything? So, then I did talk with my families that I want to explore or I want to do anything for myself. Then, I thought of searching for IT domain. So, family also supported me. They told that if you want to explore something if you want to look for career, then let's try that. But, if there's a situation or not that If I'm leaving a job then I was afraid a little. A lot not a little. That I'm leaving a job, I won't have salary, I'll sit still then, its fine. Anyhow whatever work I was doing, I didn't had any satisfaction in it. Let's take the risk, leave the job & explore. I'll be able to do these things for sure. I had confidence that I'll do something or the else. So, when I started searching, let me tell you one thing that I only knew about C and C++ language in IT domain. Because that was taught in the college. A[art from that i didn't knew anything. I knew that people code I heard it from here to there, people used to code & I should do coding but then I thought that before doing let me search then I got to know about the world called DevOps. When I went on diving deep, I went on reading, I watched a lot of Youtube channels in which they told about DevOps, so I found it interesting. So I thought of learning it. And as I said its jobs were being created a lot. So, I thought of doing it for sure. So, I found it a little interesting & I started my journey in DevOps. Perfect! Megha! Now, tell me as a lot of viewers are watching, they don't know what a DevOps engineer is what does it do So, tell us what you do now. What are the roles and responsibilities of a DevOps Engineer? What does this engineer do? So, if you talk about DevOps & think about a day-to-day activity of a DevOps person. DevOps is such a profile where people will come at you, developers will come along with issues that my code isn't working, my code hasn't been pushed we're not able to see the webpage. We're not able to see our changes if they're present there or not. So, they'll come up to you. So, you're like a savior, everyone will come to you, I've this issue and that, so DevOps is like, he should be an all rounder. So, you'll be the person that if you know about a simple software development life cycle then, developer will push the code, right! Any of the Github or wherever you store a code it'll push there. So, you'll be a DevOps person or in a DevOps role that'll be actually building up that code, testing it you'll be using CI\/CD for that. Continuous Integration, you'll do Deployment. And after deploying the code in production, your work doesn't finish there you've to even maintain because whoever our client is, will be viewing the dashboard. You'll have to keep on watching the data, so you've to maintain it. To maintain that, you need to know monitoring So, there we start using these concepts in different tools. DevOps is like an all rounder kind of thing. So, you should know everything. I'm not saying that you should know all the tools No, but you should know the basic tools. That is very important. So, you talked about tools right now. First of all, things that you're telling are very insightful, we can see clarity in those things. So, now I would like to know the roadmap to become a DevOps engineer. Now you talked about tools. Let's understand the roadmap with tools itself if a fresher is watching you now, right if they've a years' experience or 2 years or 3 years or if they're in a different domain in NON-IT, like you Then, what will a clean roadmap & which are the necessary tools to be known as you told that there are many tools now its not possible to do everything, Megha, right! But, even then I'll do this one so that I'll be able to give the interview in future. Please! Okay! So, before moving onto tools, I would like to add one more thing that even I did this mistake. I wanted a job quickly then, I told of studying the tools required for the job. So, I jumped to the tools and forgot the steps before it I left it. So, I want to say that its important to study tools. I agree but before that there are few steps. Like, you should know what DevOps is - What is DevOps? And why did DevOps arrive? IT industry was working before that as well, there wasn't any problem in it but what problem is DevOps solving due to which the entire IT world is trying to move into DevOps domain. What is there? if you'll understand that, then you'll be able to understand the concept easily. So, according to you why does DevOps profile exist? What are the DevOps engineer solving in the software industry? So, according to me if we make an application to do something & give it to the client whatever our product is so, manual thing cannot be done in everything. We need to bring automation. So, how will automation come? To automate anything, we'll write either YAML files, configuration files scripting is included as well and its not that we've to leave after writing scripting. We need to deploy it finally as well. During the time of deploy, the CI\/CD, heart of DevOps that we talk about is included as well. So, if we talk about when DevOps wasn't there even then there used to be a waterfall model. Waterfall model means no feedback from the client, Developer left the code after coding it a year or two went by, it reached the client but the code crashed, so the time has been wasted your money had been wasted as well. So, to solve all these things DevOps actually came. DevOps is something that fills the gap between the team of developer and Operations. It helps to make them talk with each other & communicate. So, this is actually DevOps. Perfect! Megha! I purposely put the spoiler that tell us now, according to you what do you feel, you told us very perfectly. But, its fine. As a viewer I've understood what a DevOps profile is what does a DevOps engineer do. But again, coming back to the roadmap and tools. I've understood what a DevOps profile is. Now, what are the tools that I should know. I want to know its roadmap from you. Okay! So, firstly talking about tools, then there's Docker or Kubernetes but before that GIT GIT is such a thing that you'll use everyday in DevOps profile. GIT commands like, adding a GIT Pushing a GIT you'll be using GIT command everyday because if you want to perform any testing you won't do it at production. You'll clone the code in your local and perform testing there and then push it. So, you should actually know GIT in it. I say that you should have a hold of good level of GIT. So, GIT is such a thing that you need to learn. Then talking about tools then, comes DOCKER. Its like a container. Container tool because we containarise every application. We'll write Docker file, image it and containarise it. And then deploy it in any environment. I want to give an example that Whenever developers write the code, they test it in their system where it runs smoothly Alright! When that code reaches operation, they say that this code isn't working. But the developers had tested, so to solve this thing, like we say - BLAMEGAME Docker came into existence. So, its important for you to know DOCKER. Docker will help you in DevOps domain to find a job. And then talking about another tool, then CI\/CD. You can start with any of Jenkins tool, like Jenkins and Azure pipelines is there. There's circle CI. So you can sign in and learn yourself. You need just username and email ID. So, this thing is necessary. And second, very important tool I would say is Kubernetes. Kubernetes is a container orchestration tool take an example, if an IPL match is ongoing on Hotstar and if Virat Kholi is arriving, then all of a sudden viewers will increase. They'll increase rapidly & there'll be a big spike, so how will you handle it? To handle that thing in back-end, Kubernetes will look if the server is down or not if users have increased, traffic has increased along with it then it'll balance, stain it, maintain it. So, Kubernetes will help you in these things. You've Docker but to orchestrate it, you need Kubernetes. So, learning this tool is very important for you. And as I said CI\/CD tool is also important. You can start like this actually. Perfect! As I've understood from you, if I put more clarity in it then if we go for a traditional approach where we used to deploy manually now, as she gave the example of docker. With its help, to create an image of our project and whatever environment we have, staging, UAD we have or production, we'll deploy the image there. Right! Megha! Yes! And as far as CI\/CD is concerned, as she talked about tools like GITLAP, you create a pipeline internally & that pipeline itself contains tasks. And we call those tasks as jobs. There'll be a job which will be deploying on Dev, another job will be deploying on staging. so that pipeline is written in.....like for example YAML file right! Any other example YAML file I think we can also write in Python as well. So, we derive the pipeline from there and then deploy things. And I think all this work belongs to the DevOps Engineer. Yes! Okay! Perfect! By the way do you have any resources from where you've learnt share whatever you've, I'll put it in the description. Sure! Now tell me, Megha, is programming required to become a DevOps engineer? Should I know a programming language before learning these tools? So, according to my experience, I can say that Programming language, if you know it, then its an add-on advantage for you. And let me tell you why? So, when I was working on a project, then we used Azure as a cloud provider I was told that, Megha, you've to create a pipeline- Azure pipeline. And developer had written the code & then I've to take it, build it & test it. And I had to bring an output. According to me, I configured YAML file & configuring the entire pipeline I ran it. After running it, my pipeline got failed. And I tried a lot. So, I was confused that is my pipeline failing due to my configuration or if code isn't linked. So, I couldn't understand it. So, I spent almost half a day trying. I went to the developer they told me that I've done a few changes in the code & he completed those mistakes values were missing. So, when I corrected it my pipeline was working fine and I got the expected output. Then I understood the importance of programming language. So, if you understand what code a developer has written then it'll help you to solve the issues. It'll fastrack, I'm not saying that learn the entire programming language. Very finely. No, keep the basic understanding & you should be able to understand what does a developer wants to write. That thing according to me is important. You should know the basic understanding for programming. Okay! And do you think DSA is important, like Data structures and algorithms Should we have abundant knowledge of Algorithm for this profile? I know everyone has different preferences but the 3.5 years that you've spent in DevOps you've worked in Infosys, now you're working in CGI. So, did you feel that my data structure & Algorithm should be that strong? Or not? So, until now as I've worked in DevOps profile, then there I didn't feel its requirement much or being asked about it in interviews that this is a requirement in our profile. So, I don't think there's much of its requirement. Okay! You worked 2 years in Infosys & you've spent 1.5 years in CGI you might've learnt some tools in Infosys within 2 years you might've got some trainings. Now you're working in CGI so, which of the tools do you frequently use in your daily day-to-day life? So, if I talk about daily then Linux, which is such a tool that I use a lot because I need to take the code in local and I need to do some changes, so Linux commands like, grep or NS lookup, LS make directory are used a lot. So, knowing Linux is very important. Then if I talk about GIT, I use it a lot & Docker because we need to create a docker file, then we need to create an image out of it. Then, after containarizing we have to deploy it. So, I use that lot. Our project is using Kubernetes So, I work on Kubernetes a lot. We create lot of pots and deploy it. So, Kubernetes run hand-in-hand. Both of them work together. Therefore, they're used a lot & talking further about infra-structurous code or terraform or every cloud provider has their own infra-structurous code. Then, you write step wise instructions in declarative & you can use it. Then I use that as well. And these are the main things that I mostly use and CI\/CD too. So, Azure pipelines. That I use a lot. So, these are the things that I use a lot. Perfect! So, I had a small question. Suppose there's a project in which development is done. Now, I've to deploy it in server. I wrote a docker file so that an image of that project is created the kubernetes that you're saying that you containorize, right! how do you do that thing ?Do you write a different file for it? No! So, as you write  a docker file and made an image and we made a package & container of that image and deployed it. So, when we deploy it after packaging, currently you're working in a single project You'll have one container which will be easy to manage but when you'll have 100s & 200s of containers, we create it in a pod. So, we writw a file for that - configuration file - it'll have pod. We deploy the pod. That we'll deploy in server. Cloud provide desk will be there. It depends on the one you're using. Is it Azure or GCP or  *AWS* whatever it is. So, you'll deploy accordingly. So there kubernetes will play its role. Perfect! We tried to understand a real life example from Megha. Megha, tell us if the companies hire freshers for this role? If yes, then how can a fresher take the opportunity in DevOps profile. What is your experience in it? So, if I talk about freshers then I had the same experience that since I was a fresher as well. I had such an experience that I'm not even a fresher. I worked 3 years in other domain and I was going somewhere else. So, according to me they don't give much opportunities to freshers if we compare with experienced then its less. but it doesn't mean that nobody gives jobs to freshers              if they'll not give jobs then no one will be able to work in DevOps             *Truly* no one will become an experienced person. They've to start from somewhere, so if you're a fresher then you won't get jobs directly in Devops role. You'll go as a junior engineer. But you'll work on DevOps tool. There'll be a GIT or Jenkins Space. Initially you'll work like that. But you won't enter into exact DevOps role. But after 2 years, maximum 2 years or 3 years you'll be called fully into DevOps role like you can even get an exact title. So, don't think that you didn't get DevOps role but the DevOps tool that you're working on knowledge is actually very important. So, we'll consider but there are less opportunities. But whatever tool you're getting to work on then I prefer you to start because I started like that. I got the chance so I just grabbed it & started working. So, rest, you're also active on Linkedln. Your profile looks very promising which justifies that you're a DevOps engineer. So, you can pitch the HR or hiring manager & take the opportunity from there as well. Yeah! True! Yes! So, according to you you're working in a good salary you're happy, I feel that its important to be happy along with the job. And the 3 years that you spent. I think you're happy from the work the one that's going on in life. So, what is the salary growth? If we talk about it, how do you look at the salary in DevOps? Megha! Okay! So, talking about salary growth, then as I told that I worked on electronics domain then my salary wasn't that good. But, if you come into DevOps domain, initially you may feel that its less but once you enter into DevOps domain, then your salary will always be in a growing model. it'll never be in a decreasing model. I won't be able to say it in number or quanntity but for sure it'll be a growing type of model. Along with the salary growth, your knowledge will equally increase. So, according to my experience, I feel that there's a good amount of salary growth in this domain actually, a lot. Rest, I would say it all depends on how much experience you've, when you're switching, how many counters are coming, how are you negotiating the salary on those counters and those things then depend individual to individual. Yes! I agree with that. So, Megha it was nice talking to you. We got lot of insights. According to your experience, the tools that you're working on, your journey that has been until now as you go, freshers are watching you, those who've 1 -2-3 years of experience, people who are in NON-IT are watching you what will be Megha's advice for them? If they want to come into DevOps? So, I want to say that any person, be it a fresher or who has already worked but want to come into DevOps then according to me, I want to say that try watching youtube channels, like Ajay you also have a youtube channel which has a lot of information on it. There's information but I think for specific DevOps, if you want  to learn technically then I don't have an expertise in that, my role is on front-end we'll take those resources from Megha. And put it in the description, even she has her own youtube channel. Where she'll make videos on DevOps, I believe after this video she'll make a lot of content in the coming time. So, you can refer that as well. Rest all the resources I'll get from her I'll put that in the description, be it documentation or a video. Yes, Megha. Please, sorry! And secondly, you can read articles because I did a lot of self-study and articles like, Dev.to.com there are a lot of DevOps engineers who write articles, they even write tutorials you can simply follow it. It'll be step wise & from your side try signing up on GITHUB. Simply run a 'Hello World' Application. Or if you need to print 'Hello' Application, do it through CI\/CD pipeline. I can give you can example that sign in Azure, that's very easy. Their documentation will contain get started. Just keep on reading it out & follow. There are lot of tools, try running them by yourself. So, that will give you a hands-on experience because only then an interviewer or any company will.....it'll be written in your resume which will be an add-on Okay, this person is having a knowledge and he's also a hands-on as well. So, you'll stand out among other freshers. So, do try this thing. It'll help. Hands-on is very important, I completely agree. Without hands-on, writing a YAML file or docker file, how will you write it? It won't run on theory. So, there are many youtube tutorials that you can follow. Rest, the resources that Megha will recommend me I'll put that in the description. Thank you so much Megha. It was very nice talking with you. It was very insightful session I would day. Thank you so much again. And guys if you liked this podcast, then do like it. Share it. Comment down. And the like aim that I've kept is 8000. Accomplish it for more such interesting podcasts. I'll meet you soon. Next in any other interesting podcast, till then Take care.",
        "videoTranscriptLog":"english:['en']. "
    },
    {
        "channelId":"UCMndKhkWZp9EOWPrRI4V6bA",
        "channelName":"Software Engineering With Scott Moore",
        "videoId":"PaO7Cb3v6XE",
        "videoTitle":"Salesforce Test Automation For DevOps",
        "videoPublishYear":2024,
        "videoPublishMonth":10,
        "videoPublishDay":31,
        "videoPublishTime":"11:01:56",
        "videoPublishedOn":"2024-10-31T11:01:56Z",
        "videoPublishedOnInSeconds":1730372516,
        "videoViewCount":150004,
        "videoLikeCount":494,
        "videoCommentCount":1,
        "videoCategoryId":28,
        "videoDefaultAudioLanguage":"en",
        "videoDuration":"PT2M44S",
        "videoDurationInSeconds":164,
        "videoContentType":"Video",
        "videoDimension":"2d",
        "videoDefinition":"hd",
        "videoCaption":"true",
        "videoLicensedContent":false,
        "videoProjection":"rectangular",
        "channelCustomUrl":"@scottmooreconsultingllc",
        "channelPublishYear":2015,
        "channelPublishMonth":2,
        "channelPublishDay":12,
        "channelPublishTime":"19:39:33",
        "channelPublishedOn":"2015-02-12T19:39:33Z",
        "channelPublishedOnInSeconds":1423769973,
        "channelCountry":"US",
        "channelViewCount":2559993,
        "channelSubscriberCount":285000,
        "channelVideoCount":552,
        "videoPublishedWeekDay":"Thursday",
        "videoDurationClassification":"Medium",
        "channelAgeInYears":10.1898132927,
        "channelNormalizedViewCount":0.0029947807,
        "channelNormalizedSubscriberCount":0.0241521289,
        "channelNormalizedVideoCount":0.03125,
        "channelNormalizedChannelAge":0.5232774584,
        "channelGrowthScore":2.8652104403,
        "videoAgeInDays":170.6413078704,
        "videoViewsPerDay":879.0602990156,
        "videoLikeToViewRatio":0.0032932455,
        "videoCommentToViewRatio":0.0000066665,
        "videoEngagementScore":43962.9080202977,
        "channelGrowthScoreRank":58,
        "videoEngagementScoreRank":4,
        "country_code":"US",
        "country_name":"United States",
        "continent":"North America",
        "continent_code":"NA",
        "it_hub_country":"Yes",
        "videoTranscript":"[Music] Hey everybody, Scott Moore on the road\nin Orange Beach Alabama at the warf and today I want to talk to those of you who are\nimplementing and testing Salesforce in your organization. Are all those updates and customizations\nbogging down your test process? Are you frustrated about it? You're not alone,\nbut there is a better way to get a high-quality Salesforce experience for everybody.\nIt's called Tricentis Testim Salesforce. This is an AI-powered test solution that's\ndesigned specifically around Salesforce and it's made to help teams of all skill levels. So whether you're a developer, a functional test\nautomation engineer, a business analyst, or an admin doing testing, it can help you. Here's just a few features that Tricentis Testim\nSalesforce offers that can help your organization revolutionize its testing process. Check it out: Create tests quickly and easily without needing\nto write code thanks to the intuitive recorder-based interface. Save time and effort with a library of pre-built\ntest components for common Salesforce operations like record creation, lead conversion,\nrelated list and more, including more complex tasks like\npicklist validation and permissions testing. AI-powered locators use metadata to easily identify\ndynamic elements used by Salesforce Lightning, ensuring your tests remain stable\neven with UI changes. Thoroughly test your automated business processes\nwith the codeless Salesforce flow testing feature. Configure checks at each step without\nwriting a single line of code. Testim Salesforce is tested against Salesforce\npre-releases, giving you peace of mind knowing your tests will remain stable\neven with the latest Salesforce updates. Now testing can be done faster, easier,\nand more efficiently than ever before. Don't let Salesforce test automation drag you down.\nScan the QR code or go to the URL on your screen and sign up for a free trial today and find out\nhow Tricentis Testim Salesforce can empower your organization to deliver high-quality\nSalesforce releases with confidence. And tell them I sent you! I'm Scott Moore\nand I'll see you on the road. [Music]",
        "videoTranscriptLog":"english:['en', 'en']. "
    },
    {
        "channelId":"UCdngmbVKX1Tgre699-XLlUA",
        "channelName":"TechWorld with Nana",
        "videoId":"l-kE11fhfaQ",
        "videoTitle":"ChatGPT Tutorial - Use ChatGPT for DevOps tasks to 10x Your Productivity",
        "videoPublishYear":2023,
        "videoPublishMonth":1,
        "videoPublishDay":18,
        "videoPublishTime":"14:58:00",
        "videoPublishedOn":"2023-01-18T14:58:00Z",
        "videoPublishedOnInSeconds":1674053880,
        "videoViewCount":664495,
        "videoLikeCount":9306,
        "videoCommentCount":321,
        "videoCategoryId":27,
        "videoDefaultAudioLanguage":"en",
        "videoDuration":"PT1H7M43S",
        "videoDurationInSeconds":4063,
        "videoContentType":"Video",
        "videoDimension":"2d",
        "videoDefinition":"hd",
        "videoCaption":"true",
        "videoLicensedContent":true,
        "videoProjection":"rectangular",
        "channelCustomUrl":"@techworldwithnana",
        "channelPublishYear":2019,
        "channelPublishMonth":10,
        "channelPublishDay":6,
        "channelPublishTime":"08:50:17",
        "channelPublishedOn":"2019-10-06T08:50:17Z",
        "channelPublishedOnInSeconds":1570351817,
        "channelCountry":"AT",
        "channelViewCount":67538390,
        "channelSubscriberCount":1250000,
        "channelVideoCount":129,
        "videoPublishedWeekDay":"Wednesday",
        "videoDurationClassification":"Extended",
        "channelAgeInYears":5.5417335109,
        "channelNormalizedViewCount":0.0790240391,
        "channelNormalizedSubscriberCount":0.1059318245,
        "channelNormalizedVideoCount":0.0072595281,
        "channelNormalizedChannelAge":0.278689899,
        "channelGrowthScore":26.1018471677,
        "videoAgeInDays":822.4773726852,
        "videoViewsPerDay":807.9188817349,
        "videoLikeToViewRatio":0.01400462,
        "videoCommentToViewRatio":0.0004830736,
        "videoEngagementScore":40438.9240941194,
        "channelGrowthScoreRank":11,
        "videoEngagementScoreRank":5,
        "country_code":"AT",
        "country_name":"Austria",
        "continent":"Europe",
        "continent_code":"EU",
        "it_hub_country":"No",
        "videoTranscript":"Hi guys! I'm sure you have all heard of ChatGPT\u00a0\nby now. It has become a buzzword within days of\u00a0\u00a0 its release and professionals in all fields,\u00a0\nespecially in high skilled areas like lawyers,\u00a0\u00a0 doctors, engineers are questioning whether\u00a0\nsuch AI can actually replace them and work.\u00a0\u00a0 So in this video I want to talk about what\u00a0\nChat GPT is and how it even popped up,\u00a0\u00a0 talk a bit about the organization behind GPT\u00a0\ncalled \"OpenAI\", which has already created\u00a0\u00a0 many other machine learning models besides Chat\u00a0\nGPT and also explain technically about all that.\u00a0\u00a0 And then we'll dive in and actually put chat\u00a0\nGPT to use for some DevOps related tasks. I\u00a0\u00a0 really want to see how it can help in generating\u00a0\nconfiguration code for building DevOps processes\u00a0\u00a0 or different parts of those processes and how\u00a0\nwell it knows different DevOps technologies,\u00a0\u00a0 but not just some shallow examples or boilerplate\u00a0\ncode that I can get from official documentation,\u00a0\u00a0 but instead also try more fine-tuning and\u00a0\nsmall optimizations in that configuration\u00a0\u00a0 code. So we will see all of that. We're also\u00a0\ngoing to check out an open source command line\u00a0\u00a0 tool that is built on top of chatgpt and was\u00a0\nspecifically created for engineers to generate\u00a0\u00a0 infrastructure as code templates and more and\u00a0\nfinally we'll talk about the impact of ChatGPT,\u00a0\u00a0 the quality and usefulness of such a tool for\u00a0\nengineers and whether it will really replace\u00a0\u00a0 the engineers and to what extent you should\u00a0\nbe concerned. So let's talk about all of that!\u00a0\u00a0 First of all, what is chat GPT and why is it\u00a0\nuseful? You can think of chat GPT as something\u00a0\u00a0 that has all the knowledge from various different\u00a0\nfields including engineering and has a chat\u00a0\u00a0 interface, where you can ask it to give you some\u00a0\ninformation based on that knowledge in an easily\u00a0\u00a0 digestible form. And it will analyze and process\u00a0\nall that knowledge it has, to give you answers in\u00a0\u00a0 a very human-like manner, as if an actual human\u00a0\nprofessional was writing back. Now how did this\u00a0\u00a0 technology even came about? Who created this? Chat\u00a0\nGPT was created by a research organization called\u00a0\u00a0 \"OpenAI\" and according to company itself and its\u00a0\nmission statement, it is actually dedicated to\u00a0\u00a0 developing and using artificial intelligence in\u00a0\na way that can benefit the general public and\u00a0\u00a0 basically democratize the access to artificial\u00a0\nintelligence as well. So the story is that\u00a0\u00a0 its founders among which were Elon Musk and Sam\u00a0\nAltman founded open AI, because they feared that\u00a0\u00a0 people and organizations would misuse artificial\u00a0\nintelligence or be careless about the development\u00a0\u00a0 and advancements in AI and they feared or still\u00a0\ndo probably that this will cause a chaos and\u00a0\u00a0 disaster for the world and through open AI they\u00a0\nwanted to develop and use AI for the benefit of\u00a0\u00a0 general public. So that was the idea behind open\u00a0\nAI organization, which is non-profit, research\u00a0\u00a0 organization. And open AI actually has done really\u00a0\nimpressive developments in the AI field. It is one\u00a0\u00a0 of the leading organizations in the AI technology\u00a0\narea and it already has many very ambitious\u00a0\u00a0 projects, including the development of AI for use\u00a0\nin natural language processing, computer vision,\u00a0\u00a0 robotics, gaming and so on. And there are some\u00a0\npopular high profile projects that open AI has\u00a0\u00a0 developed over the past years, one of them is\u00a0\n\"DALL-E\" for example, which is a neural network,\u00a0\u00a0 which basically is one specific type of machine\u00a0\nlearning model that mimics a human brain,\u00a0\u00a0 so that's what a neural network is. And DALL-E\u00a0\nalso became pretty famous, because it is really\u00a0\u00a0 impressive and it can create super realistic and\u00a0\nvery high quality images and art from a simple\u00a0\u00a0 text description. And another of the projects\u00a0\na language generation model that can generate\u00a0\u00a0 human-like text called generative pre-trained\u00a0\nTransformer or shortly \"GPT\", which is exactly\u00a0\u00a0 the model that famous chat GPT is based on. So\u00a0\nthat's basically one of several other projects\u00a0\u00a0 that open AI has created and they actually made\u00a0\nseveral improvements of GPT after its initial\u00a0\u00a0 development with GPT-2 and extension of the GPT\u00a0\nmodel that was trained on an even larger data set\u00a0\u00a0 of web pages and GPT-2 has the ability to\u00a0\ngenerate a wider range of texts including news\u00a0\u00a0 articles whole stories and poems and then even\u00a0\nmore powerful version was developed which was\u00a0\u00a0 GPT 3 which can even produce jokes puns and has\u00a0\neven wider range of usages including for language\u00a0\u00a0 translation question answering text summarization\u00a0\nand content creation so this powerful GPT 3 Model\u00a0\u00a0 was given a human friendly user interface which\u00a0\nis a chat and that's how we got GPT which open AI\u00a0\u00a0 released and made available for general public\u00a0\nand it has seen tremendous explosion in number\u00a0\u00a0 of users within just days of its release now\u00a0\nyou probably have already seen some videos on\u00a0\u00a0 YouTube of software developers demoing some use\u00a0\ncases of how ChatGPT produces really good code\u00a0\u00a0 in any programming language or framework\u00a0\nor how it even fixes bugs when provided a\u00a0\u00a0 code snippet with certain bug and now along with\u00a0\nmillions of users who have already tried chat GPT\u00a0\u00a0 we want to put it to test and see how it performs\u00a0\nbut in our case specifically for devops tasks so\u00a0\u00a0 the question is can we get really useful output\u00a0\nfrom it and get scripts and configuration code\u00a0\u00a0 for different devops tasks and basically how\u00a0\nqualitative will be the code or scripts for\u00a0\u00a0 devops tasks that chat GPT is going to produce\u00a0\nand that's exactly what we're gonna see in the\u00a0\u00a0 next section and with this let's Dive Right In\u00a0\nthe demo part and try out chat GPT for devops so let's start by opening the chat GPT site and as\u00a0\nyou see it's on openai.com so let's click inside\u00a0\u00a0 try checked GPT and this is basically the URL\u00a0\nwhere it's available it's chat.openai.com and\u00a0\u00a0 if you don't have an account yet so for the\u00a0\nfirst time you have to basically sign up and create your account and\u00a0\nconfirm it from your email address\u00a0\u00a0 and once you've signed up\u00a0\nthen you can just go to login and you'll be forwarded to the chat GPT dashboard\u00a0\nand that's basically how it looks like they may\u00a0\u00a0 change the UI a little bit in the future but\u00a0\nit's a simple user interface and that's also\u00a0\u00a0 one of the reasons for its popularity because\u00a0\nit's super simple and user friendly to use so\u00a0\u00a0 this is basically the starting prompt where you\u00a0\ncan start a conversation and ask you questions\u00a0\u00a0 to ChatGPT and that's what we're gonna start\u00a0\nfrom so I'm gonna do this from a perspective\u00a0\u00a0 of a let's say a junior devops engineer who has a\u00a0\nvague idea of what they're doing but once you use\u00a0\u00a0 this tool to get the work done most efficiently so\u00a0\nbasically use it for research and learning purpose\u00a0\u00a0 as well as for actually getting some proper output\u00a0\nthat they can use at work to implement some devops\u00a0\u00a0 tasks and let's see if ChatGPT can help here and\u00a0\nhere you have a couple of additional information\u00a0\u00a0 about chat GPT like examples of how to interact\u00a0\nwith it how to structure your questions as well\u00a0\u00a0 as the capabilities one of the most important\u00a0\ncapabilities that I think is most valuable that\u00a0\u00a0 chat GPT has is actually staying in the context\u00a0\nof the conversation so once you ask questions\u00a0\u00a0 you can actually do follow-up questions so\u00a0\nit remembers the previous context within\u00a0\u00a0 that chat which is super valuable because you\u00a0\ndon't have to explain everything from scratch\u00a0\u00a0 with every question and they also point out some\u00a0\nof the limitations one of the limitations that\u00a0\u00a0 you have to consider for sure is that the data\u00a0\nthat has been fed to chat GPT is right now at\u00a0\u00a0 this point till 2021 which means everything that\u00a0\nhappened afterwards chat GPT doesn't know about so as a first question let's say I have\u00a0\na JavaScript application with node.js\u00a0\u00a0 framework and I already have the code for the\u00a0\napplication and I want to dockerize it so I\u00a0\u00a0 want to create a Docker file for my node.js\u00a0\napplication and I'm going to ask chatgpt to\u00a0\u00a0 actually give me a Docker file an example Docker\u00a0\nfile that I can use for my node.js project so\u00a0\u00a0 simple instruction write a Docker file for\u00a0\nnode.js application let's see what happens so let's actually see what chat GPT gave us here\u00a0\nfirst of all it gave us an example Dockerfile for\u00a0\u00a0 node.js application so it based it on node image\u00a0\nwith a specific version which is actually very\u00a0\u00a0 good practice to have a fixed version of a base\u00a0\nimage but it didn't just write as a Dockerfile,\u00a0\u00a0 but it actually gave us explanation of each step\u00a0\nin the docker file so again from a perspective\u00a0\u00a0 of a junior engineer that is kind of researching\u00a0\nand doing the job at the same time this is super\u00a0\u00a0 helpful because not only do you just get a ready\u00a0\nDockerfile but it also explains to you what is\u00a0\u00a0 happening on each lens so you can use it to learn\u00a0\nin case you don't understand parts of the syntax\u00a0\u00a0 so it describes all the steps here like setting\u00a0\nthe working directory copying the file installing\u00a0\u00a0 dependencies and so on and it doesn't even stop\u00a0\nthere yet actually gives you some additional\u00a0\u00a0 useful information like making sure that you have\u00a0\npackage.json in your application and that that\u00a0\u00a0 package.json has a script called npm start inside\u00a0\nand once you have the docker file that it provides\u00a0\u00a0 you then you can build the docker image using this\u00a0\ncommand giving your image a name and then it gives\u00a0\u00a0 you a command to start a container from the docker\u00a0\nimage so basically all the instructions are there\u00a0\u00a0 for the next steps as well so it didn't just give\u00a0\nus what we asked for but it actually gave us more\u00a0\u00a0 information for the next steps which is super\u00a0\nimpressive now let's assume the description\u00a0\u00a0 of one of the lines here is not sufficient so you\u00a0\nwant to understand in more detail something in the\u00a0\u00a0 docker file let's say you want to understand\u00a0\nexactly what this work tier directive does\u00a0\u00a0 in Dockerfile so we can ask it explain exactly\u00a0\nwhat Docker dear directive means in Dockerfile and this is a really detailed explanation and what\u00a0\nI actually love about this is that it explains\u00a0\u00a0 the concept or this directive specifically with an\u00a0\nexample like it actually gives you another example\u00a0\u00a0 of dockerfile and says look in this example\u00a0\nwe are setting the work directory at slash\u00a0\u00a0 app and then when we execute npm install and copy\u00a0\ncommands they will actually be executed in that\u00a0\u00a0 slash app directory and then it also gives you a\u00a0\ncomparison to something that you may already know\u00a0\u00a0 like CDE command in shell script so basically\u00a0\nchanging into a directory and then executing\u00a0\u00a0 commands from there so I think that explanation\u00a0\nis actually very good it's in simple language and\u00a0\u00a0 most importantly it gives you examples and even\u00a0\ncomparison so that you can understand it better\u00a0\u00a0 now let's say we want ChatGPT to actually adjust\u00a0\nthe docker file that it gave us and instead of npm\u00a0\u00a0 use another build and packaging tool\u00a0\ncalled yarn which is an alternative\u00a0\u00a0 to npm and let's say we're using yarn\u00a0\nin our project so we want our Docker\u00a0\u00a0 file to also be using yarn so I'm going to\u00a0\ninstruct ChatGPT to use yarn instead of npm and as you see it actually replaced the npm with\u00a0\nyarn and it gave us a new Docker file with yarn\u00a0\u00a0 commands inside and again it tells you to have\u00a0\npackage.json and have yarn start script inside\u00a0\u00a0 and the docker build and Docker one commits now\u00a0\nlet's ask it to do even more optimizations in\u00a0\u00a0 the docker file and right here on this line\u00a0\nit is actually copying everything from the\u00a0\u00a0 projects directory to this Docker image right so\u00a0\nlet's say in the root directory of application\u00a0\u00a0 we actually have lots of other files right you\u00a0\nhave git ignore node modules maybe tests folder\u00a0\u00a0 so you have a lot of code that you don't need\u00a0\nin the docker image and we don't want to copy\u00a0\u00a0 all of that inside the docker image right so I'm\u00a0\ngoing to ask ChatGPT to actually optimize this\u00a0\u00a0 and only copy the relevant application files now\u00a0\nobviously ChatGPT doesn't know what I have in the\u00a0\u00a0 application code so I'm really curious to see how\u00a0\nit actually solves this so I'm going to ask it to\u00a0\u00a0 only copy relevant files or relevant application\u00a0\nfiles not everything to the app image and here we have the output from chatgpt and\u00a0\nI actually think that it handled the task\u00a0\u00a0 really well considering that it doesn't\u00a0\neven know what I have in the application\u00a0\u00a0 so basically what it suggested us to do is create\u00a0\nthis dot dockerignore file it even explains that\u00a0\u00a0 it is similar to git ignore file where you\u00a0\ncan specify patterns of files or directories\u00a0\u00a0 so basically anything that you want to ignore\u00a0\nand exclude from ending up in the docker image\u00a0\u00a0 and then it also gives us example with actually a\u00a0\nreally good realistic examples of excluding node\u00a0\u00a0 modules and test directory if you have them\u00a0\nin your application and creating this Docker\u00a0\u00a0 ignore file with these contents and then it also\u00a0\nadjusted the docker file with this additional flag\u00a0\u00a0 to exclude whatever is in dockerignore it took the\u00a0\nversion with npm and not yarn so at this point you\u00a0\u00a0 can actually ask it to rewrite the whole thing\u00a0\nin yarn again but I think the result is actually\u00a0\u00a0 really good because using Docker ignore file\u00a0\nis also one of the best practices to keep your\u00a0\u00a0 images smaller so I think the result is actually\u00a0\nreally good and now let's do one more optimization\u00a0\u00a0 let's say again as a junior engineer you have\u00a0\nheard of this new concept in Docker which is a\u00a0\u00a0 \"multi-stage build\", but you have no idea how to\u00a0\nactually create a multi-stage build or how the\u00a0\u00a0 syntax of that would look like in Docker file\u00a0\nso you're going to ask chat GPT to do that for\u00a0\u00a0 you so we're going to say use multi-stage\u00a0\nbuilt and let's see what it comes up with and let's see what we got here so first of all\u00a0\nit gives you a brief explanation of how to build\u00a0\u00a0 a multi-stage build to use multiple froms and\u00a0\nthat each from statement starts a new stage in\u00a0\u00a0 the build process you can use the files or output\u00a0\nfrom the previous stage in the next stage right\u00a0\u00a0 using this copy from directive and of course we\u00a0\nget an example Docker file with two stages right\u00a0\u00a0 we have this from node and then from nginx\u00a0\nin the first stage it actually builds the\u00a0\u00a0 whole application with its dependencies\u00a0\nand the code that it's copying into the\u00a0\u00a0 image and then in the next stage it takes\u00a0\nthat built artifact and runs it with nginx\u00a0\u00a0 server and expose these nginx on Port 80\u00a0\ninstead of Port 3000 that we had before\u00a0\u00a0 and then it gives you an updated command to run\u00a0\nthe image and bind it on Port 80. so I think it\u00a0\u00a0 did a pretty good job but again it didn't use\u00a0\nyarn as well as that flag on the copy directive\u00a0\u00a0 to exclude files from Docker ignore file and\u00a0\nfinally in the examples I also wanted to build\u00a0\u00a0 an image with a specific Tech so let's actually\u00a0\ndirect chat GPT to create the multi-stage build\u00a0\u00a0 but with those optimizations so adjust\u00a0\nthe multi-stage build to use yarn exclude app files from dockerignore file\u00a0\nwhen copying them into Docker image\u00a0\u00a0 and provide Docker command examples with a\u00a0\nspecific image tag of let's say 1.0 [Applause]\u00a0\u00a0 I have no idea if it's gonna get all of these\u00a0\nthings right so let's actually see the result and I think it actually got all my requests right\u00a0\nso let's see it replaced the npm commands with\u00a0\u00a0 yarn commands which is good it also edited back\u00a0\nthese exclude from and it added the version tag\u00a0\u00a0 to the docker command examples so we have my node\u00a0\napp 1.0 in docker build and docker run commands\u00a0\u00a0 so generally it was actually really good at\u00a0\nunderstanding my requests and adjusting the\u00a0\u00a0 details accordingly awesome so I think it did\u00a0\npretty well with Docker now let's see another\u00a0\u00a0 example with another tool. Now before moving on,\u00a0\nI want to give a shout out to the sponsor of this\u00a0\u00a0 video \"Firefly\", which can turn any cloud resource\u00a0\nyou have into infrastructure as code trying to\u00a0\u00a0 control your Cloud as the cloud continues to grow\u00a0\nit becomes increasingly difficult to see the full\u00a0\u00a0 picture and though your guard is always up the\u00a0\nlandscape is totally fragmented and surprises\u00a0\u00a0 lurk around any corner. Why stay in the dark?\u00a0\nLight up your cloud with firefly analyze and\u00a0\u00a0 identify any unmanaged Resources with just a\u00a0\nfew clicks a firefly automatically codifies\u00a0\u00a0 any unmanaged assets into any infrastructure\u00a0\nas code tool keeping your Cloud up to code\u00a0\u00a0 unexpected changes causing your Cloud to drift\u00a0\naway get real-time notifications on any drift\u00a0\u00a0 and remediate them as they occur obtain a full\u00a0\noverview of your entire Cloud footprint across\u00a0\u00a0 multi-cloud various infrastructure as code tools\u00a0\nand kubernetes clusters prevent system failure\u00a0\u00a0 replace manual work with automation to reduce toil\u00a0\nand reclaim control over your cloud with firefly so be sure to check them out you can get\u00a0\nstarted with their free tier offer they\u00a0\u00a0 also recently launched a report on the \"state\u00a0\nof infrastructure as code 2023\". As usual I\u00a0\u00a0 will leave all the relevant links in the video\u00a0\ndescription and now let's continue with our video and this time I'm going to use kubernetes\u00a0\nand I'm actually going to stay in the same\u00a0\u00a0 chat because we already have a\u00a0\nDocker file here and some example\u00a0\u00a0 um node.js application so I'm going to use this\u00a0\ncontext to create a kubernetes deployment file for\u00a0\u00a0 this application so I'm gonna ask it to create\u00a0\na kubernetes deployment manifest or this image okay so we got the output from check GPT and\u00a0\nit actually looks really impressive uh because\u00a0\u00a0 of couple of reasons first of all let's go\u00a0\nall the way up and see our deployment file\u00a0\u00a0 and since I told it to base that kubernetes\u00a0\nmanifest file on this Docker image that we\u00a0\u00a0 created it actually took that information and\u00a0\nreused the same application name as when building\u00a0\u00a0 this Docker image but it also reuse the image tag\u00a0\n1.0 and the container Port from this multi-stage\u00a0\u00a0 build right where we're exposing this port 80. so\u00a0\nit used all this information from Docker file to\u00a0\u00a0 create the deployment manifest file now this\u00a0\nis obviously a boilerplate code that you can\u00a0\u00a0 easily grab on kubernetes official documentation\u00a0\npage but this can actually be a very convenient\u00a0\u00a0 way to generate these boilerplate manifest files\u00a0\nbecause you can reuse some of the contacts that\u00a0\u00a0 you have and ask it to it readjust and so on and\u00a0\nanother really impressive thing that it did is it\u00a0\u00a0 just went ahead and just independently gave us an\u00a0\nexample of a corresponding service manifest file\u00a0\u00a0 so this is a service component and use the load\u00a0\nbalancer type which is actually the type of\u00a0\u00a0 service you should use in production environment\u00a0\nso if you use the correct type it has the selector\u00a0\u00a0 from the labels provided here and again you use\u00a0\nthe Target Port from the deployment file and then\u00a0\u00a0 again it provided me with Cube CTL commands for\u00a0\ncreating or applying the deployment and service\u00a0\u00a0 files and even querying the service after it\u00a0\ngets created you will probably get different\u00a0\u00a0 results for each query or each request that\u00a0\nI give chat GPT it will be something similar\u00a0\u00a0 obviously but some details may vary right so for\u00a0\nexample you may have gotten the the docker file\u00a0\u00a0 example with yarn as a default instead of npm so\u00a0\nit could be these kind of differences now let's\u00a0\u00a0 continue with that and I'm gonna ask ChatGPT to\u00a0\nactually adjust the boilerplate deployment file\u00a0\u00a0 and by the way it also gave us three replicas\u00a0\ninstead of one which is pretty cool so I'm going\u00a0\u00a0 to ask ChatGPT to adjust these deployment file\u00a0\nand add some resource limitations to our container\u00a0\u00a0 so Ed resource quotas to the deployment Awesome, so let's see the output it basically just\u00a0\nedit the resource limits and request configuration\u00a0\u00a0 in the deployment file and it used the most\u00a0\nstandard default values for each configuration\u00a0\u00a0 which is pretty good actually and here is the\u00a0\nobvious advantage over just going to the official\u00a0\u00a0 documentation and getting those examples from\u00a0\nthere because when you want to add this kind of\u00a0\u00a0 configuration first of all you have to go and\u00a0\nsearch the the example syntax then you have to\u00a0\u00a0 remember exactly the right location inside your\u00a0\ndeployment file to insert that configuration so\u00a0\u00a0 ChatGPT actually just does all of that for you so\u00a0\nyou don't have to memorize the syntax of the many\u00a0\u00a0 as well so I think that is actually a really big\u00a0\nAdvantage especially when you're adding lots of\u00a0\u00a0 configuration here and when you're working with\u00a0\ndeployment files there are hundreds of lines\u00a0\u00a0 long right maybe with multiple containers and its\u00a0\nconfigurations so I can imagine this being super\u00a0\u00a0 useful in those cases and again instead of just\u00a0\nproviding the example it actually went ahead and\u00a0\u00a0 also explained what resource limits and requests\u00a0\nare right that the limits are used to prevent a\u00a0\u00a0 single container from using too many resources and\u00a0\ncausing issue on the host so I think these details\u00a0\u00a0 where chat GPT just goes one step further and\u00a0\ninstead of just delivering you the exact result\u00a0\u00a0 to your query it actually gives you even more\u00a0\nvalue explaining conceptually the example and\u00a0\u00a0 also giving you some follow-up comments that you\u00a0\nmay need to use so you can just copy the code and\u00a0\u00a0 execute on your terminal instead of typing out\u00a0\nso I think these details are actually pretty cool\u00a0\u00a0 now I want to try out one more thing\u00a0\nwhich is instead of me just saying\u00a0\u00a0 please add resource quotas or some specific\u00a0\nconfiguration I want to give chatgpt a more\u00a0\u00a0 generic request so that it can handle\u00a0\nthe underlying details for me so let's\u00a0\u00a0 say I don't know exactly what the most ideal\u00a0\ndeployment manifest looks like so I'm gonna\u00a0\u00a0 ask ChatGPT to take over that task for me so\u00a0\nI'm going to ask it to adjust the deployment manifest with production and security best\u00a0\npractices so I'm not giving a specific\u00a0\u00a0 instruction to insert a configuration instead I'm\u00a0\nsaying whatever the production and security best\u00a0\u00a0 practices are please just add them in the Manifest\u00a0\nfile so let's actually see what it comes up with all right so let's see the output here first of\u00a0\nall it started out by listing all things that\u00a0\u00a0 are best practices and production practices for\u00a0\ndeployment file and this list is actually really\u00a0\u00a0 good so it says that resource limits and requests\u00a0\nshould be configured the liveness and Readiness\u00a0\u00a0 probes so that if container isn't working it's\u00a0\nautomatically detected and restarted Etc not\u00a0\u00a0 to hard code any passwords or tokens inside as\u00a0\na security best practice and also to consider\u00a0\u00a0 access permissions right so who can do what in the\u00a0\ncluster and so on however the deployment file was\u00a0\u00a0 not actually adjusted and plus there are a couple\u00a0\nof other things that you could have mentioned\u00a0\u00a0 as a security best practice for example so what\u00a0\nI'm going to do is I'm going to repeat my Quest\u00a0\u00a0 and let's see what happens so first I'm going to\u00a0\nsay it seems like some best practices are missing\u00a0\u00a0 so please adjust deployment file properly\u00a0\nwith production and security best practices\u00a0\u00a0 without explanations just provide the\u00a0\nexample manifest file let's see what happens okay so this looks way better than before and\u00a0\nthere is one thing that I also noticed when I\u00a0\u00a0 was playing around with chat GPT is that\u00a0\nsometimes it basically starts giving you\u00a0\u00a0 the answer and it kind of stops at some point\u00a0\nI think there is sometimes a limitation to the\u00a0\u00a0 output so that's why I told it to basically\u00a0\nspare me the explanation and just give me the\u00a0\u00a0 example so it has enough space for the answer so\u00a0\nit actually could be that it just stops Midway\u00a0\u00a0 and you can just say keep explaining or continue\u00a0\nwith the response or whatever so let's actually\u00a0\u00a0 take a look at the adjusted deployment file and\u00a0\nafter this resource configurations it actually\u00a0\u00a0 added the liveness probe the Readiness probe\u00a0\nwhich are the best practice configurations\u00a0\u00a0 if you want to automatically detect when\u00a0\ncontainer is working Etc it also added the\u00a0\u00a0 image pull Secrets when you're pulling the image\u00a0\nfrom a private repository which is super nice\u00a0\u00a0 because all this configuration again you have\u00a0\nto look up the syntax because I don't think you\u00a0\u00a0 can remember all these attribute names and key\u00a0\nvalue pairs by heart and exactly where they go\u00a0\u00a0 so it's really nice that it basically just puts\u00a0\nall that together for you it also added volumes\u00a0\u00a0 for secret and config map and also edit the\u00a0\nservice economy so basically all the things\u00a0\u00a0 that it listed above is production security best\u00a0\npractices like using liveness and redness probes\u00a0\u00a0 the resource limited Secrets roles Etc it added\u00a0\nall this configuration in the deployment file\u00a0\u00a0 which is awesome I'm gonna ask you to do one more\u00a0\nconfiguration and basically just add it on top\u00a0\u00a0 of this example and let's see whether you can do\u00a0\nthat so on top of that new configuration options\u00a0\u00a0 also add security context configuration\u00a0\nin the deployment but so let's see and it actually added the security context\u00a0\nconfiguration on top of the previous deployment\u00a0\u00a0 file and this is basically a configuration where\u00a0\nwe're saying that container should run as a user\u00a0\u00a0 that is not root right so any user which is\u00a0\nnot root and that's also kind of one of the\u00a0\u00a0 security best practices not to run containers\u00a0\nas root one of the best practices so even if\u00a0\u00a0 the container was built to run with root\u00a0\nuser with security contacts it is actually\u00a0\u00a0 overriding that configuration to avoid that\u00a0\nsecurity risk and here it says as well run as\u00a0\u00a0 non-root true so this deployment file actually\u00a0\nlooks pretty good and of course when you start\u00a0\u00a0 using it and you again from a junior engineer's\u00a0\nperspective you don't know exactly how to create\u00a0\u00a0 the other components which are referenced here\u00a0\nyou can keep asking chat GPT okay now how do I\u00a0\u00a0 create this secret or how do I create the\u00a0\nservice account or configure the volumes\u00a0\u00a0 Etc so basically you can put together the rest of\u00a0\nthe configuration around this deployment file but\u00a0\u00a0 again my general impression is that the output is\u00a0\nactually pretty good considering the nuances and\u00a0\u00a0 details however sometimes you actually need\u00a0\nto have the knowledge yourself to validate\u00a0\u00a0 the output because again if you are a junior\u00a0\nengineer and you have this more or less vague\u00a0\u00a0 idea and you don't know all the details it could\u00a0\nbe difficult to validate the output or in some\u00a0\u00a0 cases to formulate the request properly to get the\u00a0\nhigh quality output now I'm going to try one more\u00a0\u00a0 thing which is a little bit more complex and I'm\u00a0\nreally curious to see how ChatGPT can handle that and I'm actually gonna stay in the same chat\u00a0\nto reuse some of the context and I'm gonna use\u00a0\u00a0 chat GPT to actually build a cicd pipeline code\u00a0\nin Jenkins so after a couple of queries to chat\u00a0\u00a0 GPT we should actually end up with a Jenkins file\u00a0\nwhich has the complete cicd pipeline code or at\u00a0\u00a0 least the the main part of it configured and\u00a0\nnote that I want it to reuse the context that\u00a0\u00a0 it already has from this chat like the kubernetes\u00a0\nfiles Docker file our node.js application and so\u00a0\u00a0 on so I'm gonna ask now to write a Jenkins\u00a0\nfile for the complete it's cicd pipeline for the above node.js application including\u00a0\ndeployment to kubernetes Cluster and let's see all right now let's see the output I believe that\u00a0\nthe output is not always the same for everybody\u00a0\u00a0 so you may actually be seeing some completely\u00a0\ndifferent results it's actually interesting\u00a0\u00a0 to compare and see how many different options\u00a0\nor versions ChatGPT comes up with for the same\u00a0\u00a0 requests. So let's actually check out our CI\/CD\u00a0\npipeline first of all it actually reused the yarn\u00a0\u00a0 instead of npm for build and test stages which is\u00a0\npretty good so it's building the application and\u00a0\u00a0 it's running the test considering there are some\u00a0\ntests in the application then it does build Docker\u00a0\u00a0 image and push Docker image stages separately\u00a0\nso I guess in some instances this stage will\u00a0\u00a0 be put together instead of separately and this is\u00a0\nalso very interesting that it's all automatically\u00a0\u00a0 decided to use the build number environment\u00a0\nvariable from Jenkins as the image tag right so\u00a0\u00a0 this is actually very good because it makes sure\u00a0\nthat a unique image with a unique tag is generated\u00a0\u00a0 every time the build runs right so that's actually\u00a0\npretty good that we don't have a hard-coded image\u00a0\u00a0 tag like 1.0 and then this is another interesting\u00a0\npart in deploy to kubernetes Stage it actually\u00a0\u00a0 knows that kubectl needs to use the kubeconfig to\u00a0\nconnect to the cluster and to authenticate with\u00a0\u00a0 the cluster so this kubeconfig file will include\u00a0\nall the credentials to connect to the cluster\u00a0\u00a0 which is a sensitive information and because of\u00a0\nthat it assumed that it should come from Jenkins\u00a0\u00a0 credentials so I think this part is actually\u00a0\nreally impressive that it automatically knows\u00a0\u00a0 that this is a sensitive or secret information\u00a0\nand it should be in a Jenkins credential and then\u00a0\u00a0 automatically comes up with just some credential\u00a0\nID that you can now use to create that credential\u00a0\u00a0 and gives you the syntax to read from the rankings\u00a0\ncredential is kubeconfig file and basically just\u00a0\u00a0 executes the command with that configuration so\u00a0\nI think the result is actually not bad there are\u00a0\u00a0 just a couple of things that are missing here\u00a0\nwhich is um first of all \"docker login\" command\u00a0\u00a0 to log into a repository before we can actually\u00a0\npush the image there which should actually happen\u00a0\u00a0 before and also this is only updating the image\u00a0\nin the deployment in the cluster but is not\u00a0\u00a0 applying the whole deployment file or service\u00a0\nfile so again we can give it some additional\u00a0\u00a0 instructions to adjust this Pipeline with all\u00a0\nthese additional steps so I'm going to ask it to\u00a0\u00a0 and just the Jenkins file to have one stage\u00a0\nfor building and pushing the docker image and\u00a0\u00a0 log in logging in to the docker\u00a0\nRepository that's a on Docker hub and in the final stage apply deployment\u00a0\nand service files to the cluster but keep the cube config parameter let's see okay so the result does not look exactly what\u00a0\nI was looking for which could also mean that my\u00a0\u00a0 prompt my request wasn't properly formulated\u00a0\nso it adjusted the deploy to kubernetes part\u00a0\u00a0 with write configuration so it's deploying the\u00a0\ndeployment or it's applying the deployment and\u00a0\u00a0 service file changes but it kept this cubeconfig\u00a0\nfile and here you basically just put everything\u00a0\u00a0 together so I'm gonna ask it to change the stage\u00a0\nbecause this obviously doesn't look very good so\u00a0\u00a0 so keep the build and test\u00a0\nstages as they were initially\u00a0\u00a0 but create a separate stage for building a\u00a0\nDocker image and pushing to Docker Repository\u00a0\u00a0 but before pushing image make sure to log\u00a0\nin to the docker Repository first let's see okay now this looks better we still have those\u00a0\nbuild and test stages and here it is building the\u00a0\u00a0 image then logging in to Docker Hub again using\u00a0\nthe credential or assuming that because it's a\u00a0\u00a0 password it should get it from the credentials\u00a0\nfrom Jenkins credentials and using it here and\u00a0\u00a0 this is also actually a good practice to\u00a0\nuse password standard in option instead\u00a0\u00a0 of passing it as a password a flag directly\u00a0\nlike the username it could have also gotten\u00a0\u00a0 the username from credentials like the password\u00a0\nbut I didn't and it also thin add the kubernetes\u00a0\u00a0 deployment from the previous example so I'm going\u00a0\nto ask it to add deployment deploy to kubernetes\u00a0\u00a0 stage um as in the previous example also read occur hub username as a credential\u00a0\njust like the docker have password but call them Docker user and\u00a0\nDocker pass PWD respectively and also use the credentials function\u00a0\ninstead of with credentials to read both\u00a0\u00a0 of these values so I actually put a\u00a0\ncouple of directives to adjust some\u00a0\u00a0 details in the Jenkins file and I'm really\u00a0\ncurious to see what it comes up with now okay so build and test look fine so I think it got most of my instructions right\u00a0\nfirst of all it changed the variable values to\u00a0\u00a0 Docker user and Docker PWD which is great and now\u00a0\nit's reading the user also from the credential\u00a0\u00a0 so it has this Docker Hub credentials and it reads\u00a0\nboth values from that credential and again we have\u00a0\u00a0 this Docker login command and then Docker push and\u00a0\nit added the kubernetes deployment stage also in\u00a0\u00a0 the pipeline code one thing that it didn't do is\u00a0\nreplacing these with credentials with credentials\u00a0\u00a0 function so this is kind of the base of Jenkins\u00a0\nfile or cicd pipeline that you can then build on\u00a0\u00a0 top of and I think the result is actually not very\u00a0\nbad again I assume that different people may get\u00a0\u00a0 different results on the same request or query\u00a0\nso your Jenkins file may actually look a little\u00a0\u00a0 bit different but of course you would have to do\u00a0\nsome optimizations here right so for example the\u00a0\u00a0 deployment file that we let chatgpt generate for\u00a0\nus has this hard-coded image tag inside 1.0 so of\u00a0\u00a0 course you would have to dynamically set that\u00a0\nor adjust that to be my node app build number\u00a0\u00a0 whatever the value is instead of hard-coded 1.0\u00a0\nso that could be one of the optimizations you\u00a0\u00a0 could add deployment to multiple stages\u00a0\nlike development testing production\u00a0\u00a0 Etc so you can use this as a foundation to\u00a0\nextent I'm going to do one final thing here\u00a0\u00a0 and ask ChatGPT to add another stage for a select\u00a0\nnotification about the pipeline build status so\u00a0\u00a0 add a step to notify stage to notify team\u00a0\nthrough slack Channel about the build status um so this is an example that it gave us a notify\u00a0\nslick stage with a step that basically executes\u00a0\u00a0 these select send function for a success\u00a0\nmessage right and it also tells us how\u00a0\u00a0 slick notification plugin should be configured\u00a0\nwith the link so that's actually pretty good\u00a0\u00a0 um and this is what I mentioned where sometimes\u00a0\njust in the middle of reply it just stops so it\u00a0\u00a0 kind of gave us half of that Jenkins file um but\u00a0\nit suggested us to put that as a final step after\u00a0\u00a0 deploy to kubernetes one thing that I'm missing\u00a0\nhere is actually the failure notification so\u00a0\u00a0 this one actually sends just success message\u00a0\nbut it should also send a message when build\u00a0\u00a0 failed right so I'm going to instruct it to\u00a0\nconsider both cases so select notification\u00a0\u00a0 should be sent either for failure\u00a0\nor success and should always\u00a0\u00a0 execute after the pipeline or after the build\u00a0\nis finished as the last step and let's see okay so this time it actually provided better\u00a0\nresults than previously first of all it's using\u00a0\u00a0 post block which is executed after all stages\u00a0\nhave completed regardless of success or failure so\u00a0\u00a0 that's what we need and then it basically checks\u00a0\nwhether the result was success in which case it\u00a0\u00a0 sends built succeeded otherwise build failed it\u00a0\ncould have used the success and failure blocks\u00a0\u00a0 would have been a little bit cleaner and easier\u00a0\nbut this looks better than the previous example\u00a0\u00a0 and then again it stopped just midway so I can\u00a0\nactually tell it to continue with your response and e is going to give us rest of the\u00a0\nJenkins file with the post block edit so it provided us with the Jenkins file\u00a0\nwith this Slack notification Step at the\u00a0\u00a0 end however I actually forgot to put\u00a0\nthat whole thing in a separate stage\u00a0\u00a0 after the deploy to kubernetes Stage so overall\u00a0\nimpression for this specific task for building\u00a0\u00a0 the CI\/CD pipeline it wasn't actually as good so\u00a0\nyou definitely need a lot of knowledge yourself to\u00a0\u00a0 build a cicd pipeline because you can't 100 rely\u00a0\non the results that chat GPT gives you however it\u00a0\u00a0 is definitely helpful in building like the base\u00a0\nsyntax and configuration that you can then kind\u00a0\u00a0 of optimize on and I think the most value you\u00a0\ncan get here is when you actually know what\u00a0\u00a0 the pipeline should end up with but you don't have\u00a0\nthe exact syntax in mind or you don't know exactly\u00a0\u00a0 the plugins that are available for a specific\u00a0\ntask you can actually put together a pretty\u00a0\u00a0 good pipeline code with specific requests so it\u00a0\ncould be valuable in those scenarios and finally\u00a0\u00a0 as a very very last step I actually want to try\u00a0\nout one more thing which is taking this whole\u00a0\u00a0 um Jenkins file and I'm actually going to leave\u00a0\nout this select notification part so Jenkins\u00a0\u00a0 file without this last step and I'm going to\u00a0\nask ChatGPT to give me a GitLab CI equivalent of this Jenkins file I'm gonna paste it in\u00a0\nand I think I am actually missing the final um curly brace here so I'm\u00a0\ngoing to edit there you go\u00a0\u00a0 and so basically it should give me the CI\u00a0\nCD pipeline configured in Jenkins file but\u00a0\u00a0 for gitlab CI so with gitlab CI syntax and I'm\u00a0\ngoing to execute and let's see what it gives us all right so I think this time ChatGPT actually\u00a0\ndid a really good job of converting our Jenkins\u00a0\u00a0 file to GitLab ci.yaml file it gives us some\u00a0\nimmediate differences as an explanation here\u00a0\u00a0 and this file actually looks pretty good so we\u00a0\nhave the stages the stages that we had in Jenkins\u00a0\u00a0 file however here it's mixed up the stages or\u00a0\nthe configuration for stages a little bit and\u00a0\u00a0 it basically put a yarn install and yarn build in\u00a0\nbuild and push stage so these two stages basically\u00a0\u00a0 were ignored but some of the configuration is\u00a0\npretty good so for example it is using before\u00a0\u00a0 script correctly where it's logging into docker\u00a0\nbefore it builds and pushes the image it also\u00a0\u00a0 detected that it needed an execution environment\u00a0\nwith Docker in it to execute those commands\u00a0\u00a0 um and in the deploy stage again it uses\u00a0\nbefore script to configure the kubeconfig\u00a0\u00a0 location so in this dot kube folder so again\u00a0\nyou will definitely need to do some adjustments\u00a0\u00a0 and optimizations here but I think as a base\u00a0\nconfiguration file especially considering that\u00a0\u00a0 it was generated or mapped from the Jenkins\u00a0\nfile directly I think it's a pretty good base\u00a0\u00a0 configuration to build on so these were some of\u00a0\nthe examples that I thought would be realistic to\u00a0\u00a0 use chat GPT for some devops tasks especially\u00a0\nfrom a perspective of someone who is learning\u00a0\u00a0 and doesn't know these Technologies very well\u00a0\nlike examples of how they can use jbd to make\u00a0\u00a0 their work more efficient there is definitely\u00a0\nsome room for improvement in terms of accuracy\u00a0\u00a0 because you also have to be able to validate the\u00a0\noutput and can't 100 rely on it when it doesn't\u00a0\u00a0 give you accurate results but for a little\u00a0\nbit more experienced Engineers I think this\u00a0\u00a0 is a really good way to become more productive to\u00a0\nsave a lot of time especially in generating these\u00a0\u00a0 base configuration or boilerplate configuration\u00a0\nthat you can then optimize and build on top of\u00a0\u00a0 and since you are able to also validate that\u00a0\nresult I think it can save you a lot of time\u00a0\u00a0 and it can be a lot of help in scripting and\u00a0\nwriting this configuration file is code or the\u00a0\u00a0 infrastructure is code and all these automation\u00a0\ncode basically so you don't have to memorize the\u00a0\u00a0 syntax and configuration and all the details so I\u00a0\nthink this could be a really helpful tool in that now as I mentioned chat GPT is one of the projects\u00a0\nof open Ai and openai actually provides an API\u00a0\u00a0 which is open source and anyone can build on\u00a0\ntop of that so there are many companies or\u00a0\u00a0 individual developers out there who actually\u00a0\ncreate models based on open ai's API so they\u00a0\u00a0 reuse all these resources that open AI provides\u00a0\nand all these trained models basically and they\u00a0\u00a0 kind of optimize on top of that like providing a\u00a0\nbetter UI and user-friendly experience or training\u00a0\u00a0 the model further for a specific use case like\u00a0\nyou have a tool based on open ai's technology\u00a0\u00a0 but specifically for answering legal questions or\u00a0\na tool that lets you do some specific engineering\u00a0\u00a0 tasks and so on so the idea is instead of\u00a0\nhaving a general purpose tool that does\u00a0\u00a0 pretty much everything you have a tool that is\u00a0\ntrained for a specific use case days or set of\u00a0\u00a0 tasks and it does that one thing really well and\u00a0\nas a perfect example of that Firefly the company\u00a0\u00a0 who sponsored this video actually created an\u00a0\nopen source CLI tool based on chat GPT model\u00a0\u00a0 that specifically works for infrastructure as code\u00a0\ngeneration and it's called aiac so basically it's\u00a0\u00a0 an open source command line tool that lets you\u00a0\ngenerate infrastructure as code templates scripts\u00a0\u00a0 configuration code utilities queries whatever with\u00a0\na simple command line comment so basically all\u00a0\u00a0 the tasks that we just did with chat gbt so let's\u00a0\nactually see that in action and how useful can it\u00a0\u00a0 be when working on devops tasks you can install\u00a0\nit with a simple Brew command or even run it\u00a0\u00a0 as a Docker container if you want I have already\u00a0\ninstalled it you need to also generate an API key\u00a0\u00a0 on open AI platform itself and provide that API\u00a0\nkey through environment variable when executing\u00a0\u00a0 the aiac commands in order to authenticate\u00a0\nwith open Ai and it's actually a very simple\u00a0\u00a0 straightforward method it probably took me two\u00a0\nminutes to install and set the whole thing up and\u00a0\u00a0 I'll provide the link to the guide in the video\u00a0\ndescription and once you're all set up now we can\u00a0\u00a0 go ahead and use it to generate some configuration\u00a0\nfiles for us so I just executed a simple Brew\u00a0\u00a0 install command and then basically just set\u00a0\nthe environment variable for openai API key\u00a0\u00a0 to the API key that I just generated on their site\u00a0\nand once you have all of that set up we're ready\u00a0\u00a0 to go and actually generate some infrastructure\u00a0\nas code scripts manifests any devops configuration\u00a0\u00a0 files so just like in our previous example let's\u00a0\nask aiac to generate a Docker file for node.js\u00a0\u00a0 application and the command for that is AI AC\u00a0\nget and then we're going to say Docker file\u00a0\u00a0 or node.js application and let's see what we get so this is our Docker file it uses npm it\u00a0\nbasically installs all the dependencies\u00a0\u00a0 then copies all the files and starts\u00a0\nthe application using npm script and\u00a0\u00a0 now this is actually a very useful part\u00a0\nwhere it asks you to either regenerate\u00a0\u00a0 the result so you can ask it to give you a\u00a0\ndifferent example so we can do R for retry and it's gonna try to generate a code example\u00a0\nagain so this time it gave us a little bit\u00a0\u00a0 different configuration so for example it\u00a0\ntook an Alpine version of the base image\u00a0\u00a0 which is probably going to be smaller in size\u00a0\nit also used a different work directory it is\u00a0\u00a0 now also copying package log Json and so\u00a0\non so it actually gave us a different code\u00a0\u00a0 example and a very convenient thing you can do\u00a0\nwith it is actually save the results whatever\u00a0\u00a0 was output in the console you can save it\u00a0\ndirectly into a file so I'm going to do s\u00a0\u00a0 so save and it's going to ask in which file\u00a0\nshould I save it so we can tell it to create a\u00a0\u00a0 Docker file and save the output there and now we\u00a0\nshould have a Docker file that was created with\u00a0\u00a0 that code example so that's how you can use the\u00a0\ntool now let's try another example where we ask\u00a0\u00a0 aiac to generate some example a terraform script\u00a0\nfor creating an ec2 instance again AI AC gets\u00a0\u00a0 this time terraform for ec2 instance\u00a0\nor let's say for two ec2 instances so we got this pretty basic terraform\u00a0\nscript for defining these two resources\u00a0\u00a0 again we can do retry and let's see\u00a0\nwhat other template it comes up with and this time it gave us a little bit different\u00a0\nexample with the AWS provider configuration for\u00a0\u00a0 providing your AWS credentials and defining your\u00a0\nregion and also instead of having two separate\u00a0\u00a0 resources it basically just edit account attribute\u00a0\nhere to avoid some code redundancy again if you\u00a0\u00a0 are unhappy with the results you can do retry as\u00a0\nmany times as you want and once you're happy with\u00a0\u00a0 the configuration you can save it directly into a\u00a0\nfile so I'm going to do s and let's say I want to\u00a0\u00a0 save it into main.tf file and if we check main.tf\u00a0\nwe should have that code example in the file so\u00a0\u00a0 that's basically how you can use this tool to help\u00a0\nyou in generating some boilerplate code to give\u00a0\u00a0 it some commands for different Technologies for\u00a0\nDocker terraform ansible kubernetes manifest files\u00a0\u00a0 whatever we need in our devops tasks and use it as\u00a0\nyour small CLI devops assistant basically so apart\u00a0\u00a0 from the fact that you can use it on command line\u00a0\ninterface I find Convenient that you don't get any\u00a0\u00a0 a needy text explanations with every output so\u00a0\ncheck GPD explains the examples which is great\u00a0\u00a0 for Learning and understanding the output and you\u00a0\ncould actually tell ChatGPT to not give you an\u00a0\u00a0 explanation it's just the code but here you just\u00a0\nget the code snippet that you asked for without\u00a0\u00a0 any text around it and you can then just directly\u00a0\nsave it into a file just a convenient thing now\u00a0\u00a0 you can go ahead and install and play around\u00a0\nwith this tool yourself if you want I will as\u00a0\u00a0 always leave all the relevant links in the video\u00a0\ndescription below so make sure to check that out so overall what do I think about ChatGPT will it\u00a0\nreplace Engineers or engineering jobs threatened\u00a0\u00a0 and is everything I said in my latest video\u00a0\nabout getting into it still relevant I remember\u00a0\u00a0 with the first advancements of AI people were\u00a0\nsaying that AI can do certain things better\u00a0\u00a0 than humans and replace the boring repetitive\u00a0\ntedious and less creative tasks that humans\u00a0\u00a0 don't want to do anyways like memorizing stuff\u00a0\ncalculating researching and analyzing tons of\u00a0\u00a0 data and so on so humans would be free to do more\u00a0\nthought-provoking and creative tasks but now we're\u00a0\u00a0 seeing AI doing those human feed tasks pretty good\u00a0\nas well AI can do exactly those creative complex\u00a0\u00a0 tasks like creating digital art images and videos\u00a0\noften better than humans and I mentioned this\u00a0\u00a0 project Dali one of open ai's models which can\u00a0\ndo exactly that also other creative things like\u00a0\u00a0 troubleshooting an issue fixing a code writing up\u00a0\na legal document or writing a whole creative story\u00a0\u00a0 or article however one thing that hasn't changed\u00a0\nyet is that AI like ChatGPT still needs to be used\u00a0\u00a0 by humans and that's the whole point right using\u00a0\nAI so humans can be more productive so the fact\u00a0\u00a0 that AI can do certain tasks better than humans\u00a0\ndoes not necessarily mean that you need less\u00a0\u00a0 Engineers it means that the engineers will become\u00a0\nmore efficient so the same number of Engineers\u00a0\u00a0 can do more stuff faster so it accelerates the\u00a0\ngrowth and speed of development which obviously\u00a0\u00a0 every company wants to have right no I believe\u00a0\nwhat actually is threatened by chenshipity is the\u00a0\u00a0 pretty outdated educational system throughout\u00a0\nthe world and the problem is that the modern\u00a0\u00a0 educational system even the higher education like\u00a0\nuniversities focus more on teaching to memorize\u00a0\u00a0 stuff and things which humans are weaker at than\u00a0\na computer learning things from books research\u00a0\u00a0 results things that have happened and other people\u00a0\nhave done like lawyers memorizing legal texts and\u00a0\u00a0 instead they focus Less on teaching the creative\u00a0\nand analytic independent thinking or even teaching\u00a0\u00a0 to use AI tools to do more creative stuff so\u00a0\nprofessions like tax advisors lawyers graphic\u00a0\u00a0 designers especially ones doing more more or\u00a0\nless standard work can be very well replaced\u00a0\u00a0 by a better cheaper and faster AI so in those\u00a0\nfields we may have only the top players who\u00a0\u00a0 still generate value over AI by doing more\u00a0\ncomplex work and they will be using AI to\u00a0\u00a0 do their job at a higher level and that's what I\u00a0\npersonally think a logical development of things\u00a0\u00a0 may look like for these professions and this means\u00a0\nwe need to start working on a high level and start\u00a0\u00a0 critical thinking problem solving and how to\u00a0\nuse AI to solve actual problems and generate\u00a0\u00a0 value instead of Simply carrying out the tasks\u00a0\nthat AI can do better than us we need to be ones\u00a0\u00a0 envisioning and planning what needs to be done\u00a0\nnow in terms of Engineers I believe that AI will\u00a0\u00a0 not be able to fill the gap of deficit that\u00a0\nstill exists for different engineering roles\u00a0\u00a0 because the IT industry is the fastest developing\u00a0\none with more new Fields being added each field\u00a0\u00a0 itself expanding and encompassing more skills\u00a0\nthink about some of the I.T professions like\u00a0\u00a0 blockchain developer or machine learning engineer\u00a0\ndata scientists how new these professionals are\u00a0\u00a0 compared to some traditional professions like they\u00a0\ndidn't even exist decades ago and weren't even a\u00a0\u00a0 realistic career path options right so maybe some\u00a0\nengineering roles will replace or outgrow others\u00a0\u00a0 but if there is one profession or skill that\u00a0\nwill grow in demand that's Engineers Engineers\u00a0\u00a0 play a crucial role in the design development and\u00a0\nimplementation of new technologies and systems and\u00a0\u00a0 their skills and knowledge will actually continue\u00a0\nto be more in demand as long as there are new\u00a0\u00a0 challenges and problems to solve so my latest\u00a0\nvideo on getting into it is probably even more\u00a0\u00a0 relevant now than ever with the development\u00a0\nof AI considering the engineering jobs will\u00a0\u00a0 become even more demanded that being said I\u00a0\nalso believe that Engineers who do not learn\u00a0\u00a0 new skills don't grow professionally and don't\u00a0\nadapt to the technological changes and kind of\u00a0\u00a0 stay in their comfort zone just doing the same\u00a0\ntests will be placed or automated by AI at some\u00a0\u00a0 point in the future but I think that's really the\u00a0\nexceptional case considering that it projects are\u00a0\u00a0 actually very Dynamic and you always have some\u00a0\nkind of incentive to grow and develop your skills\u00a0\u00a0 as an engineer so as long as you as a developer\u00a0\ngrow your skills and knowledge at a normal speed\u00a0\u00a0 which often happens naturally when you work as an\u00a0\nengineer I think you're going to be more than fine\u00a0\u00a0 but even in the case where a specific engineering\u00a0\nrole or job may become automated through AI having\u00a0\u00a0 the base foundation in it will actually help\u00a0\nyou transition to any other it field way easier\u00a0\u00a0 compared to people who are just getting into it\u00a0\nwithout it background and we see that with system\u00a0\u00a0 administrators who are increasingly transitioning\u00a0\nto Cloud engineering or devops engineering they're\u00a0\u00a0 just adapting and it's of course way easier for\u00a0\nthem to transition on into those fields having the\u00a0\u00a0 backgrounds that they have so overall Engineers\u00a0\nare needed now more than ever given the speed of\u00a0\u00a0 development in Tech world but you need to be ready\u00a0\nto learn new things and adapt to the changes in\u00a0\u00a0 Tech and one of those new things you will need\u00a0\nto learn as an engineer may actually become\u00a0\u00a0 the actual skill of using AI tools things like\u00a0\nprompt engineering which basically means preparing\u00a0\u00a0 formulating your requests in the best way to get\u00a0\nthe most optimal output from the AI tool in that\u00a0\u00a0 sense AI should be considered as an additional\u00a0\ntool in the toolset of an engineer to do their\u00a0\u00a0 job so that's my take on the whole thing I would\u00a0\nbe interested to know what are your thoughts on AI\u00a0\u00a0 and have you actually used any AI tool maybe\u00a0\nchat GPT at your work already as an engineer\u00a0\u00a0 and what were the results share them in the\u00a0\ncomments because I'm sure it will be interesting\u00a0\u00a0 for other viewers as well and with that thank\u00a0\nyou for watching and see you in the next video",
        "videoTranscriptLog":"english:['en', 'en']. "
    }
]